{
    "__type__": "task_card",
    "loader": {
        "__type__": "load_hf",
        "path": "cais/mmlu",
        "name": "jurisprudence"
    },
    "preprocess_steps": [
        {
            "__type__": "rename_splits",
            "mapper": {
                "dev": "train"
            }
        },
        {
            "__type__": "set",
            "fields": {
                "topic": "jurisprudence"
            }
        }
    ],
    "task": "tasks.qa.multiple_choice.with_topic",
    "templates": "templates.qa.multiple_choice.with_topic.all",
    "__tags__": {
        "annotations_creators": "no-annotation",
        "arxiv": [
            "2009.03300",
            "2005.00700",
            "2005.14165",
            "2008.02275"
        ],
        "language": "en",
        "language_creators": "expert-generated",
        "license": "mit",
        "multilinguality": "monolingual",
        "region": "us",
        "size_categories": "10K<n<100K",
        "source_datasets": "original",
        "task_categories": "question-answering",
        "task_ids": "multiple-choice-qa"
    },
    "__description__": "Measuring Massive Multitask Language Understanding by Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt (ICLR 2021). \nThis is a massive multitask test consisting of multiple-choice questions from various branches of knowledge. The test spans subjects in the humanities, social sciences, hard sciences, and other areas that are important for some people to learn. This covers 57â€¦ See the full description on the dataset page: https://huggingface.co/datasets/cais/mmlu."
}
