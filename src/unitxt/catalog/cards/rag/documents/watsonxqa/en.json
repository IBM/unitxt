{
    "__type__": "task_card",
    "loader": {
        "__type__": "load_hf",
        "path": "ibm-research/watsonxDocsQA",
        "name": "corpus",
        "data_classification_policy": [
            "public"
        ]
    },
    "preprocess_steps": [
        {
            "__type__": "rename_splits",
            "mapper": {
                "test": "train"
            }
        },
        {
            "__type__": "copy",
            "field": "doc_id",
            "to_field": "document_id"
        },
        {
            "__type__": "wrap",
            "field": "document",
            "inside": "list",
            "to_field": "passages"
        }
    ],
    "task": "tasks.rag.corpora",
    "templates": {
        "empty": {
            "__type__": "input_output_template",
            "input_format": "",
            "output_format": ""
        }
    },
    "__tags__": {
        "license": "Apache 2.0",
        "url": "https://huggingface.co/datasets/ibm-research/watsonxDocsQA"
    },
    "__description__": "watsonxDocsQA is a new open-source dataset and benchmark contributed by IBM. The dataset is derived from enterprise product documentation and designed specifically for end-to-end Retrieval-Augmented Generation (RAG) evaluation. The dataset consists of two components:\n\n    Documents: A corpus of 1,144 text and markdown files generated by crawling enterprise documentation (main page).\n    Benchmark: A set of 75 question-answer (QA) pairs with gold document labels and answers.The QA pairs are crafted as follows:\n        25 questions: Human-generated by two subject matter experts.\n        50 questions: Synthetically generated using the tiiuae/falcon-180b model, then manually filtered and reviewed for quality."
}
