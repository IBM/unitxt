{
    "__type__": "task_card",
    "loader": {
        "__type__": "load_hf",
        "path": "ibm-research/REAL-MM-RAG_FinReport",
        "name": "default",
        "split": "test",
        "data_classification_policy": [
            "public"
        ]
    },
    "preprocess_steps": [
        {
            "__type__": "rename_splits",
            "mapper": {
                "test": "train"
            }
        },
        {
            "__type__": "hash_image",
            "field": "image",
            "to_field": "document_id"
        },
        {
            "__type__": "deduplicate",
            "by": [
                "document_id"
            ]
        },
        {
            "__type__": "to_image",
            "field": "image"
        },
        {
            "__type__": "wrap",
            "field": "image",
            "inside": "list",
            "to_field": "passages"
        }
    ],
    "task": "tasks.rag.corpora",
    "templates": {
        "empty": {
            "__type__": "input_output_template",
            "input_format": "",
            "output_format": ""
        }
    },
    "__tags__": {
        "license": "cdla-permissive-2.0",
        "url": "https://huggingface.co/datasets//ibm-research/REAL-MM-RAG_FinReport"
    },
    "__title__": "REALMMRAG: FinReport",
    "__description__": "We introduced REAL-MM-RAG-Bench, a real-world multi-modal retrieval benchmark designed to evaluate retrieval models in reliable, challenging, and realistic settings. The benchmark was constructed using an automated pipeline, where queries were generated by a vision-language model (VLM), filtered by a large language model (LLM), and rephrased by an LLM to ensure high-quality retrieval evaluation. To simulate real-world retrieval challenges, we introduce multi-level query rephrasing, modifying queries at three distinct levels—from minor wording adjustments to significant structural changes—ensuring models are tested on their true semantic understanding rather than simple keyword matching."
}
