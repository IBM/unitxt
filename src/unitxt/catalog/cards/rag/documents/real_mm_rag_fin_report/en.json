{
    "__type__": {
        "module": "unitxt.card",
        "name": "TaskCard"
    },
    "loader": {
        "__type__": {
            "module": "unitxt.loaders",
            "name": "LoadHF"
        },
        "path": "ibm-research/REAL-MM-RAG_FinReport",
        "name": "default",
        "split": "test",
        "data_classification_policy": [
            "public"
        ]
    },
    "preprocess_steps": [
        {
            "__type__": {
                "module": "unitxt.splitters",
                "name": "RenameSplits"
            },
            "mapper": {
                "test": "train"
            }
        },
        {
            "__type__": {
                "module": "unitxt.image_operators",
                "name": "HashImage"
            },
            "field": "image",
            "to_field": "document_id"
        },
        {
            "__type__": {
                "module": "unitxt.operators",
                "name": "Deduplicate"
            },
            "by": [
                "document_id"
            ]
        },
        {
            "__type__": {
                "module": "unitxt.image_operators",
                "name": "ToImage"
            },
            "field": "image"
        },
        {
            "__type__": {
                "module": "unitxt.collections_operators",
                "name": "Wrap"
            },
            "field": "image",
            "inside": "list",
            "to_field": "passages"
        }
    ],
    "task": "tasks.rag.corpora",
    "templates": {
        "empty": {
            "__type__": {
                "module": "unitxt.templates",
                "name": "InputOutputTemplate"
            },
            "input_format": "",
            "output_format": ""
        }
    },
    "__tags__": {
        "license": "cdla-permissive-2.0",
        "url": "https://huggingface.co/datasets//ibm-research/REAL-MM-RAG_FinReport"
    },
    "__title__": "REALMMRAG: FinReport",
    "__description__": "We introduced REAL-MM-RAG-Bench, a real-world multi-modal retrieval benchmark designed to evaluate retrieval models in reliable, challenging, and realistic settings. The benchmark was constructed using an automated pipeline, where queries were generated by a vision-language model (VLM), filtered by a large language model (LLM), and rephrased by an LLM to ensure high-quality retrieval evaluation. To simulate real-world retrieval challenges, we introduce multi-level query rephrasing, modifying queries at three distinct levels—from minor wording adjustments to significant structural changes—ensuring models are tested on their true semantic understanding rather than simple keyword matching."
}
