{
    "__type__": "task_card",
    "loader": {
        "__type__": "load_hf",
        "path": "ibm-research/REAL-MM-RAG_TechSlides",
        "name": "default",
        "split": "test",
        "data_classification_policy": [
            "public"
        ]
    },
    "preprocess_steps": [
        {
            "__type__": "filter_by_condition",
            "values": {
                "query": null
            },
            "condition": "ne"
        },
        {
            "__type__": "execute_expression",
            "to_field": "image",
            "expression": "hashlib.md5(image.tobytes()).hexdigest()",
            "imports_list": [
                "hashlib"
            ]
        },
        {
            "__type__": "copy",
            "field_to_field": {
                "image": "reference_context_ids",
                "query": "question"
            }
        },
        {
            "__type__": "add_incremental_id",
            "to_field": "question_id"
        },
        {
            "__type__": "cast",
            "field": "question_id",
            "to": "str"
        },
        {
            "__type__": "split_random_mix",
            "mix": {
                "test": "test[30%]",
                "train": "test[70%]"
            }
        },
        {
            "__type__": "wrap",
            "field": "answer",
            "inside": "list",
            "to_field": "reference_answers"
        },
        {
            "__type__": "wrap",
            "field": "reference_context_ids",
            "inside": "list",
            "to_field": "reference_context_ids"
        }
    ],
    "task": "tasks.rag.end_to_end",
    "templates": {
        "default": "templates.rag.end_to_end.json_predictions"
    },
    "__tags__": {
        "license": "cdla-permissive-2.0",
        "url": "https://huggingface.co/datasets//ibm-research/REAL-MM-RAG_TechSlides"
    },
    "__description__": "We introduced REAL-MM-RAG-Bench, a real-world multi-modal retrieval benchmark designed to evaluate retrieval models in reliable, challenging, and realistic settings. The benchmark was constructed using an automated pipeline, where queries were generated by a vision-language model (VLM), filtered by a large language model (LLM), and rephrased by an LLM to ensure high-quality retrieval evaluation. To simulate real-world retrieval challenges, we introduce multi-level query rephrasing, modifying queries at three distinct levels—from minor wording adjustments to significant structural changes—ensuring models are tested on their true semantic understanding rather than simple keyword matching."
}
