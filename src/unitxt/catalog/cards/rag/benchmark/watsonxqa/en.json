{
    "__type__": "task_card",
    "loader": {
        "__type__": "load_hf",
        "path": "ibm-research/watsonxDocsQA",
        "name": "question_answers",
        "data_classification_policy": [
            "public"
        ]
    },
    "preprocess_steps": [
        {
            "__type__": "copy",
            "field_to_field": {
                "question": "question",
                "question_id": "question_id"
            }
        },
        {
            "__type__": "wrap",
            "field": "correct_answer_document_ids",
            "inside": "list",
            "to_field": "reference_context_ids"
        },
        {
            "__type__": "wrap",
            "field": "correct_answer",
            "inside": "list",
            "to_field": "reference_answers"
        }
    ],
    "task": "tasks.rag.end_to_end",
    "templates": {
        "default": "templates.rag.end_to_end.json_predictions"
    },
    "__tags__": {
        "license": "Apache 2.0",
        "url": "https://huggingface.co/datasets/ibm-research/watsonxDocsQA"
    },
    "__description__": "watsonxDocsQA is a new open-source dataset and benchmark contributed by IBM. The dataset is derived from enterprise product documentation and designed specifically for end-to-end Retrieval-Augmented Generation (RAG) evaluation. The dataset consists of two components:\n\n    Documents: A corpus of 1,144 text and markdown files generated by crawling enterprise documentation (main page).\n    Benchmark: A set of 75 question-answer (QA) pairs with gold document labels and answers.The QA pairs are crafted as follows:\n        25 questions: Human-generated by two subject matter experts.\n        50 questions: Synthetically generated using the tiiuae/falcon-180b model, then manually filtered and reviewed for quality."
}
