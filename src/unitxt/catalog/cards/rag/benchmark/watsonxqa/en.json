{
    "__type__": "task_card",
    "loader": {
        "__type__": "load_hf",
        "path": "ibm-research/watsonxDocsQA",
        "name": "question_answers",
        "data_classification_policy": [
            "public"
        ]
    },
    "preprocess_steps": [
        {
            "__type__": "split_random_mix",
            "mix": {
                "test": "train[30%]",
                "train": "train[70%]"
            }
        },
        {
            "__type__": "copy",
            "field_to_field": {
                "question": "question",
                "question_id": "question_id",
                "ground_truths": "reference_answers",
                "ground_truths_context_ids": "reference_context_ids"
            }
        },
        {
            "__type__": "wrap",
            "field": "ground_truths_contexts",
            "inside": "list",
            "to_field": "reference_contexts"
        }
    ],
    "task": "tasks.rag.end_to_end",
    "templates": {
        "default": "templates.rag.end_to_end.json_predictions"
    },
    "__tags__": {
        "license": "Apache 2.0",
        "url": "https://huggingface.co/datasets/ibm-research/watsonxDocsQA"
    },
    "__description__": "watsonxDocsQA is a new open-source dataset and benchmark contributed by IBM. The dataset is derived from enterprise product documentation and designed specifically for end-to-end Retrieval-Augmented Generation (RAG) evaluation. The dataset consists of two components:\n\nDocuments: A corpus of 1,144 text and markdown files generated by crawling enterprise documentation (main page).\nBenchmark: A set of 75 question-answer (QA) pairs with gold document labels and answers.The QA pairs are crafted as follows:\n    25 questions: Human-generated by two subject matter experts.\n    50 questions: Synthetically generated using the tiiuae/falcon-180b model, then manually filtered and reviewed for quality.\n"
}
