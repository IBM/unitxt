{
    "type": "task_card",
    "loader": {
        "type": "load_hf",
        "path": "super_glue",
        "name": "copa"
    },
    "preprocess_steps": [
        "splitters.small_no_test",
        {
            "type": "list_field_values",
            "fields": [
                "choice1",
                "choice2"
            ],
            "to_field": "choices"
        },
        {
            "type": "rename_fields",
            "field_to_field": {
                "premise": "context",
                "label": "answer"
            }
        },
        {
            "type": "map_instance_values",
            "mappers": {
                "question": {
                    "cause": "What was the cause of this?",
                    "effect": "What happened as a result?"
                }
            }
        },
        {
            "type": "add_fields",
            "fields": {
                "context_type": "sentence"
            }
        }
    ],
    "task": "tasks.qa.multiple_choice.with_context",
    "templates": "templates.qa.multiple_choice.with_context.all",
    "__description__": "SuperGLUE (https://super.gluebenchmark.com/) is a new benchmark styled afterGLUE with a new set of more difficult language understanding tasks, improvedresources, and a new public leaderboard.The Choice Of Plausible Alternatives (COPA, Roemmele et al., 2011) dataset is a causalreasoning task in which a system is given a premise sentence and two possible alternatives. Thesystem must choose the alternative which has the more plausible causal relationship with the premise.The method used for the construction of the alternatives ensures that the task requires causal reasoningto solve. Examples either deal with alternative possible causes or alternative possible effects of thepremise sentence, accompanied by a simple question disambiguating between the two instancetypes for the model. All examples are handcrafted and focus on topics from online blogs and aphotography-related encyclopedia. Following the recommendation of the authors, we evaluate usingaccuracy.",
    "__tags__": {
        "NLU": true,
        "annotations_creators": "expert-generated",
        "arxiv": "1905.00537",
        "croissant": true,
        "language": "en",
        "language_creators": "other",
        "license": "other",
        "multilinguality": "monolingual",
        "natural language understanding": true,
        "region": "us",
        "size_categories": "10K<n<100K",
        "source_datasets": "extended|other",
        "superglue": true,
        "task_categories": [
            "text-classification",
            "token-classification",
            "question-answering"
        ],
        "task_ids": [
            "natural-language-inference",
            "word-sense-disambiguation",
            "coreference-resolution",
            "extractive-qa"
        ]
    }
}
