{
    "__type__": "task_card",
    "loader": {
        "__type__": "load_hf",
        "path": "basicv8vc/SimpleQA",
        "data_classification_policy": [
            "public"
        ]
    },
    "preprocess_steps": [
        {
            "__type__": "rename",
            "field": "problem",
            "to_field": "question"
        },
        {
            "__type__": "wrap",
            "field": "answer",
            "inside": "list",
            "to_field": "answers"
        }
    ],
    "task": "tasks.qa.open",
    "templates": "templates.qa.open.all",
    "__description__": "A factuality benchmark called SimpleQA that measures the ability for language models to answer short, fact-seeking questions.",
    "__tags__": {
        "annotations_creators": "expert-generated",
        "arxiv": "1904.09728",
        "flags": [
            "NLU",
            "natural language understanding"
        ],
        "language": "en",
        "language_creators": "other",
        "license": "mit",
        "multilinguality": "monolingual",
        "region": "us",
        "size_categories": "10K<n<100K",
        "source_datasets": "extended|other",
        "task_categories": [
            "text-classification",
            "token-classification",
            "question-answering"
        ],
        "task_ids": [
            "natural-language-inference",
            "word-sense-disambiguation"
        ]
    }
}
