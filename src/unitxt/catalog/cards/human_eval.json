{
    "type": "task_card",
    "loader": {
        "type": "load_hf",
        "path": "openai_humaneval",
        "split": "test"
    },
    "preprocess_steps": [
        {
            "type": "execute_expression",
            "expression": "[t for t in re.findall(r\"assert.*?(?=\\n\\s*assert|$)\", test.replace(\"candidate\", entry_point), re.DOTALL)]",
            "imports_list": [
                "re"
            ],
            "to_field": "test_list"
        }
    ],
    "task": {
        "type": "task",
        "inputs": [
            "prompt"
        ],
        "outputs": [
            "prompt",
            "canonical_solution",
            "test_list"
        ],
        "metrics": [
            "metrics.bleu"
        ]
    },
    "templates": {
        "type": "templates_list",
        "items": [
            {
                "type": "input_output_template",
                "input_format": "{prompt}\n",
                "output_format": "{prompt}\n{canonical_solution}"
            }
        ]
    },
    "__tags__": {
        "annotations_creators": "expert-generated",
        "arxiv": "2107.03374",
        "flags": [
            "code-generation"
        ],
        "language": "en",
        "language_creators": "expert-generated",
        "license": "mit",
        "multilinguality": "monolingual",
        "region": "us",
        "size_categories": "n<1K",
        "source_datasets": "original",
        "task_categories": "text2text-generation"
    },
    "__description__": "The HumanEval dataset released by OpenAI includes 164 programming problems with a function sig- nature, docstring, body, and several unit tests. They were handwritten to ensure not to be included in the training set of code generation modelsâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/openai_humaneval."
}
