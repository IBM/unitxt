{
    "type": "task_card",
    "loader": {
        "type": "load_hf",
        "path": "wiki_bio"
    },
    "preprocess_steps": [
        {
            "type": "split_random_mix",
            "mix": {
                "train": "train",
                "validation": "val",
                "test": "test"
            }
        },
        {
            "type": "list_to_key_val_pairs",
            "fields": [
                "input_text/table/column_header",
                "input_text/table/content"
            ],
            "to_field": "kvpairs"
        },
        {
            "type": "serialize_key_val_pairs",
            "field_to_field": [
                [
                    "kvpairs",
                    "input"
                ]
            ]
        },
        {
            "type": "rename_fields",
            "field_to_field": {
                "target_text": "output"
            }
        },
        {
            "type": "add_fields",
            "fields": {
                "type_of_input": "Key-Value pairs",
                "type_of_output": "Text"
            }
        }
    ],
    "task": "tasks.generation",
    "templates": "templates.generation.all",
    "__tags__": {
        "annotations_creators": "found",
        "arxiv": "1603.07771",
        "language": "en",
        "language_creators": "found",
        "license": "cc-by-sa-3.0",
        "multilinguality": "monolingual",
        "region": "us",
        "size_categories": "100K<n<1M",
        "source_datasets": "original",
        "task_categories": "table-to-text"
    },
    "__description__": "This dataset gathers 728,321 biographies from wikipedia. It aims at evaluating text generation algorithms. For each article, we provide the first paragraph and the infobox (both tokenized). For each article, we extracted the first paragraph (text), the infobox (structured data). Each infobox is encoded as a list of (field name, field value) pairs. We used Stanford CoreNLP (http://stanfordnlp.github.io/CoreNLP/) to preprocess the data, i.e. we broke the text into sentences and tokenized both the text and the field valuesâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/wiki_bio"
}
