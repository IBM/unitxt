{
    "__type__": "task_card",
    "loader": {
        "__type__": "load_hf",
        "path": "lmms-lab/DocVQA",
        "name": "InfographicVQA",
        "data_classification_policy": [
            "public"
        ]
    },
    "preprocess_steps": [
        {
            "__type__": "rename_splits",
            "mapper": {
                "validation": "test"
            }
        },
        {
            "__type__": "to_image",
            "field": "image",
            "to_field": "context"
        },
        {
            "__type__": "set",
            "fields": {
                "context_type": "image"
            }
        }
    ],
    "task": "tasks.qa.with_context.abstractive[metrics=[metrics.anls]]",
    "templates": "templates.qa.with_context.all",
    "default_template": {
        "__type__": "multi_reference_template",
        "input_format": "{context}\n{question}\nAnswer the question using a single word or phrase.",
        "references_field": "answers",
        "__description__": "lmms-evals default template for infovqa."
    },
    "__tags__": {
        "license": "apache-2.0",
        "multilinguality": "monolingual",
        "modalities": [
            "image",
            "text"
        ],
        "size_categories": "10K<n<100K",
        "task_categories": "question-answering",
        "task_ids": "extractive-qa"
    },
    "__description__": "InfographicVQA is a dataset that comprises a diverse collection of infographics along with natural language questions and answers annotations. The collected questions require methods to jointly reason over the document layout, textual content, graphical elements, and data visualizations. We curate the dataset with emphasis on questions that require elementary reasoning and basic arithmetic skills."
}
