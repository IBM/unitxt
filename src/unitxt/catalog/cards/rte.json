{
    "__type__": "task_card",
    "loader": {
        "__type__": "load_hf",
        "path": "glue",
        "name": "rte"
    },
    "preprocess_steps": [
        "splitters.small_no_test",
        {
            "__type__": "map_instance_values",
            "mappers": {
                "label": {
                    "0": "entailment",
                    "1": "not entailment"
                }
            }
        },
        {
            "__type__": "set",
            "fields": {
                "classes": [
                    "entailment",
                    "not entailment"
                ],
                "type_of_relation": "entailment",
                "text_a_type": "premise",
                "text_b_type": "hypothesis"
            }
        },
        {
            "__type__": "rename_fields",
            "field_to_field": {
                "sentence1": "text_a",
                "sentence2": "text_b"
            }
        }
    ],
    "task": "tasks.classification.multi_class.relation",
    "templates": "templates.classification.multi_class.relation.all",
    "__tags__": {
        "annotations_creators": "other",
        "arxiv": "1804.07461",
        "flags": [
            "coreference-nli",
            "paraphrase-identification",
            "qa-nli"
        ],
        "language": "en",
        "language_creators": "other",
        "license": "other",
        "multilinguality": "monolingual",
        "region": "us",
        "size_categories": "10K<n<100K",
        "source_datasets": "original",
        "task_categories": "text-classification",
        "task_ids": [
            "acceptability-classification",
            "natural-language-inference",
            "semantic-similarity-scoring",
            "sentiment-classification",
            "text-scoring"
        ]
    },
    "__description__": "The Recognizing Textual Entailment (RTE) datasets come from a series of annual textual entailment challenges. The authors of the benchmark combined the data from RTE1 (Dagan et al., 2006), RTE2 (Bar Haim et al., 2006), RTE3 (Giampiccolo et al., 2007), and RTE5 (Bentivogli et al., 2009)â€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyu-mll/glue."
}
