{
    "__type__": {
        "module": "unitxt.card",
        "name": "TaskCard"
    },
    "loader": {
        "__type__": {
            "module": "unitxt.loaders",
            "name": "MultipleSourceLoader"
        },
        "sources": [
            {
                "__type__": {
                    "module": "unitxt.loaders",
                    "name": "LoadHF"
                },
                "path": "stanford-crfm/air-bench-2024",
                "name": "default"
            },
            {
                "__type__": {
                    "module": "unitxt.loaders",
                    "name": "LoadHF"
                },
                "path": "stanford-crfm/air-bench-2024",
                "data_files": {
                    "judge_prompts": "judge_prompt_final.csv"
                }
            }
        ],
        "data_classification_policy": [
            "public"
        ]
    },
    "preprocess_steps": [
        {
            "__type__": {
                "module": "unitxt.operators",
                "name": "SelectFields"
            },
            "fields": [
                "cate-idx",
                "judge_prompt"
            ],
            "apply_to_streams": [
                "judge_prompts"
            ]
        },
        {
            "__type__": {
                "module": "unitxt.stream_operators",
                "name": "JoinStreams"
            },
            "left_stream": "test",
            "right_stream": "judge_prompts",
            "how": "inner",
            "on": [
                "cate-idx"
            ],
            "new_stream_name": "test"
        },
        {
            "__type__": {
                "module": "unitxt.stream_operators",
                "name": "DeleteSplits"
            },
            "splits": [
                "judge_prompts"
            ]
        }
    ],
    "task": {
        "__type__": {
            "module": "unitxt.task",
            "name": "Task"
        },
        "input_fields": {
            "cate-idx": "str",
            "l2-name": "str",
            "l3-name": "str",
            "l4-name": "str",
            "prompt": "str",
            "judge_prompt": "str"
        },
        "reference_fields": {},
        "prediction_type": "str",
        "metrics": [
            "metrics.granite_guardian.assistant_risk.harm[prediction_type=str,user_message_field=prompt,assistant_message_field=prediction]"
        ]
    },
    "templates": {
        "__type__": {
            "module": "unitxt.templates",
            "name": "TemplatesDict"
        },
        "items": {
            "default": {
                "__type__": {
                    "module": "unitxt.templates",
                    "name": "InputOutputTemplate"
                },
                "input_format": "{prompt}\n",
                "output_format": ""
            }
        }
    },
    "__description__": "AIRBench 2024 is a AI safety benchmark that aligns with emerging government regulations and company policies. It consists of diverse, malicious prompts spanning categories of the regulation-based safety categories in the AIR 2024 safety taxonomy.",
    "__tags__": {
        "languages": [
            "english"
        ],
        "urls": {
            "arxiv": "https://arxiv.org/abs/2407.17436"
        }
    }
}
