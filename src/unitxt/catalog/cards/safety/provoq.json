{
    "__type__": {
        "module": "unitxt.card",
        "name": "TaskCard"
    },
    "loader": {
        "__type__": {
            "module": "unitxt.loaders",
            "name": "LoadHF"
        },
        "path": "IBM/ProvoQ",
        "data_classification_policy": [
            "public"
        ]
    },
    "task": {
        "__type__": {
            "module": "unitxt.task",
            "name": "Task"
        },
        "input_fields": {
            "question": "str",
            "stigma": "str",
            "group": "str"
        },
        "reference_fields": {},
        "prediction_type": "str",
        "metrics": [
            "metrics.llm_as_judge.safety.llama_3_3_70b_instruct_ibm_watsonx_template_harm_rating"
        ]
    },
    "templates": {
        "__type__": {
            "module": "unitxt.templates",
            "name": "TemplatesDict"
        },
        "items": {
            "default": {
                "__type__": {
                    "module": "unitxt.templates",
                    "name": "InputOutputTemplate"
                },
                "input_format": "{question}",
                "output_format": ""
            }
        }
    },
    "__description__": "The ProvoQ dataset is designed to evaluate the sensitivity of large language models (LLMs) to stigma-related topics. It contains a comprehensive set of human-curated provocative questions that systematically target minority-stigma pairs in the United States, creating a diverse and nuanced set of questions that reflect these sensitive topics. The dataset aims to support research in understanding and mitigating biases in AI systems, particularly in the context of minority groups. While most questions are toxic, others may seem benign but potentially elicit harmful responses. The dataset contains questions in text format, organized by minority-stigma pairs.",
    "__tags__": {
        "languages": [
            "english"
        ]
    }
}
