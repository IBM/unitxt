{
    "__type__": "task_card",
    "loader": {
        "__type__": "load_hf",
        "path": "glue",
        "name": "qnli"
    },
    "preprocess_steps": [
        "splitters.large_no_test",
        {
            "__type__": "map_instance_values",
            "mappers": {
                "label": {
                    "0": "entailment",
                    "1": "not entailment"
                }
            }
        },
        {
            "__type__": "set",
            "fields": {
                "classes": [
                    "entailment",
                    "not entailment"
                ],
                "type_of_relation": "entailment",
                "text_a_type": "question",
                "text_b_type": "sentence"
            }
        },
        {
            "__type__": "rename_fields",
            "field_to_field": {
                "question": "text_a",
                "sentence": "text_b"
            }
        }
    ],
    "task": "tasks.classification.multi_class.relation",
    "templates": "templates.classification.multi_class.relation.all",
    "__tags__": {
        "annotations_creators": "other",
        "arxiv": "1804.07461",
        "flags": [
            "coreference-nli",
            "paraphrase-identification",
            "qa-nli"
        ],
        "language": "en",
        "language_creators": "other",
        "license": "other",
        "multilinguality": "monolingual",
        "region": "us",
        "size_categories": "10K<n<100K",
        "source_datasets": "original",
        "task_categories": "text-classification",
        "task_ids": [
            "acceptability-classification",
            "natural-language-inference",
            "semantic-similarity-scoring",
            "sentiment-classification",
            "text-scoring"
        ]
    },
    "__description__": "The Stanford Question Answering Dataset is a question-answering dataset consisting of question-paragraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question (written by an annotator). The authors of the benchmark convert the task into sentence pair classification by forming a pair between each question and each sentence in the corresponding context, and filtering out pairs with low lexical overlap between the question and the context sentenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyu-mll/glue."
}
