{
    "type": "task_card",
    "loader": {
        "type": "load_hf",
        "path": "mbpp",
        "name": "full",
        "split": "test"
    },
    "preprocess_steps": [
        {
            "type": "join_str",
            "field_to_field": {
                "test_list": "test_list_str"
            },
            "separator": "\n"
        }
    ],
    "task": {
        "type": "task",
        "inputs": [
            "text",
            "test_list_str"
        ],
        "outputs": [
            "test_list",
            "code"
        ],
        "metrics": [
            "metrics.bleu"
        ]
    },
    "templates": {
        "type": "templates_list",
        "items": [
            {
                "type": "input_output_template",
                "input_format": "\"\"\"{text}\n\n{test_list_str}\"\"\"",
                "output_format": "{code}"
            }
        ]
    },
    "__tags__": {
        "annotations_creators": [
            "crowdsourced",
            "expert-generated"
        ],
        "arxiv": "2108.07732",
        "flags": [
            "code-generation"
        ],
        "language": "en",
        "language_creators": [
            "crowdsourced",
            "expert-generated"
        ],
        "license": "cc-by-4.0",
        "multilinguality": "monolingual",
        "region": "us",
        "size_categories": "n<1K",
        "source_datasets": "original",
        "task_categories": "text2text-generation"
    },
    "__description__": "The benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases. As described in the paper, a subset of the data has been hand-verified by us. Released here as partâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/mbpp."
}
