{
    "__type__": "task_card",
    "loader": {
        "__type__": "load_hf",
        "path": "glue",
        "name": "wnli"
    },
    "preprocess_steps": [
        {
            "__type__": "split_random_mix",
            "mix": {
                "train": "train[95%]",
                "validation": "train[5%]",
                "test": "validation"
            }
        },
        {
            "__type__": "rename_fields",
            "field": "sentence1",
            "to_field": "text_a"
        },
        {
            "__type__": "rename_fields",
            "field": "sentence2",
            "to_field": "text_b"
        },
        {
            "__type__": "map_instance_values",
            "mappers": {
                "label": {
                    "0": "yes",
                    "1": "no"
                }
            }
        },
        {
            "__type__": "set",
            "fields": {
                "classes": [
                    "yes",
                    "no"
                ]
            }
        },
        {
            "__type__": "set",
            "fields": {
                "type_of_relation": "truthfulness"
            }
        },
        {
            "__type__": "set",
            "fields": {
                "text_a_type": "premise"
            }
        },
        {
            "__type__": "set",
            "fields": {
                "text_b_type": "hypothesis"
            }
        }
    ],
    "task": "tasks.classification.multi_class.relation",
    "templates": "templates.classification.multi_class.relation.truthfulness.all",
    "__tags__": {
        "annotations_creators": "other",
        "arxiv": "1804.07461",
        "flags": [
            "coreference-nli",
            "paraphrase-identification",
            "qa-nli"
        ],
        "language": "en",
        "language_creators": "other",
        "license": "other",
        "multilinguality": "monolingual",
        "region": "us",
        "size_categories": "10K<n<100K",
        "source_datasets": "original",
        "task_categories": "text-classification",
        "task_ids": [
            "acceptability-classification",
            "natural-language-inference",
            "semantic-similarity-scoring",
            "sentiment-classification",
            "text-scoring"
        ]
    },
    "__description__": "The Winograd Schema Challenge (Levesque et al., 2011) is a reading comprehension task in which a system must read a sentence with a pronoun and select the referent of that pronoun from a list of choices. The examples are manually constructed to foil simple statistical methods: Each one is contingent on contextual information provided by a single word or phrase in the sentenceâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/nyu-mll/glue."
}
