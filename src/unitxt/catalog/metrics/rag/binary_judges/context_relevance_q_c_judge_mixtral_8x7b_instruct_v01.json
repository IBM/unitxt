{
    "__type__": "metric_pipeline",
    "main_score": "context_relevance_q_c_judge_mixtral_8x7b_instruct_v01",
    "metric": {
        "__type__": "generative_binary_judge_wml",
        "main_score": "context_relevance_q_c_judge_mixtral_8x7b_instruct_v01",
        "model_name": "mistralai/mixtral-8x7b-instruct-v01",
        "task_name": "tasks.rag_eval.context_relevance.binary",
        "template_name": "templates.rag_eval.context_relevance.judge_context_relevance_ares_logprobs"
    },
    "preprocess_steps": [
        {
            "__type__": "copy",
            "field_to_field": {
                "contexts": "task_data/contexts",
                "ground_truths": "task_data/ground_truths",
                "question": "task_data/question",
                "answer": "task_data/answer"
            },
            "not_exist_ok": true
        },
        {
            "__type__": "set",
            "fields": {
                "prediction": 0.0,
                "references": [
                    0.0
                ]
            }
        }
    ]
}
