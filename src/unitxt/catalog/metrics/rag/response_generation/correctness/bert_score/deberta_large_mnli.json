{
    "__type__": "metric_pipeline",
    "main_score": "recall",
    "preprocess_steps": [],
    "postpreprocess_steps": [
        {
            "__type__": "copy",
            "field_to_field": {
                "score/instance/f1": "score/instance/correctness_f1_bert_score.deberta_large_mnli",
                "score/instance/recall": "score/instance/correctness_recall_bert_score.deberta_large_mnli",
                "score/instance/precision": "score/instance/correctness_precision_bert_score.deberta_large_mnli",
                "score/global/f1": "score/global/correctness_f1_bert_score.deberta_large_mnli",
                "score/global/recall": "score/global/correctness_recall_bert_score.deberta_large_mnli",
                "score/global/precision": "score/global/correctness_precision_bert_score.deberta_large_mnli"
            },
            "not_exist_ok": true
        }
    ],
    "metric": "metrics.bert_score.deberta_large_mnli",
    "prediction_type": "str"
}
