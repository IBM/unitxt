{
    "__type__": "eval_assist_llm_as_judge_pairwise",
    "inference_engine": {
        "__type__": "ibm_gen_ai_inference_engine",
        "model_name": "kaist-ai/prometheus-8x7b-v2",
        "max_new_tokens": 1024,
        "random_seed": 42
    },
    "assessment_template": {
        "__type__": "input_output_template",
        "input_format": "###Task Description:\nA context that includes information relevant to the nature or generation of the response, a response to evaluate, and a score rubric representing a evaluation criteria are given.\n1. Write a detailed feedback that assess the quality of two responses strictly based on the given score rubric, not evaluating in general.\n2. After writing a feedback, choose a better response between Response {option_a} and Response {option_b}. You should refer to the score rubric.\n3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] ({option_a} or {option_b})\"\n4. Please do not generate any other opening, closing, and explanations.\n###Context:\n{context_variables}\n###Response {option_a}:\n{response_a}\n###Response {option_b}:\n{response_b}\n###Score Rubric:\n{criteria}\n###Feedback: \n"
    },
    "summarization_template": null,
    "option_selection_template": {
        "__type__": "input_output_template",
        "input_format": "{assessment_prompt}{assessment} [RESULT] "
    },
    "option_selection_strategy": "PARSE_OPTION_LOGPROB",
    "evaluator_name": "PROMETHEUS"
}
