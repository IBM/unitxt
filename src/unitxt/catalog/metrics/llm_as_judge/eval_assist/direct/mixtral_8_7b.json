{
    "__type__": "eval_assist_llm_as_judge_direct",
    "inference_model": {
        "__type__": "ibm_gen_ai_inference_engine",
        "model_name": "mistralai/mixtral-8x7b-instruct-v01",
        "parameters": {
            "__type__": "ibm_gen_ai_inference_engine_params_mixin",
            "max_new_tokens": 1024,
            "random_seed": 42
        }
    },
    "assessment_template": {
        "__type__": "input_output_template",
        "input_format": "<s> [INST]\nYou are presented with a response generated subject to a context.\nThe context includes information relevant to the nature or generation of the response.\nYou will assess the quality of the response subject to an evaluation criteria.\n\n###Context:\n{context_variables}\n\n###Response:\n{response}\n\n###Evaluation criteria:\n{criteria}\n{options}\n\nBriefly assess the quality of the response subject to the evaluation criteria.\nFocus on the evaluation criteria during assessment, do not provide a general assessment.\n[/INST]\nAssessment:\n"
    },
    "summ_template": {
        "__type__": "input_output_template",
        "input_format": "<s> [INST]\nSummarize the following assessment while maintaining the pertinent details.\nAssessment: {assessment}\n[/INST]\nAssessment Summary:\n"
    },
    "answer_template": {
        "__type__": "input_output_template",
        "input_format": "</s>\n[INST]\nNow consider the evaluation criteria and choose a final answer.\nValidate the answer against the assessment.\n###Evaluation criteria:\n{{criteria}}\n{{options}}\n[/INST]\nAnswer:"
    }
}
