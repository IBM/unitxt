# docs/docs/inference.rst
.. _inference:

==============
Inference
==============

.. note::

   This tutorial requires a :ref:`Unitxt installation <install_unitxt>`.

Introduction
------------
Unitxt offers a wide array of :class:`Inference Engines <unitxt.inference>` for running models either locally (using HuggingFace, Ollama, and VLLM) or by making API requests to services like WatsonX, OpenAI, AWS, and Together AI.

Unitxt inference engines serve two main purposes:

    1. Running a full end-to-end evaluation pipeline with inference.
    2. Using models for intermediate steps, such as evaluating other models (e.g., LLMs as judges) or for data augmentation.

Running Models Locally
-----------------------
You can run models locally with inference engines like:

    - :class:`HFPipelineBasedInferenceEngine <unitxt.inference.HFPipelineBasedInferenceEngine>`
    - :class:`VLLMInferenceEngine <unitxt.inference.VLLMInferenceEngine>`
    - :class:`OllamaInferenceEngine <unitxt.inference.OllamaInferenceEngine>`

To get started, prepare your engine:

.. code-block:: python

    model = HFPipelineBasedInferenceEngine(
        model_name="meta-llama/Llama-3.2-1B", max_new_tokens=32
    )

Then load the data:

.. code-block:: python

    dataset = load_dataset(
        card="cards.xsum",
        template="templates.summarization.abstractive.formal",
        format="formats.chat_api",
        metrics=[llm_judge_with_summary_metric],
        loader_limit=5,
        split="test",
    )

Notice: we create the data with  `format="formats.chat_api"` which produce data as list of chat turns:

.. code-block:: python

    [
        {"role": "system", "content": "Summarize the following Document."},
        {"role": "user", "content": "Document: <...>"}
    ]

Now run inference on the dataset:

.. code-block:: python

    predictions = model(dataset)

Finally, evaluate the predictions and obtain final scores:

.. code-block:: python

    evaluate(predictions=predictions, data=dataset)

Calling Models Through APIs
---------------------------
Calling models through an API is even simpler and is primarily done using one class: :class:`CrossProviderInferenceEngine <unitxt.inference.CrossProviderInferenceEngine>`.

You can create a :class:`CrossProviderInferenceEngine` as follows:

.. code-block:: python

    model = CrossProviderInferenceEngine(
        model="llama-3-2-1b-instruct", provider="watsonx"
    )

This engine supports providers such as ``watsonx``, ``together-ai``, ``open-ai``, ``aws``, ``ollama``, ``bam``, and ``watsonx-sdk``.

It can be used with all supported models listed here: :class:`supported models <unitxt.inference.CrossProviderInferenceEngine>`.

Running inference follows the same pattern as before:

.. code-block:: python

    predictions = model(dataset)

Creating a Cross-API Engine
---------------------------
Alternatively, you can create an engine without specifying a provider:

.. code-block:: python

    model = CrossProviderInferenceEngine(
        model="llama-3-2-1b-instruct"
    )

You can set the provider later by:

.. code-block:: python

    import unitxt

    unitxt.settings.default_provider = "watsonx"

Or by setting an environment variable:

.. code-block:: bash

    export UNITXT_DEFAULT_PROVIDER="watsonx"
# docs/docs/adding_template.rst
.. _adding_template:

.. note::

   To use this tutorial, you need to :ref:`install unitxt <install_unitxt>`.

=====================================
Templates âœ¨
=====================================

In this section you learn how to add a Template. Templates are the way for unitxt to take your task data and verbalize the task instructions to the model.
The templates made by the community can be found in the catalog :ref:`templates section <dir_catalog.templates>`
and the documentation for the base classes used for templates can be found here: :class:`Templates Documentation <unitxt.templates>`

Unitxt Prompt Structure
-----------------------

.. _prompt_layout:
.. image:: ../../assets/prompt_layout.png
   :alt: The unitxt prompt layout
   :width: 75%
   :align: center

As can be seen in the image, the template is in charge of every text
that is task dependent, meaning, in the example it's every text explaining the classification task.

Within the template, there are also different parts:

.. _template_layout:
.. image:: ../../assets/template_layout.png
   :alt: The unitxt template layout
   :width: 75%
   :align: center

Including:

* The task ``instruction``, marked in green, which appears once at the top of the example.

* The ``input_format``, marked in red, formatting the layout of the different fields of the task.

* The ``target_prefix``, marked in yellow, priming the target.

Now that we understand the taxonomy
of the different parts of the template, we can see how to define it in code and add it to the unitxt catalog.

Adding a New Template
----------------------------

In this code example, we will use a translation task with the following task fields in every instance: `text`, `text_type`, `source_language`, `target_language`, and lastly the target `translation`.
We can define a template for this task like this:

.. code-block:: python

    from unitxt.templates import InputOutputTemplate

    template = InputOutputTemplate(
        instruction="In the following task, you translate a {text_type}.",
        input_format="Translate this {text_type} from {source_language} to {target_language}: {text}.",
        target_prefix="Translation: ",
        output_format='{translation}',
    ),


The ``instruction`` attribute defines that part of the prompt that appears once (marked green in the second figure above), 
while the ``input_format`` defines the part of prompt that repeats for 
each in-context learning demonstration and for the final instance (marked red in the second figure above).

The ``output_format`` defines how the reference answer is verbalized as string (marked in purple in the first figure above).   
The InputOutputTemplate assumes there is at most only a single reference (gold answer). 
If you pass a field value which is a list to the InputOutputTemplate, then it is verbalized as comma separated string. For example, ["happy","angry"]
becomes the string reference "happy,angry", and it is expected that the model will return that string as the correct answer.

.. note::
    If you don't have references , just specify ``output_format=""``.
    If you have multiple references, use the MultiReferenceTemplate (see below)
    The only fields that are mandatory are the ``input_format`` and ``output_format``

Post Processors
---------------

The template also defines the post processing steps applied to the output predictions of the model before they are passed to the :ref:`Metrics <adding_metric>`.
Typically, the post processors applied both to the model prediction and to the references. 
For example, we could use the ``processors.lower_case`` processor to lowercase both the model predictions and references,
so the metric computation will ignore case. 

.. code-block:: python

    from unitxt.templates import InputOutputTemplate
    template = InputOutputTemplate(
        instruction="In the following task, you translate a {text_type}.",
        input_format="Translate this {text_type} from {source_language} to {target_language}: {text}.",
        target_prefix="Translation: ",
        output_format='{translation}',
        postprocessors= [
                "processors.lower_case"
            ]
    )

The reason the post processors are set in the template, is because different templates prompt the model to generate answers in different formats. 
For example, one template may prompt the model to answer ``Yes`` or ``No`` while another 
template may prompt the model to answer ``True`` or ``False``. Both can use different post processors to convert them to standard model prediction of `0` or `1`.

Post processors implemented as operators.  Usually they are implemented as fields operators that are applied to the ``prediction``
and ``references``` fields.   When needed, It is possible to add post processors that are applied only to the prediction of the model and not the references or vice versa. 
Here we see how we can lowercase only the model prediction.

.. code-block:: python

    from unitxt.processors import PostProcess
    from unitxt.operators import FieldOperator

    class Lower(FieldOperator):
        def process_value(self, text: Any) -> Any:
            return text.lower()

    from unitxt.templates import InputOutputTemplate
    template = InputOutputTemplate(
        instruction="In the following task, you translate a {text_type}.",
        input_format="Translate this {text_type} from {source_language} to {target_language}: {text}.",
        target_prefix="Translation: ",
        output_format='{translation}',
        postprocessors= [ 
            PostProcess(Lower(),process_references=False)    
        ]
    )

You can see all the available predefined post processors in the catalog (:ref:`Processor <dir_catalog.processors>`.)

Templates for Special Cases
----------------------------

There are different templates for different types of data. For example, for data with many references, we have:

.. code-block:: python

    MultiReferenceTemplate(
        instruction="Answer the question based on the information provided in the document given below. The answer should be a single word, a number, or a short phrase of a few words.\n\n",
        input_format="Document: {context}\nQuestion: {question}",
        target_prefix="Answer: ",
        references_field="answers",
    )

The template uses the list of values in the dataset field defined by the ``references_field`` attribute to define all the references.

You can see all the available predefined templates here: :ref:`Templates Documentation <dir_catalog.templates>`.

Making Your Custom Template
----------------------------

In order to make your own template, you need to create a class inheriting from ``Template`` and
implementing its abstract methods:

.. code-block:: python

     @abstractmethod
    def input_fields_to_source(self, input_fields: Dict[str, object]) -> str:
        """Create the textual input for the model from the input fields"""
        pass

    @abstractmethod
    def reference_fields_to_target_and_references(self, reference_fields: Dict[str, object]) -> Tuple[str, List[str]]:
        """Create a list of references from the reference fields. Also returns one of the references
           as the 'target' - the reference used if the instance is used as a demonstration."
        pass

    

For instance, this template passes all the input fields to the model as a json string.
It also formats the references by taking two of the dataset reference fields: the 'top_answer' and the 'alternative_answer'.

.. code-block:: python

    class MyCustomTemplate(Template):

        def input_fields_to_source(self, inputs_fields: Dict[str, object]) -> str:
            return json.dumps(inputs_fields) # provide the json string with all fields as the input to the model
        def reference_fields_to_target_and_references(self, reference_fields: Dict[str, object]) -> Tuple[str, List[str]]
            return outputs_fields["top_answer"],  # target
                   [outputs_fields["top_answer"],outputs_fields["alternative_answer"]]   # all references

# docs/docs/adding_metric.rst
.. _adding_metric:

.. note::

   To use this tutorial, you need to :ref:`install unitxt <install_unitxt>`.


=====================================
Metrics âœ¨
=====================================

Unitxt supports a large collection of built in metrics, from classifical ones such as
rouge, bleu, f1 to embedding based score like SentenceBert and Bert score, as well as
llm as judges using local or API based models.

You specify the metrics metrics in the Task.

For example:

.. code-block:: python

    task = Task(
            input_fields={"question" : str},
            reference_fields={"answer" : str},
            prediction_type=str,
            metrics=[
                "metrics.rouge",
                "metrics.normalized_sacrebleu",
                "metrics.bert_score.deberta_xlarge_mnli",
                "metrics.bert_score.deberta_large_mnli"
            ],
    )

The full list of built in metrics shows in the :ref:`Metrics part of the catalog <dir_catalog.metrics>`.
In this section we will understand Unitxt metrics and learn how to add new metrics.


Metric Inputs
-------------

Unitxt metrics receive three inputs for each instance:

1. **Prediction** (``prediction``):  The prediction passed to the metric is not the raw textual prediction
returned by the model, but rather the processed prediction, after applying the post processors
defined by the template.  The role of the template's post processors is to convert the output
of the model to the required type of the metrics.  For example, a spearman metric expects a float
prediction.  A post processor in the template will cast the string input to a float, and return NaN
if the string can not be converted to a float.  Another example, a multi-label f1 expects a list of
string class names as predictions.   The post processor may convert the string output into a list
(e.g. by splitting using a separator).

2. **References** (``references`` - optional):  This is a list of gold references, from the same type of the prediction.
For example, if the prediction is a string, the references field is a list of strings.  If the prediction is
a list of strings (e.g in multi-label classification), then the references field is a *list* of lists of strings.
The metric should return a perfect score, if the prediction is equal to one of the references.

3. **Task data** (``task_data`` - optional) - all the input and output fields of a task as a dictionary.
The input fields can be used to create reference-less metrics.



In the rest of the section, we will assume we want to create a new metric for the
task of calculating of the sum of integers (see  :ref:`adding task <task>`.)

It's important that all processing will be done in the template's post processor and not in the metric,
because different templates may require different processing.  For example, one template may request
the model response as a number (e.g. "35") or request a model response in words (e.g. "thirty five").
The metric should receive a single integer.

Metric Outputs
--------------

By default, each metric provides scores for each instance separately and global aggregated scores over all instances together.
The output of the metrics is a nested dictionary per instance.

The scores calculated on instance ``i`` by itself are found in ``results[i]["score"]["instance"]``.
The global scores calculated over all instances are found in ``results[i]["score"]["global"]``.
Note the global scores are the same in all instances, so typically, ``results[0]["score"]["global"]`` is used to get the global scores.

A metric could return multiple scores, but it should always return a field called ``score`` with the main score of the metric,
and ``score_name`` which is the name of the main score.

For example, the score list for an instance could be:

.. code-block:: python

    {
        "sum_accuracy_approximate": 0.0,
        "score": 1.0,
        "score_name": "sum_accuracy_approximate"
    }

The global scores are calculated over all instances.

Metrics can also calculate confidence intervals for the global scores.
This gives you an assessment of the inherient noise in the scores.  When you compare runs on same data, check if their confidence
intervals overlap. If so, the difference may not be statistically significant.

.. code-block:: python

    {
        "sum_accuracy_approximate": 0.67,
        "score": 0.67,
        "score_name": "sum_accuracy_approximate",
        "sum_accuracy_approximate_ci_low": 0.53,
        "sum_accuracy_approximate_ci_high": 0.83,
        "score_ci_low": 0.53,
        "score_ci_high": 0.83,
    }

Metric Outputs with Multiple Metrics
------------------------------------

When multiple metrics are specified, their scores are appended to the score list.
If multiple metrics have the same score names, the score of the metric that appears first in the metrics list has precedence.

If you want to avoid the scores being overwritten by other metrics, you can add a prefix to each metric score.

.. code-block:: python

    task = Task(
        ...
        metrics=[
            "metrics.rouge",
            "metrics.normalized_sacrebleu",
            "metrics.bert_score.deberta_xlarge_mnli[score_prefix=sbert_deberta_xlarge_mnli_]",
            "metrics.bert_score.deberta_large_mnli[score_prefix=sbert_deberta_large_mnli_]"
            ],
    )

Note that the ``score`` and ``score_name`` are always taken from the first metric in the ``metrics`` list.

Metric Base Classes
-------------------

As described in the previous section, a metric generates a set of scores per instance (called ``instance`` scores),
and a set of scores over all instances (called ``global`` scores).

Unitxt has several base classes, subclasses of class :class:`Metric <unitxt.metric.Metric>`, that simplify the creation 
of metrics, depending on how the scores are calculated.

:class:`InstanceMetric <unitxt.metrics.InstanceMetric>` - Class for metrics in which the global scores are calculated by aggregating the instance scores.
Typically, the global score is the average of all instance scores. :class:`InstanceMetric <unitxt.metrics.InstanceMetric>` first evaluates each instance separately,
and then aggregates the scores of the instances. Some examples of instance metrics are ``Accuracy``, ``TokenOverlap``, ``CharEditDistance``.

:class:`BulkInstanceMetric <unitxt.metrics.BulkInstanceMetric>` - Similar to :class:`InstanceMetric <unitxt.metrics.InstanceMetric>`, it is for metrics 
in which the global score can be calculated by aggregating over the instance scores.  However,
for the sake of efficient implementation, it's better to run them in bulks (for example, when using LLMs during score calculations).
A ``BulkInstanceMetric`` calculates the instance scores of a batch of instances each time, but then aggregates over the scores of all the instances.
Some examples of bulk instance metrics are ``SentenceBert``, ``Reward``.

:class:`GlobalMetric <unitxt.metrics.GlobalMetric>` - Class for metrics for which the global scores must be calculated over all the instances together.
Some examples of global metrics are ``f1``, ``Spearman``, ``Kendall Tau``.  Note that by default, global metrics are executed once per instance
to generate per instance scores, and then once again over all instances together. So if there are 100 instances,
it will first be called 100 times, each on a single instance, and then one time on all 100 instances.

Instance scores of ``GlobalMetrics`` are useful for error-analysis. Consider ``f1`` score, for example.
It can be calculated only on all instances together. Yet it is useful to report the score of every instance
so you can see that good instances get ``f1`` score of 1 and bad ones get 0.

   .. note::
    By default global metrics are also executed once per instance as list (of size one),
    to generate per instance scores that are useful for debugging and sanity checks.

Adding a New Instance metric
----------------------------

Assume we want to create a referenceless metric for the task of adding two numbers.
It will take the processed prediction of the task (an integer) and compare to the sum of the
two task input fields ``num1`` and ``num2``.  It will check, for each instance,
how close the predicted sum is to the actual sum.
The metric can be configured with a ``relative_tolerance`` threshold for approximate comparison.
If the difference between the prediction and actual result is smaller than the ``relative_tolerance``
threshold, the instance score is 1. Otherwise, the instance result is 0.
The global accuracy result is the mean of the instance scores.

.. code-block:: python

    class SumAccuracy(InstanceMetric):

        main_score = "sum_accuracy" # name of the main score
        reduction_map = {"mean": ["sum_accuracy"]} # defines that the global score is a mean of the instance scores
        ci_scores = ["sum_accuracy"] # define that confidence internal should be calculated on the score
        prediction_type = int      # the metric expect the prediction as an int

        # Relation tolerance for errors by default it is 0, but can be changed for approximate comparison
        relative_tolerance : float = 0

        def compute(
            self, references: List[int], prediction: int, task_data: List[Dict]
        ) -> dict:
            actual_sum = task_data["num1"] + task_data["num2"]
            isclose_enough =  isclose(actual_sum, prediction, rel_tol=self.relative_tolerance)
            result = { self.main_score : 1.0 if isclose_enough else 0.0}
            return result

To verify that our metric works as expected we can use unitxt built in testing suit:

.. code-block:: python

    #
    # Test SumAccuracy metric and add to catalog
    #

    from unitxt_extension_example.metrics import SumAccuracy
    metric = SumAccuracy()

    predictions = [3, 799 , 50]
    references = [[5],[800],[50]]
    task_data = [{"num1" : 2, "num2" : 3}, {"num1" : 300, "num2" : 500}, {"num1" : -25, "num2" : 75}]
    instance_targets = [
        {"sum_accuracy": 0.0, "score": 0.0, "score_name": "sum_accuracy"},
        {"sum_accuracy": 0.0, "score": 0.0, "score_name": "sum_accuracy"},
        {"sum_accuracy": 1.0, "score": 1.0, "score_name": "sum_accuracy"},
    ]

    global_target = {
        "sum_accuracy": 0.33,
        "score": 0.33,
        "score_name": "sum_accuracy",
        "sum_accuracy_ci_low": 0.0,
        "sum_accuracy_ci_high": 1.0,
        "score_ci_low": 0.0,
        "score_ci_high": 1.0,
    }

    outputs = test_metric(
        metric=metric,
        predictions=predictions,
        references=references,
        instance_targets=instance_targets,
        global_target=global_target,
        task_data=task_data
    )

    add_to_catalog(metric, "metrics.sum_accuracy")

Adding a Global Metric
----------------------

Now let's consider a global reference based metric that checks if accuracy depends on the magnitude of the results.
For example, is more accurate when the result is 1 digits vs 5 digits.
To check this, we will see if there is a correlation between the number of digits in the reference value and the accuracy.
This is a global metric because it performs the calculation over all the instance predictions and references together.

.. code-block:: python

    class SensitivityToNumericMagnitude(GlobalMetric):
    """
    SensitiveToNumericMagnitude is a reference-based metric that calculates if accuracy depends
    on the numeric magnitude of the reference value.  It receives integer prediction values and integer reference values
    and calculates the correlation between the number of digits in the reference values and the accuracy
    (whether predictions=references).

    The score is negative (up to -1), if predictions tend to be less accurate when reference values are larger.
    The score is close to 0, if the magnitude of the reference answer does not correlate with accuracy.
    The score is positive (up to 1), if predictions tend to be less accurate when reference values are smaller.

    In most realistic cases, the score is likely to be zer or negative.

    """
    prediction_type = int
    main_score="sensitivity_to_numeric_magnitude"
    single_reference_per_prediction = True  # validates only one reference is passed per prediction

    def compute(
        self, references: List[List[int]], predictions: List[int], task_data: List[Dict]
    ) -> dict:
        import scipy.stats as stats # Note the local import to ensure import is required only if metric is actually used
        magnitude = [ len(str(abs(reference[0]))) for reference in references ]
        accuracy = [ reference[0] == prediction  for (reference, prediction) in zip(references, predictions) ]
        spearman_coeff, p_value =  stats.spearmanr(magnitude, accuracy)
        result = { self.main_score :  spearman_coeff }
        return result



1. Calculating confidence intervals for global metrics can be costly if each invocation of the metric takes a long time.
To avoid calculating confidence internals for global metrics set ``n_resamples = 0``.

2. Unitxt calculates instance results in global metrics to allow viewing the output on a single instances.
This can help ensure metric behavior is correct, because it can be checked on single instance.
However, sometimes it does not make sense because the global metric assumes a minimum amount of instances.
The per instance calculations can be disabled by setting ``process_single_instances = False``.

Managing Metric Dependencies
----------------------------

If a metric depends on an external package (beyond the unitxt dependencies),
use of ``_requirements_list`` allows validating the package is installed and provides instructions to the users if it is not.

.. code-block:: python

    _requirements_list = { "sentence_transformers" : "Please install sentence_transformers using  'pip install -U sentence-transformers'" }

To ensure the package is imported only if the metric is actually used, include the import inside the relevant methods and not in global scope of the file.

Using Metric Pipelines
----------------------

Unitxt metrics must be compatible with the task they are used with.  However, sometime there is an implementation
of a metric that performs the needed calculations but expects different inputs.
The :class:`MetricPipeline <unitxt.metrics.MetricPipeline>` is a way to adapt an existing metric to a new task.
For example, the :class:`TokenOverlap <unitxt.metrics.TokenOverlap>` metric takes a string input prediction and a string references and calculates
the token overlap between them. If we want to reuse it, in a ``Retrieval Augmented Generation`` task to measure the token
overlap between the predictions and the context, we can define a ``MetricPipeline`` to copy the ``context`` field of the task
to the ``references`` field.  Then it runs the existing metric. Finally, it renames the scores to more meaningful names.

.. code-block:: python

    metric = MetricPipeline(
        main_score="score",
        preprocess_steps=[
            Copy(field="task_data/context", to_field="references"),
            ListFieldValues(fields=["references"], to_field="references"),
        ],
        metric="metrics.token_overlap",
        postprocess_steps=[
            Rename(
                field_to_field=[
                    ("score/global/f1", "score/global/f1_overlap_with_context"),
                    ("score/global/recall", "score/global/recall_overlap_with_context"),
                    (
                        "score/global/precision",
                        "score/global/precision_overlap_with_context",
                    ),
                ],
            ),
        ],
    )
    add_to_catalog(metric, "metrics.token_overlap_with_context", overwrite=True)

Adding a Hugginface metric
--------------------------

Unitxt provides a simple way to wrap existing Huggingface metrics without the need to write code.
This is done using the predefined :class:`HuggingfaceMetric <unitxt.metrics.HuggingfaceMetric>` class.

.. code-block:: python

    metric = HuggingfaceMetric(
        hf_metric_name="bleu",  # The name of the metric in huggingface
        main_score="bleu",      # The main score (assumes the metric returns this score name)
        prediction_type=str   # The type of the prediction and references (note that by default references are a list of the prediction_type)
    )
    add_to_catalog(metric, "metrics.bleu", overwrite=True)

By default, the HuggingfaceMetric wrapper passes only the ``prediction`` and ``references`` fields to
the metrics. You can also pass fields from the ``task_data`` inputs, by specifying ``hf_additional_input_fields``.
For example:

.. code-block:: python

    metric = HuggingfaceMetric(
        ...
        hf_additional_input_fields_pass = ["num1","num2"], # passes the task's num1 and num2 fields
        ...

    )

In the above example, ``num1`` and ``num2`` fields are passed as lists of values to the metric
(each element in the list corresponds to an instance). If you want to pass a scalar (single) value to the metric
you can use:

.. code-block:: python

    metric = HuggingfaceMetric(
        ...
        hf_additional_input_fields_pass_one_value=["tokenize"],
        ...
    )


This assumes the field has the same value is in all instances.


Note that ``Huggingface`` metrics are independent from the tasks they are used for, and receive arbitrary types of predictions, references, and additional
parameters.  A mapping may be needed between unitxt field names, values and types to the corresponding interface of the metric, using
the ``MetricPipeline`` described in the previous section.

.. note::

   Use HuggingfaceMetric to wrap metrics defined in Huggingface Hub. Do not use it to wrap Huggingface metrics implemented
   in local files.  This is because local metrics are accessed via relative or absolute file paths, and both
   may not be relevant if running code on different machines or root directories.
# docs/docs/llm_as_judge.rst
.. _llm_as_judge:

.. note::

    To follow this tutorial, ensure you have :ref:`unitxt installed <install_unitxt>`.

=====================================
LLM as a Judge Metrics Guide ðŸ“Š
=====================================

This section will walk you through harnessing the power of LLM as judge (LLMaJ) metrics using the Unitxt package. LLM as a judge
provides a method to assess the performance of a model based on the judgments of another model.

When to use LLM as Judge
------------------------

LLMs as judges are most useful when
    1. You don't have ground truth (references) to compare with
    2. When you have ground truth, but comparing the ground truth to the model response is non-trivial (e.g. requires semantic understanding)
    3. When you want to assess specific properties of the model's output that can easily expressed via an LLM prompt (e.g. does the model response contain profanity).

Disadvantages of LLM as Judge
-----------------------------

While LLMs as Judges are powerful and effective in many cases, they have some drawbacks:
    1. Good LLM as Judges are often large models with relatively high inference latency.
    2. Deploying large LLMs is difficult and may require API access to external services.
    3. Not all LLMs (including large ones) can serve as good judges - their assessment may not correlate with human judgements and can also be biased.
       This means that unless you have a prior indication that the LLM you use is a good judge for your task, you need to evaluate its judgements and see they match your expectations.


Using LLMs
-----------
In this guide, we'll explore three key aspects of LLMaJ:
    1. Utilizing LLM as judge as a metric in Unitxt.
    2. Incorporating a new LLM as a judge metric into Unitxt.
    3. Assessing the quality of an LLM as a judge metric.

But first, let's start with an overview:

Overview
---------

An LLM as a Judge metric consists of several essential components:

1. The judge model, such as *Llama-3-8B-Instruct* or *gpt-3.5-turbo*, which evaluates the performance of other models.
2. The platform responsible for executing the judge model, such as Huggingface or OpenAI API.
3. The template used to construct prompts for the judge model. This template should be reflective of the judgment needed and usually incorporates both the input and output of the evaluated model. For instance:

    .. code-block:: text

        Please rate the clarity, coherence, and informativeness of the following summary on a scale of 1 to 10\\n Full text: {model_input}\\nSummary: {model_output}

4. The format in which the judge model expects to receive prompts. For example:

    .. code-block:: text

        <INST>{input}</INST>

5. Optionally, a system prompt to pass to the judge model. This can provide additional context for evaluation.

Understanding these components is crucial for effectively leveraging LLM as a judge metrics. With this foundation, let's examine  how to utilize and create these metrics in the Unitxt package.

Using LLM as a Judge in Unitxt
-------------------------------
Employing a pre-defined LLM as a judge metric is effortlessly achieved within Unitxt.

The Unitxt catalog boasts a variety of preexisting LLM as judges that seamlessly integrate into your workflow.

Let's consider an example of evaluating a *flan-t5-small* model on the MT-Bench benchmark, specifically utilizing the single model rating evaluation part of the benchmark. In this part, we provide the LLM as a Judge, the input provided to the model and the output it generation. The LLM as Judge is asked to rate how well the output of the model address the request in the input.

To accomplish this evaluation, we require the following:

1. A Unitxt dataset card containing MT-Bench inputs, which will serve as the input for our evaluated model.
2. A Unitxt template to be paired with the card. As the MT-Bench dataset already includes full prompts, there is no need to construct one using a template; hence, we'll opt for the *empty* template, which just passes the input prompt from the dataset to the model.
3. A unitxt format to be utilized with the card. Given that *flan* models do not demand special formatting of the inputs, we'll utilize the *empty* format here as well.
4. An LLM as a judge metric leveraging the MT-Bench evaluation prompt.

Fortunately, all these components are readily available in the Unitxt catalog, including a judge model based on *Mistral* from Huggingface that employs the MT-Bench format.
From here, constructing the full unitxt recipe string is standard and straightforward:

.. code-block:: text

    card=cards.mt_bench.generation.english_single_turn,
    template=templates.empty,
    format=formats.empty,
    metrics=[metrics.llm_as_judge.rating.mistral_7b_instruct_v0_2_huggingface_template_mt_bench_single_turn]

.. note::

   Pay attention!
   We are using the mistralai/Mistral-7B-Instruct-v0.2 model from Huggingface. Using this model requires you to agree to the Terms of Use on the model page and set the HUGGINGFACE_TOKEN environment argument. Other platforms might have different requirements. For example if you are using an LLM as judge based on the OpenAI platform, you will need to set your OpenAI api key.


The following code performs the desired evaluation:

.. code-block:: python

    from datasets import load_dataset
    from unitxt.inference import HFPipelineBasedInferenceEngine
    from unitxt import evaluate

    # 1. Create the dataset
    card = ("card=cards.mt_bench.generation.english_single_turn,"
            "template=templates.empty,"
            "format=formats.empty,"
            "metrics=[metrics.llm_as_judge.rating.mistral_7b_instruct_v0_2_huggingface_template_mt_bench_single_turn]"
            )

    dataset = load_dataset("unitxt/data",
                           card,
                           split='test')
    # 2. use inference module to infer based on the dataset inputs.
    model = HFPipelineBasedInferenceEngine(model_name="google/flan-t5-small", max_new_tokens=32, use_fp16=True)
    predictions = model(dataset)

    # 3. create a metric and evaluate the results.
    results = evaluate(predictions=predictions, data=dataset)

    print(results.global_scores.summary)



Creating a new LLM as a Judge Metric
-------------------------------------

To construct a new LLM as a Judge metric, several key components must be defined:

1. **Judge Model**: Select a model that will assess the performance of other models.
2. **Execution Platform**: Choose the platform responsible for executing the judge model, such as Huggingface or OpenAI API.
3. **The Judging Task**: This define the inputs the judge model expect to receive and its output. This is coupled with the template. Two common tasks are single model rating we saw above and pairwise model comparison, in which the outputs of two models is compared, to see which better addressed the required input.
4. **Template**: Develop a template reflecting the criteria for judgment, usually incorporating both the input and output of the evaluated model.
5. **Format**: Specify the format in which the judge model expects to receive prompts.
6. **System Prompt (Optional)**: Optionally, include a system prompt to provide additional context for evaluation.

Let's walk through an example of creating a new LLM as a Judge metric, specifically recreating the MT-Bench judge metric single-model-rating evaluation:

1. **Selecting a Judge Model**: We will utilize the *mistralai/Mistral-7B-Instruct-v0.2* model from Huggingface as our judge model.
2. **Selecting an Execution Platform**: We will opt to execute the model locally using Huggingface.

    For this example, we will use the *HFPipelineBasedInferenceEngine* class:

    .. code-block:: python

        from unitxt.inference import HFPipelineBasedInferenceEngine
        from unitxt.llm_as_judge import LLMAsJudge

        model_id = "mistralai/Mistral-7B-Instruct-v0.2"
        inference_model = HFPipelineBasedInferenceEngine(model_name=model_id, max_generated_tokens=256)


    .. note::

        If you wish to use a different platform for running your judge model, you can implement
        a new `InferenceEngine` class and substitute it with the `HFPipelineBasedInferenceEngine`.
        You can find the definition of the `InferenceEngine` abstract class and pre-built inference engines
        (e.g., `OpenAiInferenceEngine`) in `src/unitxt/inference.py`.


3. **Selecting the Judging Task**: This is a standard Unitxt task that defines the api of the judge model. The task specifies the input fields expected by the judge model, such as "question" and "answer," in the example below, which are utilized in the subsequent template. Additionally, it defines the expected output field as a float type. Another significant field is "metrics," which is utilized for the (meta) evaluation of the judge, as explained in the following section. Currently supported tasks are "rating.single_turn" and "rating.single_turn_with_reference".

    .. code-block:: python

        from unitxt.blocks import Task
        from unitxt.catalog import add_to_catalog

        add_to_catalog(
            Task(
                inputs={"question": "str", "answer": "str"},
                outputs={"rating": "float"},
                metrics=["metrics.spearman"],
            ),
            "tasks.response_assessment.rating.single_turn",
            overwrite=True,
        )

4. **Define the Template**: We want to construct a template that is identical to the MT-Bench judge metric. Pay attention that this metric have field that are compatible with the task we chose ("question", "answer" and "rating").

    .. code-block:: python

        from unitxt import add_to_catalog
        from unitxt.templates import InputOutputTemplate

        add_to_catalog(
            InputOutputTemplate(
                instruction="Please act as an impartial judge and evaluate the quality of the response provided"
                " by an AI assistant to the user question displayed below. Your evaluation should consider"
                " factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of"
                " detail of the response. Begin your evaluation by providing a short explanation. Be as"
                " objective as possible. After providing your explanation, you must rate the response"
                ' on a scale of 1 to 10 by strictly following this format: "[[rating]]", for example:'
                ' "Rating: [[5]]".\n\n',
                input_format="[Question]\n{question}\n\n"
                "[The Start of Assistant's Answer]\n{answer}\n[The End of Assistant's Answer]",
                output_format="[[{rating}]]",
                postprocessors=[
                    r"processors.extract_mt_bench_rating_judgment",
                ],
            ),
            "templates.response_assessment.rating.mt_bench_single_turn",
            overwrite=True,
        )

    .. note::

        Ensure the template includes a postprocessor for extracting the judgment from the judge model output and
        passing it as a metric score. In our example, the template specifies for the judge the expected judgment format
        ("you must rate the response on a scale of 1 to 10 by strictly following this format: "[[rating]]""),
        and such, it also defines the processor for extracting the judgment. (postprocessors=[r"processors.extract_mt_bench_rating_judgment"],).
        This processor simply extract the number within [[ ]] and divide it by 10 in order to scale to to [0, 1].


5. **Define Format**: Define the format expected by the judge model for receiving prompts. For Mitral models, you can use the format already available in the Unitxt catalog under *"formats.models.mistral.instruction""*.

6. **Define System Prompt**: We will not use a system prompt in this example.

With these components defined, creating a new LLM as a Judge metric is straightforward:

.. code-block:: python

    from unitxt import add_to_catalog
    from unitxt.inference import HFPipelineBasedInferenceEngine
    from unitxt.llm_as_judge import LLMAsJudge

    model_id = "mistralai/Mistral-7B-Instruct-v0.2"
    format = "formats.models.mistral.instruction"
    template = "templates.response_assessment.rating.mt_bench_single_turn"
    task = "rating.single_turn"

    inference_model = HFPipelineBasedInferenceEngine(
        model_name=model_id, max_new_tokens=256, use_fp16=True
    )
    model_label = model_id.split("/")[1].replace("-", "_").replace(".", "_").lower()
    model_label = f"{model_label}_huggingface"
    template_label = template.split(".")[-1]
    metric_label = f"{model_label}_template_{template_label}"
    metric = LLMAsJudge(
        inference_model=inference_model,
        template=template,
        task=task,
        format=format,
        main_score=metric_label,
    )

    add_to_catalog(
        metric,
        f"metrics.llm_as_judge.rating.{model_label}_template_{template_label}",
        overwrite=True,
    )



.. note::

    The *LLMAsJudge* class can receive the boolean argument *strip_system_prompt_and_format_from_inputs*
    (defaulting to *True*). When set to *True*, any system prompts or formatting in the inputs received by
    the evaluated model will be stripped.

Evaluating a LLMaJ metric (Meta-evaluation)
--------------------------------------------
But wait, we missed a step! We know the LLM as a judge we created worth anything?
The answer is: You evaluate it like any other model in Unitxt.
Remember the task we defined in the previous section?

    .. code-block:: python

        from unitxt.blocks import Task
        from unitxt.catalog import add_to_catalog

        add_to_catalog(
            Task(
                inputs={"question": "str", "answer": "str"},
                outputs={"rating": "float"},
                metrics=["metrics.spearman"],
            ),
            "tasks.response_assessment.rating.single_turn",
            overwrite=True,
        )

This task define the (meta) evaluation of our LLMaJ model.
We will fetch a dataset of MT-Bench inputs and models outputs, together with scores judged by GPT-4.
We will consider these GPT4 scores as our gold labels and evaluate our LLMaJ model by comparing its score on the model outputs
to the score of GPT4 using spearman correlation as defined in the task card.

We will create a card, as we do for every other Unitxt scenario:

.. code-block:: python

    from unitxt.blocks import (
        TaskCard,
    )
    from unitxt.catalog import add_to_catalog
    from unitxt.loaders import LoadHF
    from unitxt.operators import (
        Copy,
        FilterByCondition,
        Rename,
    )
    from unitxt.processors import LiteralEval
    from unitxt.splitters import RenameSplits
    from unitxt.test_utils.card import test_card

    card = TaskCard(
        loader=LoadHF(path="OfirArviv/mt_bench_single_score_gpt4_judgement", split="train"),
        preprocess_steps=[
            RenameSplits({"train": "test"}),
            FilterByCondition(values={"turn": 1}, condition="eq"),
            FilterByCondition(values={"reference": "[]"}, condition="eq"),
            Rename(
                field_to_field={
                    "model_input": "question",
                    "score": "rating",
                    "category": "group",
                    "model_output": "answer",
                }
            ),
            LiteralEval(field="question"),
            Copy(field="question/0", to_field="question"),
            LiteralEval(field="answer"),
            Copy(field="answer/0", to_field="answer"),
        ],
        task="tasks.response_assessment.rating.single_turn",
        templates=["templates.response_assessment.rating.mt_bench_single_turn"],
    )

    test_card(card, demos_taken_from="test", strict=False)
    add_to_catalog(
        card,
        "cards.mt_bench.response_assessment.rating.single_turn_gpt4_judgement",
        overwrite=True,
    )

This is a card for the first turn inputs of the MT-Bench benchmarks (without reference),
together with the outputs of multiple models to those inputs and the scores of GPT-4
to those outputs.

Now all we need to do is to load the card, with the template and format the judge model is expected to use,
and run it.

.. code-block:: python

    from datasets import load_dataset
    from unitxt.inference import HFPipelineBasedInferenceEngine
    from unitxt import evaluate

    # 1. Create the dataset
    card = ("card=cards.mt_bench.response_assessment.rating.single_turn_gpt4_judgement,"
            "template=templates.response_assessment.rating.mt_bench_single_turn,"
            "format=formats.models.mistral.instruction")

    dataset = load_dataset("unitxt/data",
                           card,
                           split='test')
    # 2. use inference module to infer based on the dataset inputs.
    model = HFPipelineBasedInferenceEngine(model_name="mistralai/Mistral-7B-Instruct-v0.2",
                                                     max_new_tokens=256,
                                                     use_fp16=True)
    predictions = model(dataset)
    # 3. create a metric and evaluate the results.
    results = evaluate(predictions=predictions, data=dataset)

    print(results.global_scores.summary)

The output of this code is:

.. code-block:: text

    ('spearmanr', 0.18328402960291354)
    ('score', 0.18328402960291354)
    ('score_name', 'spearmanr')
    ('score_ci_low', 0.14680574316651868)
    ('score_ci_high', 0.23030798909064645)
    ('spearmanr_ci_low', 0.14680574316651868)
    ('spearmanr_ci_high', 0.23030798909064645)

We can see the Spearman correlation is *0.18*, which is considered low.
This means *"mistralai/Mistral-7B-Instruct-v0.2"* is not a good model to act as an LLM as a Judge,
at least when using the MT-Bench template.

In order to understand precisely why it is so, examination of the outputs of the model is needed.
In this case, it seems Mistral is having difficulties outputting the scores in the double square brackets format.
An example for the model output is:

.. code-block:: text

    Rating: 9

    The assistant's response is engaging and provides a good balance between cultural experiences and must-see attractions in Hawaii. The description of the Polynesian Cultural Center and the Na Pali Coast are vivid and evoke a sense of wonder and excitement. The inclusion of traditional Hawaiian dishes adds depth and authenticity to the post. The response is also well-structured and easy to follow. However, the response could benefit from a few more specific details or anecdotes to make it even more engaging and memorable.

# docs/docs/demo.rst
.. _demo:

==============
Explore Unitxt
==============
Explore the existing assets in the Unitxt catalog and generate your prompt.

.. raw:: html

    <div class="gradio-loading">
        <div class="centered">
            <div class="spinner"></div>
            <h2>Loading...</h2>
        </div>
    </div>
    <div class="wide-box-container">
        <div class="wide-box">
            <gradio-app src="https://unitxt-ui.hf.space" theme_mode="light" eager="true" info="false"></gradio-app>
        </div>
    </div>

# docs/docs/rag_support.rst
.. _rag_support:
 
=====================================
RAG Support âœ¨
=====================================

.. _rag_intro:

Introduction
============
Retrieval-Augmented Generation (RAG) pipelines consist of the following steps:

#. The user submits a question.
#. The question is sent as a query to a retrieval system to obtain relevant texts addressing the question.
#. A generative model is prompted to answer the user question based on the retrieved content. 
#. The generated answer is returned to the user.

RAG evaluation covers the performance of both the retrieval system and the generative model. We refer to the output of the retrieval system as ``retrieved texts`` or ``contexts`` (an ordered list of texts with ids) and the output of the generative model as ``generated answer`` (a text string). 


.. _rag_task:

Task
======
RAG Task Definition
===================

This definition outlines the inputs, outputs, and reference fields necessary for executing the RAG Task.

**Inputs:**
  - `question` (str): The user's question.
  - `question_id` (str): A unique identifier for the question.
  - `metadata_field` (str): Optional metadata, such as the question category.

**Outputs:**
  - `contexts` (List[str]): Texts retrieved from the index for the question.
  - `contexts_id` (List[str]): Unique IDs of the retrieved contexts.
  - `answer` (str): The answer generated by the RAG pipeline for the question.

**Reference-Fields:**
  - `reference_answers` (List[str]): Ground truth answers for the question.
  - `reference_contexts` (List[str]): Ground truth retrieved texts for the question.
  - `reference_context_ids` (List[str]): Unique IDs of the ground truth contexts.
  - `is_answerable_label` (bool): Indicator whether the question is answerable or not.


.. _rag_metrics:

Metrics
========
RAG evaluation is performed using both automatic `reference-based` and `reference-less` metrics. The figure below displays a variety of automatic metrics for RAG, organized with pointers to their inputs and implementations. Detailed explanations of these metrics are provided below the figure.

.. image:: ../../assets/rag/metrics_slide.png
   :alt: RAG Metrics
   :width: 100%
   :align: center


.. _context_relevance:

Context-Relevance
-----------------
This is a reference-less metric gauging the relevance of the retrieved texts to answering the user question. The metric range is [0, 1], where higher is better. 

* Motivation and Approach 

We contend that in a comparative setup, in which two texts compete to serve as context to answer a user's question, the text which is more likely to evoke this question is more relevant to it. 

Thus, we use a perplexity score, which uses an LLM to act as a Judge to determine the probability a retrieved text will evoke the user's question. When applied to a set of texts, the maximum score is returned as ``Context Relevance``. 

By computing ``Context Relevance`` over results from different vector stores and retrieval setups, it is possible to conclude which setup provides results that are more relevant to answering the user's question.

* Implementation Details

We employ a small LLM - ``google/flan-t-5-small`` - that is known to show strong results in a faithfulness assessment, and we prompt it with the instruction ``Generate a question based on the given content:`` followed by one retrieved text at a time. As the model generates the question iteratively, token by token, we employ a teacher forcing strategy that uses the tokens from the actual question as ground truth. Thus, at each step, the model uses the ground-truth tokens as input rather than the output from previous steps, and predicts the probability of generating the next ground-truth token. The geometric mean over these probabilities defines the perplexity of the retrieved text.

* Limitations and Future Plans

In future releases we will add a list of complementary metrics ``Context Relevance @ K`` for $K = {1, 3, 5, ...}$ that are computed by averaging the perplexity scores of the top-K retrieved texts. This will be useful for assessing the ranking of the retrieval. After all, normally in RAG applications only the top results from the search are passed to the LLM for generating an answer.

-----

.. _context_correctness:

Context Correctness
-------------------
This is a reference-based metric reflecting the rank of the ground-truth text in the retrieved texts. The metric range is [0, 1] where higher is better.

* Motivation and Approach 
  
The metric aims to indicate to what extent the retrieved context aligns with the ground truth context. As a proxy, we use the id associated with each retrieved text and match it against the id(s) of ground-truth text(s). Since the order of the retrieved texts is significant, we assign higher scores to results that rank the ground-truth text(s) higher than other texts.

* Implementation Details

We use the well known `Mean Reciprocal Rank <https://en.wikipedia.org/wiki/Mean_reciprocal_rank>`_ (MRR) metric from Information Retrieval. RR is computed by ``1/rank``, where ``rank`` is the 1-based position of the first ground-truth text id found in the retrieval results. If no such text is found, the metric returns ``0``. 

* Limitations and Future Plans

Currently, the metric only uses the associated ids to match the results against ground truth. We plan to add a metric that assesses context correctness based on content as well, possibly employing LLM as a Judge. 

Another issue with the current metric is that only the top-ranked ground truth is used in the metric score. It does not penalize the retrieval for assigning a low rank to other ground truths. In the future this will be mitigated by supplementing MRR by the `Mean Average Precision <https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision>`_ (MAP) metric.  

------------------

.. _faithfulness:

Faithfulness
------------
This is a reference-less metric gauging the groundedness of the generated answer in the retrieved texts. The metric range is [0, 1], where higher is better.

* Motivation and Approach

We based our approach on `Adlakha et. al (2023) <https://arxiv.org/abs/2307.16877>`_ - "Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering", which found that fast and inexpensive lexical analysis can provide a relatively high correlation with human judgement on faithfulness. 

Table 4 from the paper is provided below, showing that the `K-Precision` lexical approach is close to GPT-4. The main advantage of lexical strategies over the LLM as a Judge strategy is that they are easy to implement, fast to run, and inexpensive to deploy (in other words, they do not require GPUs). 

.. image:: ../../assets/rag/adlaka_table4.png
   :alt: Table 2 of Adlakha et. al (2023)
   :width: 50%
   :align: center


* Implementation Details

The `K-Precision` ("Knowledge Precision") metric that is mentioned in the paper has been part of public open source projects for a long time, and now it is also adopted in the Unitxt package for computing faithfulness scores. 

The metric is essentially token precision: we count how many of the generated tokens in the system response are included in the context retrieved from the index.

* Limitations and Future Plans

Lexical strategies look at words in isolation, ignoring word order and context. This is clearly a suboptimal approach that can lead to inaccurate assessments in many cases. We plan to switch to a more robust LLM as a Judge approach once we have models that can offer a better trade-off between speed, cost and quality. 

------------

.. _answer_reward:

Answer Reward
-------------
This is a reference-less metric that predicts which generated answer is better judged by a human, given a question. The metric range is [0, 1], where higher is better.

* Motivation and Approach
  
When it comes to the assessment of answer quality, we typically see an attempt to characterize this abstract property using various more basic and apparently well-defined aspects, such as: factual correctness, naturalness, appropriateness, conciseness, faithfulness, relevance, and clarity, among others. However, due to the convoluted interrelationships between these properties, labeling each one of them in isolation effectively and consistently by humans is a non-trivial task that is hardly practical. It requires an exhaustive and well-defined, but also clear and intuitive, annotation scheme. It also requires long-term training and monitoring of the labelers.

As a counter approach, the holistic view on quality aims to characterize this property using simple, direct questions in a realistic scenario. For example, in the comparative setup, instead of asking human labelers to rate answers by various abstract properties as mentioned above and then somehow mixing all the scores together to conclude which answer is better, it does something else: it directly asks the labelers to indicate which answer is better in the use case in which the answer is to be given (e.g. a chatbot about enterprise HR policies). 

The underlying assumption here is that the labeler implicitly considers all fine-grained properties like naturalness, conciseness, faithfulness, etc. and favors the overall better answer.

For completeness, in a non-comparative setup, the holistic approach could, for example, ask the labeler to indicate if he/she would recommend the answer to a friend who asks the question, appealing, as in the comparative setup, to overall judgement.  

This is the approach taken by reward models that learn from holistic judgements on quality. In this way we bypass the need to distinguish between non-mutually exclusive properties, or to deal with how to fuse scores of such properties. We leave it to the human labelers to do the breaking and fusing.  

* Implementation Details
  
The implementation utilizes a reward model - `OpenAssistant/reward-model-deberta-v3-large-v2 <https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2>`_ by `OpenAssistant <https://open-assistant.io/bye>`_. The model was trained to predict "which generated answer is better judged by a human, given a question". The training data comes from four datasets: (a) ``webgpt_comparisons``, (b) ``summarize_from_feedback``, (c) ``synthetic-instruct-gptj-pairwise``, and (d) ``anthropic_hh-rlhf``.

Although the model was trained in a comparative setup (one question, multiple answers), it is commonly used as a regression model that scores each question and answer individually.

* Limitations and Future Plans
  
The reward model provides a meaningful signal on the quality of answers, but in some cases pinpointing specific qualities such as relevance is desired. In the future, we plan to add metrics that address these qualities. 

------

.. _answer_correctness:

Answer Correctness
------------------

This is a reference-based metric gauging the similarity between the generated answer to a gold answer. The metric range is [0, 1], where higher is better.

* Motivation and Approach
  
As with [Faithfulness](#Faithfulness), we based our approach on `Adlakha et. al (2023) <https://arxiv.org/abs/2307.16877>`_, who reported a relatively high correlation of lexical strategies with human judgement on answer correctness.

Table 2 from the paper is provided below. The results indicate that the `Recall` lexical approach is close to GPT 3.5 and GPT-4 while being easier to implement, faster to run and inexpensive to deploy. 

.. image:: ../../assets/rag/adlaka_table2.png
   :alt: Table 2 of Adlakha et. al (2023)
   :width: 50%
   :align: center


* Implementation Details
  
As with `K-Precision`, `Recall` has been part of public open source projects for a while. We included a common implementation in the Unitxt package for computing answer correctness scores. 

The metric is essentially token recall: we count how many of the ground-truth response tokens are included in the generated response. 

* Limitations and Future Plans
  
See :ref:`Faithfulness <faithfulness>`.

# docs/docs/types_and_serializers.rst
.. _types_and_serializers:

=====================================
Types and Serializers
=====================================

Unitxt employs various tools for serializing data into textual format. One of these mechanisms is the Type-Serializers mechanism, which assigns serialization to specific types. For example, consider the following ``typing`` types:

.. code-block:: python

    from typing import NewType, TypedDict, Union, Literal, List, Any

    Text = NewType("Text", str)
    Number = NewType("Number", Union[float, int])

    class Turn(TypedDict):
        role: Literal["system", "user", "agent"]
        content: Text

    Dialog = NewType("Dialog", List[Turn])

    class Table(TypedDict):
        header: List[str]
        rows: List[List[Any]]

For each type, we can assign different serialization methods specific to that type. This enables us to "plug and play" different serialization methods and modify our dataâ€™s textual representation accordingly.

Registering the Types
-----------------------

First, we need to register the different types we want to support from the Python ``typing`` types defined above.

.. code-block:: python

    from unitxt.type_utils import register_type

    register_type(Text)
    register_type(Number)
    register_type(Turn)
    register_type(Dialog)
    register_type(Table)

Defining a Serializer for a Type
---------------------------------

Once the types are registered, we can define serializers for those types. For example, consider creating a serializer for the ``Dialog`` type:

.. code-block:: python

    from unitxt.serializers import SingleTypeSerializer

    class DialogSerializer(SingleTypeSerializer):
        serialized_type = Dialog

        def serialize(self, value: Dialog, instance: Dict[str, Any]) -> str:
            # Convert the Dialog into a string representation, typically combining roles and content
            return "\n".join(f"{turn['role']}: {turn['content']}" for turn in value)

Using the New Serializer
--------------------------

To use the new serializer, we need to do two things:
1. Ensure our task supports this type.
2. Add the serializer to the data loading recipe.

Using the New Type in a Task
----------------------------

Once the new type is registered, we can create a task that requires this type:

.. code-block:: python

    from unitxt.task import Task

    dialog_summarization_task = Task(
        input_fields={"dialog": Dialog},
        reference_fields={"summary": str},
        prediction_type=str,
        metrics=["metrics.rouge"],
    )

Loading Data with the Serializer
---------------------------------

Once the task is defined with the type, we can use the serializer when loading the data.

Given this standalone card:

.. code-block:: python

    data = {
        "test": [
            {
                "dialog": [
                    {"role": "user", "content": "What is the time?"},
                    {"role": "system", "content": "4:13 PM"},
                ],
                "summary": "User asked for the time and got an answer."
            }
        ]
    }

    card = TaskCard(
        loader=LoadFromDictionary(data=data),
        task=dialog_summarization_task,
    )

We can load the data with the serializer using:

.. code-block:: python

    dataset = load_dataset(
        card=card,
        template=InputOutputTemplate(
            instruction="Summarize the following dialog.",
            input_format="{dialog}",
            output_format="{summary}",
        ),
        serializer=DialogSerializer(),
    )

Now if you print the input of the first instance of the dataset by ``print(dataset["test"][0]["source"])`` you will get:

.. code-block::

    Summarize the following dialog.
    user: What is the time?
    system: 4:13 PM



Adding a Serializer to a Template
------------------------------------

Another option is to set a default serializer for a given template. When creating a template, we need to add all the serializers for all the types we want to support. For this purpose, we use a multi-type serializer that wraps all the serializers together.

.. code-block:: python

    from unitxt.serializers import (
        MultiTypeSerializer, ImageSerializer, TableSerializer, DialogSerializer, ListSerializer,
    )

    serializer = MultiTypeSerializer(
        serializers=[
            ImageSerializer(),
            TableSerializer(),
            DialogSerializer(),
            ListSerializer(),
        ]
    )

Now, we can add them to the template:

.. code-block:: python

    InputOutputTemplate(
        instruction="Summarize the following dialog.",
        input_format="{dialog}",
        output_format="{summary}",
        serializer=serializer
    )

Important: Serializers are activated in the order they are defined, in a "first in, first serve" manner. This means that if you place the ``ListSerializer`` before the ``DialogSerializer``, the `ListSerializer` will serialize the dialog, as the ``Dialog`` is also a ``List`` and matches the type requirement of the ``ListSerializer``.
# docs/docs/examples.rst
.. _examples:

========
Examples
========

Here you will find complete coding samples showing how to perform different tasks using Unitxt.
Each example comes with a self contained python file that you can run and later modify.


Basic Usage
------------

Evaluate an existing dataset from the Unitxt catalog. 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Demonstrates how to evaluate an existing entailment dataset using Unitxt.
Unitxt is used to load the dataset, generate the input to the model, run inference and evaluate the results.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_existing_dataset_with_install.py>`__

Related documentation: :ref:`Installation <install_unitxt>` , :ref:`WNLI dataset card in catalog <catalog.cards.wnli>`, :ref:`Relation template in catalog <catalog.templates.classification.multi_class.relation.default>`, :ref:`Inference Engines <inference>`.


Evaluate a custom dataset
+++++++++++++++++++++++++

This example demonstrates how to evaluate a user QA answering dataset in a standalone file using a user-defined task and template.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/standalone_qa_evaluation.py>`__

Related documentation: :ref:`Add new dataset tutorial <adding_dataset>`.

Evaluate a custom dataset - reusing existing catalog assets
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

This example demonstrates how to evaluate a user QA dataset using the predefined open qa task and templates.
It also shows how to use preprocessing steps to align the raw input of the dataset with the predefined task fields.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/qa_evaluation.py>`__

Related documentation: :ref:`Add new dataset tutorial <adding_dataset>`, :ref:`Open QA task in catalog <catalog.tasks.qa.open>`, :ref:`Open QA template in catalog <catalog.templates.qa.open.title>`, :ref:`Inference Engines <inference>`.

Evaluate a custom dataset - with existing predictions
=====================================================

These examples demonstrate how to evaluate a datasets of different tasks when predictions are already available and no inference is required.

`Example code for QA task  <https://github.com/IBM/unitxt/blob/main/examples/evaluate_qa_dataset_with_given_predictions.py>`__

`Example code for classification task  <https://github.com/IBM/unitxt/blob/main/examples/evaluate_classification_dataset_with_given_predictions.py>`__  

Related documentation: :ref:`Evaluating datasets <evaluating_datasets>`

Evaluate a Named Entity Recognition (NER) dataset
===================================================

This example demonstrates how to evaluate a named entity recognition task.
The ground truth entities are provided as spans within the provided texts, 
and the model is prompted to identify these entities.
Classifical f1_micro, f1_macro, and per-entity-type f1 metrics are reported.

Example code <https://github.com/IBM/unitxt/blob/main/examples/ner_evaluation.py>`__

Related documentation: :ref:`Add new dataset tutorial <adding_dataset>`, :ref:`Open NER task in catalog <catalog.tasks.ner.all_entity_types>`, :ref:`Inference Engines <inference>`.

Evaluation usecases
-----------------------

Evaluate the impact of different templates and in-context learning demonstrations
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

This example demonstrates how different templates and the number of in-context learning examples impacts the performance of a model on an entailment task.
It also shows how to register assets into a local catalog and reuse them.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_different_templates.py>`__

Related documentation: :ref:`Templates tutorial <adding_template>`, :ref:`Formatting tutorial <adding_format>`, :ref:`Using the Catalog <using_catalog>`, :ref:`Inference Engines <inference>`.

Evaluate the impact of different formats and system prompts
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

This example demonstrates how different formats and system prompts affect the input provided to a llama3 chat model and evaluate their impact on the obtained scores.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_different_formats.py>`__

Related documentation: :ref:`Formatting tutorial <adding_format>`.

Evaluate the impact of different demonstration example selections
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

This example demonstrates how different methods of selecting the demonstrations in in-context learning affect the results.
Three methods are considered: fixed selection of example demonstrations for all test instances,
random selection of example demonstrations for each test instance,
and choosing the demonstration examples most (lexically) similar to each test instance.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_different_demo_selections.py>`__

Related documentation: :ref:`Formatting tutorial <adding_format>`.

Evaluate dataset with a pool of templates and some number of demonstrations
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

This example demonstrates how to evaluate a dataset using a pool of templates and a varying number of in-context learning demonstrations. It shows how to sample a template and specify the number of demonstrations for each instance from predefined lists.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_different_templates_num_demos.py>`__

Related documentation: :ref:`Templates tutorial <adding_template>`, :ref:`Formatting tutorial <adding_format>`, :ref:`Using the Catalog <using_catalog>`, :ref:`Inference Engines <inference>`.

Long Context
+++++++++++++++++++++++++++++

This example explores the effect of long context in classification.
It converts a standard multi class classification dataset (sst2 sentiment classification),
where single sentence texts are classified one by one, to a dataset
where multiple sentences are classified using a single LLM call.
It compares the f1_micro in both approaches on two models.
It uses serializers to verbalize and enumerated list of multiple sentences and labels.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_batched_multiclass_classification.py>`__

Related documentation:  :ref:`Sst2 dataset card in catalog <catalog.cards.sst2>` :ref:`Types and Serializers Guide <types_and_serializers>`.

Construct a benchmark of multiple datasets and obtain the final score
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

This example shows how to construct a benchmark that includes multiple datasets, each with a specific template. It demonstrates how to use these templates to evaluate the datasets and aggregate the results to obtain a final score. This approach provides a comprehensive evaluation across different tasks and datasets.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_benchmark.py>`__

Related documentation: :ref:`Benchmarks tutorial <adding_benchmark>`, :ref:`Formatting tutorial <adding_format>`, :ref:`Using the Catalog <using_catalog>`, :ref:`Inference Engines <inference>`.

LLM as Judges
--------------

Evaluate an existing dataset using a predefined LLM as judge
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

This example demonstrates how to evaluate an existing QA dataset (squad) using the HuggingFace Datasets and Evaluate APIs and leveraging a predefine LLM as a judge metric.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_existing_dataset_by_llm_as_judge.py>`__

Related documentation: :ref:`Evaluating datasets <evaluating_datasets>`, :ref:`LLM as a Judge Metrics Guide <llm_as_judge>`, :ref:`Inference Engines <inference>`.

Evaluate a custom dataset using a custom LLM as Judge
+++++++++++++++++++++++++++++++++++++++++++++++++++++

This example demonstrates how to evaluate a user QA answering dataset in a standalone file using a user-defined task and template. In addition, it shows how to define an LLM as a judge metric, specify the template it uses to produce the input to the judge, and select the judge model and platform.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/standalone_evaluation_llm_as_judge.py>`__

Related documentation: :ref:`LLM as a Judge Metrics Guide <llm_as_judge>`.

Evaluate an existing dataset from the catalog comparing two custom LLM as judges
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

This example demonstrates how to evaluate a document summarization dataset by defining an LLM as a judge metric, specifying the template it uses to produce the input to the judge, and selecting the judge model and platform.
The example adds two LLM judges, one that uses the ground truth (references) from the dataset and one that does not.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_summarization_dataset_llm_as_judge.py>`__

Related documentation: :ref:`LLM as a Judge Metrics Guide <llm_as_judge>`.

Evaluate the quality of an LLM as judge
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

This example demonstrates how to evaluate an LLM as judge by checking its scores using the gold references of a dataset.
It checks if the judge consistently prefers correct outputs over clearly wrong ones.
Note that to check the the ability of the LLM as judge to discern suitable differences between
partially correct answers requires more refined tests and corresponding labeled data.
The example shows an 8b llama based judge is not a good judge for a summarization task,
while the 70b model performs much better.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_llm_as_judge.py>`__

Related documentation: :ref:`LLM as a Judge Metrics Guide <llm_as_judge>`, :ref:`Inference Engines <inference>`.


Evaluate your model on the Arena Hard benchmark using a custom LLMaJ
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

This example demonstrates how to evaluate a user model on the Arena Hard benchmark, using an LLMaJ other than the GPT4.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_a_model_using_arena_hard.py>`__

Related documentation: :ref:`Evaluate a Model on Arena Hard Benchmark <dir_catalog.cards.arena_hard>`, :ref:`Inference Engines <inference>`.

Evaluate a judge model performance judging the Arena Hard Benchmark
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

This example demonstrates how to evaluate the capabilities of a user model, to act as a judge on the Arena Hard benchmark.
The model is evaluated on its capability to give a judgment that is in correlation with GPT4 judgment on the benchmark.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_a_judge_model_capabilities_on_arena_hard.py>`__

Related documentation: :ref:`Evaluate a Model on Arena Hard Benchmark <dir_catalog.cards.arena_hard>`, :ref:`Inference Engines <inference>`.

Evaluate using ensemble of LLM as a judge metrics
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

This example demonstrates how to create a metric which is an ensemble of LLM as a judge metrics.
The example shows how to ensemble two judges which uses different templates.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_using_metrics_ensemble.py>`__

Related documentation: :ref:`LLM as a Judge Metrics Guide <llm_as_judge>`, :ref:`Inference Engines <inference>`.

Evaluate predictions of models using pre-trained ensemble of LLM as judges
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

This example demonstrates how to use a pre-trained ensemble model or an off-the-shelf LLM as judge to assess multi-turn conversation quality of models on a set of pre-defined metrics.

Topicality: Response of the model only contains information that is related to and helpful for the user inquiry.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_ensemble_judge.py>`__

Groundedness: Every substantial claim in the response of the model is derivable from the content of the document

`Example code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_grounded_ensemble_judge.py>`__

IDK: Does the model response say I don't know?

`Example code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_idk_judge.py>`__

Related documentation: :ref:`LLM as a Judge Metrics Guide <llm_as_judge>`, :ref:`Inference Engines <inference>`.

RAG
---

Evaluate RAG response generation
++++++++++++++++++++++++++++++++

This example demonstrates how to use the standard Unitxt RAG response generation task.
The response generation task is the following:
Given a question and one or more context(s), generate an answer that is correct and faithful to the context(s).
The example shows how to map the dataset input fields to the RAG response task fields
and use the existing metrics to evaluate model results.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_rag_response_generation.py>`__

Related documentation: :ref:`RAG Guide <rag_support>`, :ref:`Response generation task <catalog.tasks.rag.response_generation>`, :ref:`Inference Engines <inference>`.

Evaluate RAG End to End - with existing predictions
=====================================================

This example demonstrates how to evaluate an end to end RAG system, given that the RAG system outputs are available.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_rag_end_to_end_dataset_with_given_predictions.py>`__

Related documentation: :ref:`Evaluating datasets <evaluating_datasets>`

Multi-Modality
--------------

Evaluate Image-Text to Text Model
+++++++++++++++++++++++++++++++++
This example demonstrates how to evaluate an image-text to text model using Unitxt.
The task involves generating text responses based on both image and text inputs. This is particularly useful for tasks like visual question answering (VQA) where the model needs to understand and reason about visual content to answer questions.
The example shows how to:

    1. Load a pre-trained image-text model (LLaVA in this case)
    2. Prepare a dataset with image-text inputs
    3. Run inference on the model
    4. Evaluate the model's predictions

The code uses the document VQA dataset in English, applies a QA template with context, and formats it for the LLaVA model. It then selects a subset of the test data, generates predictions, and evaluates the results.
This approach can be adapted for various image-text to text tasks, such as image captioning, visual reasoning, or multimodal dialogue systems.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_image_text_to_text.py>`__

Related documentation: :ref:`Multi-Modality Guide <multi_modality>`, :ref:`Inference Engines <inference>`.


Evaluate Image-Text to Text Model With Different Templates
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Evaluate Image-Text to Text Models with different templates and explore the sensitivity of the model to different textual variations.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_image_text_to_text_with_different_templates.py>`__

Related documentation: :ref:`Multi-Modality Guide <multi_modality>`, :ref:`Inference Engines <inference>`.

Advanced topics
----------------------------

Custom Types and Serializers
+++++++++++++++++++++++++++++

This example show how to define new data types as well as the way these data type should be handled when processed to text.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/custom_types.py>`__

Related documentation: :ref:`Types and Serializers Guide <types_and_serializers>`, :ref:`Inference Engines <inference>`.


Evaluate an existing dataset from the Unitxt catalog (No installation)
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

This example demonstrates how to evaluate an existing entailment dataset (wnli) using HuggingFace Datasets and Evaluate APIs, with no installation required.

`Example code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_existing_dataset_no_install.py>`__

Related documentation:  :ref:`Evaluating datasets <evaluating_datasets>`, :ref:`WNLI dataset card in catalog <catalog.cards.wnli>`, :ref:`Relation template in catalog <catalog.templates.classification.multi_class.relation.default>`, :ref:`Inference Engines <inference>`.

# docs/docs/tags_and_descriptions.rst
.. _tags_and_descriptions:

=====================================
Tags and Descriptions
=====================================

Artifacts in the Unitxt catalog, such as datasets, templates, formats, and operators, can be valuable to others. To help others discover and understand them, Unitxt provides an option to add tags and descriptions to catalog assets.

To search for a catalog asset by tag or description, use the text-based search located in the top-right corner of the Unitxt website.

Adding Descriptions and Tags for Catalog Assets
-----------------------------------------------

Each Unitxt asset has two dedicated fields for tags and descriptions: `__tags__` and `__description__`.

You can assign their values while constructing a Unitxt asset and then save them to the catalog.

For example, if you want to add tags and a description to the `wikitq` dataset card:

.. code-block:: python

    card = TaskCard(
        loader=LoadHF(path="wikitablequestions"),
        task="tasks.qa.with_context.extractive",
        templates="templates.qa.with_context.all",
        __description__="The WikiTableQuestions dataset is a large-scale dataset for the task of question answering on semi-structured tables.",
        __tags__={
            "modality": "table",
            "urls": {"arxiv": "https://arxiv.org/abs/1508.00305"},
            "languages": ["english"],
        },
    )

You can then save the card to the catalog with:

.. code-block:: python

    add_to_catalog(card, "cards.wikitq", overwrite=True)

As a result, the description and tags will appear on the catalog webpage for `cards.wikitq`, as seen here: :ref:`WikiTQ <catalog.cards.wikitq>`.

Editing Existing Assets to Add Tags or Descriptions
---------------------------------------------------

High-quality tags and descriptions enrich the Unitxt catalog.

You can find the preparation code for any Unitxt asset at: https://github.com/IBM/unitxt/tree/main/prepare

Choose a Unitxt asset, add information about it, and then submit a Pull Request with your changes.

How to Write Good Descriptions and Tags
---------------------------------------

1. **Description**: Keep it brief and include all essential information needed to understand the asset's purpose.
2. **Tags**: Classify the asset based on its main aspects, such as `domain`, `task`, `language`, `modality`, tested `skill`, etc.

# docs/docs/lm_eval.rst
.. _lm-eval:

===========================
Running Unitxt with LM-Eval
===========================

Unitxt can be seamlessly integrated with the `LM-Evaluation-Harness <https://github.com/EleutherAI/lm-evaluation-harness>`_, enabling the selection and evaluation of models from the extensive lm-evaluation-harness models catalog using data recipes created by Unitxt.

Installation
------------

To begin, install lm-evaluation-harness from the source (a set version will be available in the future):

.. code-block:: bash

    pip install git+https://github.com/EleutherAI/lm-evaluation-harness

Define Your Unitxt Recipe
-------------------------

Next, choose your preferred Unitxt recipe:

.. code-block:: python

    card=cards.wnli,template=templates.classification.multi_class.relation.default

If you are uncertain about your choice, you can utilize the :ref:`Explore Unitxt <demo>` tool for an interactive recipe exploration UI. After making your selection, click on "Generate Prompts," and then navigate to the "Code" tab. You will see a code snippet similar to the following:

.. code-block:: python

    dataset = load_dataset('unitxt/data', 'card=cards.wnli,template=templates.classification.multi_class.relation.default,max_train_instances=5', split='train')

The second string parameter to `load_dataset()` is the recipe. Note that you may want to remove `max_train_instances=5` from the recipe before using it. If you wish to employ few-shot in-context learning, configure this using the `num_demos` and `demos_pool_size` parameters instead, e.g., `num_demos=5,demos_pool_size=10`.

Set Up Your Custom LM-Eval Unitxt Tasks Directory
-------------------------------------------------

First, create a directory:

.. code-block:: bash

    mkdir ./my_tasks

Next, run the following code to save the Unitxt configuration file in your tasks directory:

.. code-block:: bash

    python -c 'from lm_eval.tasks.unitxt import task; import os.path; print("class: !function " + task.__file__.replace("task.py", "task.Unitxt"))' > ./my_tasks/unitxt

You will now have a `unitxt` file in your `./my_tasks` directory that defines the integration with your local virtual environment. This step should be performed once. Note that when changing virtual environments, you will need to update it using the code above.

You can designate your task as `my_task` and save it in any folder as `./my_tasks/my_task.yaml` in a YAML file:

.. code-block:: yaml

    task: my_task
    include: unitxt
    recipe: card=cards.wnli,template=templates.classification.multi_class.relation.default

Select the model you wish to evaluate from the diverse types of models supported by the lm-evaluation-harness platform (See a comprehensive list `here <https://github.com/EleutherAI/lm-evaluation-harness?tab=readme-ov-file#model-apis-and-inference-servers>`_).

Execute your newly constructed task with:

.. code-block:: bash

    lm_eval --model hf \
        --model_args pretrained=google/flan-t5-base \
        --device cpu --tasks my_task --include_path ./my_tasks

# docs/docs/helm.rst
.. _helm:

========================
Running Unitxt with HELM
========================

.. _prompt_format_layout:
.. image:: ../../assets/helm+unitxt.png
   :alt: HELM + Unitxt
   :width: 75%
   :align: center

Unitxt can be integrated with `HELM <https://github.com/stanford-crfm/helm/>`__, enabling you to select and evaluate models from the extensive HELM models catalog with data recipes created by Unitxt.

First, install HELM at version v0.5.0 or later:

.. code-block:: bash

    pip install --upgrade 'crfm-helm[unitxt]>=0.5.0'

Next, define your preferred Unitxt recipe:

.. code-block:: bash

    recipe="card=cards.wnli,template=templates.classification.multi_class.relation.default"

If you're unsure about your choice, you can use the :ref:`Explore Unitxt <demo>` tool for an interactive recipe exploration UI. After making your selection, click on Generate Prompts, and then click on the Code tab. You will see a code snippet such as the following:

.. code-block:: python

    dataset = load_dataset('unitxt/data', 'card=cards.wnli,template=templates.classification.multi_class.relation.default,max_train_instances=5', split='train')

The second string parameter to ``load_dataset()`` is the recipe. Note that you will have to remove ``max_train_instances=5`` from the recipe before using it in HELM, as the ``max_train_instances`` parameter is not supported when using Unitxt in HELM. If you wish to use few-shot in-context learning, you should configure this using the ``num_demos`` and ``demos_pool_size`` parameters instead, e.g., ``num_demos=5,demos_pool_size=10``.

Select the model you wish to evaluate from the HELM catalog (for a comprehensive list, refer to: https://crfm-helm.readthedocs.io/en/latest/models/):

.. code-block:: bash

    model="openai/gpt2"

To execute the evaluation, combine the components with the following command:

.. code-block:: bash

    helm-run \
        --run-entries "unitxt:$recipe,model=$model" \
        --max-eval-instances 10 --suite v1

Unitxt also supports evaluating models available on the HuggingFace Hub:

.. code-block:: bash

    model="stanford-crfm/alias-gpt2-small-x21"
    helm-run \
        --run-entries "unitxt:$recipe,model=$model" \
        --enable-huggingface-models $model \
        --max-eval-instances 10 --suite v1

To summarize the results of all runs within the created suite, use:

.. code-block:: bash

    helm-summarize --suite v1

To view the aggregated results look at ``benchmark_output/runs/v1/unitxt:$recipe,model=${model/\//_}/stats.json``

Finally, to review the predictions in your web browser, execute:

.. code-block:: bash

    helm-server



# docs/docs/adding_operator.rst
.. _adding_operator:

.. note::

   To use this tutorial, you need to :ref:`install unitxt <install_unitxt>`.

=====================================
Operators âœ¨
=====================================

Operators are specialized functions designed to process data.

They are used in the TaskCard for preparing data for specific tasks and by Post Processors
to process the textual output of the model to the expect input of the metrics. 

There are several types of operators. 

1. Field Operators - Operators that modify individual fields of the instances in the input streams.  Example of such operators are operators that
cast field values, uppercase string fields, or translate text between languages.

2. Instance Operators - Operators that modify individual instances in the input streams. For example, operators that add or remove fields.

3. Stream Operators - Operators that perform operations on full streams. For example, operators that remove instances based on some condition.

4. MultiStream Operators - Operator that perform operations on multiple streams.  For example, operators that repartition the instances between train and test splits.

Unitxt comes with a large collection of built in operators - that were design to cover most common requirements of dataset processing.

The list of available operators can be found in the :ref:`operators <operators_list>` section.

Built in operators have some benefits:

1. **Testability**: Built-in Operators are unit-tested, ensuring reliability and functionality.
2. **Code Reusability**: Shared, well-maintained operator code can be reused across projects.
3. **Performance**: Built-in Operators are designed to maintain high performance standards, particularly suitable for stream processing.
4. **Security** : Built-in Operators do not require running of arbitrary user code, and hence can be run in secured environments that prohibit running user code.

It is recommended to use existing operators when possible. 

However, if a highly specific or uncommon operation is needed that existing operators do not cover, and it is unlikely to be reused, you can use :class:`ExecuteExpression <unitxt.operators.ExecuteExpression>` or :class:`FilterByExpression <unitxt.operators.FilterByExpression>` operators:

.. code-block:: python

    ExecuteExpression('question + "?"', to_field="question")
    FilterByExpression('len(question) == 0')

**Explanation**: These lines demonstrate how to use two specific operators for string manipulation and conditional filtering.

In addition, this tutorial will now guide you on creating new operators in Python for personal use and community contribution.

Field Operators
---------------

To manipulate a single field, inherit from :class:`FieldOperator <operator.FieldOperator>` and implement your manipulation in the `process` method:

.. code-block:: python

    from unitxt.operator import FieldOperator

    class AddNumber(FieldOperator):
        number: float

        def process(self, value):
            return value + self.number

**Explanation**: This class adds a specified number to the input value. It inherits from `FieldOperator` which is designed to operate on a single field.

Usage example:

.. code-block:: python

    operator = AddNumber(number=5, field="price", to_field="new_price")

**Explanation**: This creates an instance of `AddNumber` to add 5 to the `price` field and store the result in `new_price`.

.. note::

    Every :class:`Operator <operator.Operator>` has a `process_instance` function that can be used for debugging. For example, using `AddNumber` implemented above:

    .. code-block:: python

        operator.process_instance({"price": 0.5})
        # Output: {"price": 0.5, "new_price": 5.5}

**Explanation**: This example demonstrates how to debug the `AddNumber` operator by manually processing a sample instance.

Instance Operators
-------------------

Instance operators process data instance by instance. You can access and manipulate the entire instance directly:

.. code-block:: python

    from unitxt.operator import InstanceOperator

    class Join(InstanceOperator):
        fields: List[str]
        separator: str = ""
        to_field: str

        def process(self, instance: Dict[str, Any], stream_name: str = None) -> Dict[str, Any]:
            instance[self.to_field] = self.separator.join([instance[field] for field in self.fields])
            return instance

**Explanation**: This operator joins multiple fields into a single string, separated by a specified delimiter, and stores the result in another field.

Usage example:

.. code-block:: python

    operator = Join(fields=["title", "text"], separator="\n", to_field="context")

**Explanation**: This operator instance will concatenate the `title` and `text` fields with a newline and store the result in `context`.

Example command output:

.. code-block:: python

    operator.process_instance({"title": "Hello!", "text": "World!"})
    # Output: {"title": "Hello!", "text": "World!", "context": "Hello!\nWorld!"}

**Explanation**: This shows the output of the `Join` operator when processing a sample instance.

Stream Operators
----------------

Stream operators are designed to manage and manipulate entire data streams. These operators process instances sequentially, allowing for operations that affect the entire stream, such as limiting the number of instances processed.

.. code-block:: python

    from unitxt.stream import Stream
    from unitxt.operator import StreamOperator

    class LimitSize(StreamOperator):
        size: int
        def process(self, stream: Stream, stream_name: Optional[str] = None) -> Generator:
            for i, instance in enumerate(stream):
                if i > self.size:
                    break
                yield instance

**Explanation**: The `LimitSize` class inherits from `StreamOperator` and is used to limit the number of instances processed in a stream. It iterates over each instance in the stream and stops yielding new instances once the specified size limit is exceeded. This operator is useful for scenarios such as data sampling or when resource constraints limit the number of instances that can be processed.

MultiStream Operators
---------------------

MultiStream operators handle operations across multiple data streams concurrently. These operators are capable of merging, filtering, or redistributing data from multiple streams into a new stream configuration.

.. code-block:: python

    from unitxt.stream import MultiStream, GeneratorStream
    from unitxt.operator import MultiStreamOperator

    class MergeAllStreams(MultiStreamOperator):

        def merge(self, streams) -> Generator:
            for stream in streams:
                for instance in stream:
                    yield instance

        def process(self, multi_stream: MultiStream) -> MultiStream:
            return MultiStream(
                {
                    "merged": GeneratorStream(
                        self.merge, gen_kwargs={"streams": multi_stream.values()}
                    )
                }
            )

**Explanation**: The `MergeAllStreams` class extends `MultiStreamOperator` and provides functionality to merge several streams into a single stream. 
The `merge` method iterates over each provided stream, yielding instances from each one consecutively. The `process` method then utilizes this merging logic to create a new `MultiStream` that consolidates all input streams into a single output stream named "merged". 
This operator is particularly useful in scenarios where data from different sources needs to be combined into a single dataset for analysis or further processing.

Unit Testing Operators
-----------------------

To ensure that an operator functions as expected, it's essential to test it. Hereâ€™s how you can use the built-in testing suite in Unitxt:

.. code-block:: python

    from unitxt.test_utils.operators import check_operator

    operator = AddNumber(number=2)  # Assuming AddNumber is already defined
    inputs = [{'price': 100}, {'price': 150}]
    targets = [{'price': 100, 'new_price': 102}, {'price': 150, 'new_price': 152}]

    result = check_operator(operator, inputs, targets)
    print(result)  # Output: True if the operator performs as expected

**Explanation**: This test verifies that the `AddNumber` operator correctly adds 2 to the `price` field and stores the result in `new_price`. The function `check_operator` compares the output against the expected `targets` to confirm correct behavior.

# docs/docs/contributors_guide.rst
.. _contributors_guide:

==================
Contributors Guide
==================

This guide will assist you in contributing to unitxt.

------------------------
The Unitxt Documentation
------------------------

The unitxt external documentation is at https://unitxt.readthedocs.io/en/main/docs/introduction.html.

The documentation is produced from two sources:

- RST files located within the **docs** directory (https://github.com/IBM/unitxt/tree/main/docs).
- From the docstrings within the library python files. Changes to the docstrings are automatically propagated
  into the documentation for the latest version.

Editing the RST files
*********************

The main file is **index.rst**. Files for the different sections are under **docs/docs**.

To update the documentation, edit the **.rst** documentation files.

To test the documentation locally:

1. Make sure you have the documentation requirements installed:

.. code-block:: console

    pip install -r requirements/docs.rqr

2. Start a local documentation server:

.. code-block:: console

    make docs-server

3. Access the documentation at http://localhost:8478/.

-----------------------------
Creating a new Unitxt release
-----------------------------

The following process describes how to create a new release of Unitxt.

1. In a development environment, checkout the main branch:

.. code-block:: console

    git checkout main

2. Pull the latest code:

.. code-block:: console

    git pull

3. Determine the new version number. Increase the version number
by:

- 1 for major changes (e.g. 1.3.1 -> 2.0)
- 0.1 for regular changes (e.g. 1.3.1 -> 1.4)
- 0.0.1 for small bug fixes or patches (e.g. 1.3.1 -> 1.3.2)

4. Create a branch with the new version number:

.. code-block:: console

    make version=<new version number> new-version

for example:

.. code-block:: console

    make version=1.4.1 new-version

This will create a branch named with the new version number,
and will push the new branch to the remote git Unitxt repo.

5. Create a pull request for merging the new branch to the main branch, on the
Unitxt git repo https://github.com/IBM/unitxt.

6. Squash and merge the new pull request. It is ok to skip the tests for this PR since it changes only the
version number. This can be done by marking "merge without waiting for the requirements" within the
pull request
(note this option may not be available in the UI, since it requires specific permissions that are not given to all contributors).

7. After the merge, pull the merged changes to your local development environment:


.. code-block:: console

    git pull --rebase

Make sure your local main is now after the merge, with an updated version number in **version.py**.

8. Create a new version tag:

.. code-block:: console

    make version-tag

This will tag the main branch with a new tag equal to the updated version number.

9. Go to the Unitxt Releases list: https://github.com/IBM/unitxt/releases.

10. Choose "Draft a new release", and choose the new tag that was just created.
The new release name should be "Unitxt <new version number", for example "Unitxt 1.4.0".

11. Use "Generate release notes" to create an initial list of changed for the new release.
Click "Save Draft" to first save this auto-generated list.

12. Edit the release notes:

- Remove minor items, such as smaller version bumps.
- Add sections Enhancements, Bug fixes, Non backward compatible changes (see release notes of previous versions for examples).
- For each auto-generated item in the "What's Changed" section, copy it, if needed, to one of the above sections.
  Add a description that is concise and clear. Follow previous release notes for examples.

13. Click "Publish release".

14. There are a few actions that are triggered when a new release is published.
The actions are available at https://github.com/IBM/unitxt/actions.

- Check that the action "Release new version to PyPI" completes successfully
  (https://github.com/IBM/unitxt/actions/workflows/pipy.yml).
- The action "Release new version HuggingFace Hub" is currently known to be failing (since 1.2.0).

15. Check that the new release is available on pypi (https://pypi.org/project/unitxt).


# docs/docs/debugging.rst
.. _debugging:

===================================
Debugging Unitxt
===================================

Debugging cards
----------------

To help test and debug cards, there is a utility function called test_card imported from `unitxt.test_utils.card`. 

.. code-block:: python

  from unitxt.test_utils.card import test_card

  card = TaskCard(
    ...
  )
  test_card(card)


By default, the function generates the dataset using all templates defined in the card.
For each template, it prints out up to 5 examples from the test fold.  For each example,
you can see all the fields in the dataset.

In the rest of this tutorial we will review the output of the `test_card` function
of the `universal_ner` card.

.. code-block:: bash
  
  prepare/cards/universal_ner.py

::

  Loading limited to 30 instances by setting LoadHF.loader_limit;
  ----------
  Showing up to 5 examples from stream 'test':
  
  metrics (list):
      ['metrics.ner']
  data_classification_policy (list):
      ['public']
  source (str):
      From the following text, identify spans with entity type:Person, Organization, Location.
      text: What is this Miramar?
  target (str):
      Miramar: Location
  references (list):
      ['Miramar: Location']
  task_data (str):
      {"text": "What is this Miramar?", "text_type": "text", "class_type": "entity type", "classes": ["Person", "Organization", "Location"], "spans_starts": [13], "spans_ends": [20], "labels": ["Location"], "metadata": {"template": "templates.span_labeling.extraction.identify"}}
  group (str):
      unitxt
  postprocessors (list):
      ['processors.to_span_label_pairs']



The code then runs the metrics defined on the datasets based on:

1.  Predictions that are equal to one of the references.

2.  Random text predictions.

To help validate the post processing of the predictions and references, the code prints the post processed values.
For example, we can see how the string "Miramar: Location" is parsed by the post processors to a list of tuples.



::

  ****************************************
  Running with the gold references as predictions.
  Showing the output of the post processing:
  *****
  Prediction: (str)     Miramar: Location
  Processed prediction: (list) [('Miramar', 'Location')]
  Processed references: (list) [[('Miramar', 'Location')]]
  *****
  Prediction: (str)     Argentina: Location
  Processed prediction: (list) [('Argentina', 'Location')]
  Processed references: (list) [[('Argentina', 'Location')]]
  *****
  Prediction: (str)     None
  Processed prediction: (list) []
  Processed references: (list) [[]]
  *****
  Prediction: (str)     Argentina: Location
  Processed prediction: (list) [('Argentina', 'Location')]
  Processed references: (list) [[('Argentina', 'Location')]]
  *****
  Prediction: (str)     None
  Processed prediction: (list) []
  Processed references: (list) [[]]
  *****
  
  *****
  Score output:
  {
      "global": {
          "f1_Location": 1.0,
          "f1_macro": 1.0,
          "f1_micro": 1.0,
          "f1_micro_ci_high": NaN,
          "f1_micro_ci_low": NaN,
          "in_classes_support": 1.0,
          "precision_macro": 1.0,
          "precision_micro": 1.0,
          "recall_macro": 1.0,
          "recall_micro": 1.0,
          "score": 1.0,
          "score_ci_high": NaN,
          "score_ci_low": NaN,
          "score_name": "f1_micro"
      },
      "instance": {
          "f1_Location": 1.0,
          "f1_macro": 1.0,
          "f1_micro": 1.0,
          "in_classes_support": 1.0,
          "precision_macro": 1.0,
          "precision_micro": 1.0,
          "recall_macro": 1.0,
          "recall_micro": 1.0,
          "score": 1.0,
          "score_name": "f1_micro"
      }
  }


Most metrics should return a low score (near 0) on random text and a score of 1 when the data is equal to the references.
Errors/warnings are printed if that is not the case. 

If you want to disable these tests, set ``test_exact_match_score_when_predictions_equal_references=False`` and/or 
``test_full_mismatch_score_with_full_mismatch_prediction_values=False``.   

You can set the expected scores using the following parameters:

1. ``exact_match_score``: The expected score to be returned when predictions are equal to the gold reference. Default is 1.0.

2. ``maximum_full_mismatch_score``: The maximum score allowed to be returned when predictions are fully mismatched. Default is 0.0.

3. ``full_mismatch_prediction_values``: An optional list of prediction values to use for testing full mismatches. If not set, a default set of values: ["a1s", "bfsdf", "dgdfgs", "gfjgfh", "ghfjgh"] is used.

If you want to generate the card with different parameters, they can be provided as additional
arguments to the test_card() function.

.. code-block:: python

  # Test the templates with few shots
  test_card(card,num_demos=1,demo_pool_size=10)

test_card has an optional parameter debug flag. When set to true, the card is executed in debug mode, one step at a time. For example, it starts with loading the dataset, then performing the defined preprocessing steps, then performing the template rendering steps. 
After each step it prints the number of instances in each split, and one example from each split.

.. code-block:: python

  # Shows the step by step processing of data.
  test_card(card,debug=True)

If you get an error, it's best that you turn this flag on and see where in the execution flow it happens. It's also a good tactic if you want to understand exactly how datasets are generated and what each step performs.

Increase log verbosity
----------------------

If you want to get more information during the run (for example, which artifacts are loaded from which catalog),
you can set the UNITXT_DEFAULT_VERBOSITY environment variable or modify the global setting in the code.

.. code-block:: bash

  env UNITXT_DEFAULT_VERBOSITY=debug python prepare/cards/wnli.py

.. code-block:: python

  from .settings_utils import get_settings
  settings = get_settings()
  settings.default_verbosity = "debug"

# docs/docs/data_classification_policy.rst
.. _data_classification_policy:

.. note::

   To use this tutorial, you need to :ref:`install unitxt <install_unitxt>`.

=====================================
Sensitive data in unitxt âœ¨
=====================================

The section discusses how to properly handle sensitive data in Unitxt in order to avoid accidentally exposing 
proprietary/confidential/personal data to unauthorized services or 3rd parties. For example, sending sensitive 
data for inference by an external API in LLM as Judge metric.

The problem is exacerbated since the person who owns the data and uses the metric in their card
may not know what 3rd party services are used internally by the metric.

To address this, Unitxt allows the data owner to specify the data classification of their data, and similarly it requires that
any metric (or other component) that processes the data must be explicitly allowed to process data with this classification.


Data classification policy
----------------------------

When data is loaded from an external data source using a Loader, it can be tagged with a `data_classification_policy`,
which is a list of string identifiers, such as `public`, `proprietary`, `confidential`, `pii`.
You can define your own data classification identifiers.

Each component that processes data in Unitxt ( operators, metrics, inference engines, etc.) also has 
a parameter called `data_classification_policy`.  This parameter determines which kinds of data
it can process.  The parameter is also a list of string identifiers, each of which is a name of allowed data classification.

Before processing the data, the component verifies that the `data_classification_policy` of the data meets its `data_classification_policy`.
If the policies for a component include the classification of the data, then the data may be further processed. Otherwise, an error will be raised.
For example, an LLM as judge that calls an external api may set `data_classification_policy` to `['public']`.
If data marked [`confidential`] is passed to the metric, it will not process the data and fail.

If the data has multiple values under `data_classification_policy` then the component must be allowed to handle all of them.
If the `data_classification_policy` is not set, the component can handle all data.  

It is possible to override the `data_classification_policy` of a component with an environment variable.  See below.

Adding `data_classification_policy` for data
--------------------------------------------

Data classification information is added to streams of data by the use of Unitxt loaders.
Existing loaders have default data classification policies. For example, LoadHF sets the policy to `['public']` for datasets
downloaded from the HuggingFace and `['proprietary']` for datasets loaded from local files.  You can override this by setting
the `data_classification_policy` parameter of the loader. 

The data classification value is added as an additional field to all instances within a stream.

Example:

.. code-block:: python

    from unitxt.loaders import LoadFromDictionary

    data = {
        "train": [{"text": "SomeText1", "output": "SomeResult1"}],
        "test": [{"text": "SomeText2", "output": "SomeResult2"}],
    }

    loader = LoadFromDictionary(
        data=data,
        data_classification_policy=["public"], # Overrides the default of ["proprietary"]
    )

    multi_stream = loader.process()  # the field will be added during processing
    dataset = multi_stream.to_dataset()
    assert dataset["test"][0]["data_classification_policy"] == ["public"]

Adding `data_classification_policy` for components
--------------------------------------------------

In case of Unitxt components, the parameter can be added by setting the attribute of a class in the code or by setting an environment variable.

1. **Setting default data classification policy class attribute**:

The `data_classification_policy` attribute can be set in the code when the class is created.
The attribute should be a list of strings, and its default value is None.

Example:

.. code-block:: python

    from unitxt.metrics import F1Binary
    from unitxt.operators import DuplicateInstances

    stream = [
        {"input": "Input1", "data_classification_policy": ["pii", "proprietary"]},
        {"input": "Input2", "data_classification_policy": ["pii", "proprietary"]},
    ]

    metric = F1Binary(data_classification_policy=["public"])
    list(metric.process(stream))  # will raise an error as policies are different

    operator = DuplicateInstances(
        num_duplications=2,
        data_classification_policy=["pii"],
    )
    list(operator.process(stream))  # will not raise an error as the policy is included


1. **Overriding default policy during environment variable**:


You can override the data classification of artifacts that was saved in the catalog by setting the `UNITXT_DATA_CLASSIFICATION_POLICY` env variable accordingly.
It should be a string representation of type `Dict[str, List[str]]`, where a key is a name of a given artifact, and a corresponding value is the allowed data classification. For example:

.. code-block:: bash

    export UNITXT_DATA_CLASSIFICATION_POLICY '{ "metrics.llm_as_judge.rating.mistral_7b_instruct_v0_2_huggingface_template_mt_bench_single_turn": ["public","proprietary", "pii"], "processors.translate": ["public", "proprietry"]}'




# docs/docs/operators.rst
.. _operators_list:

==============
Operators List
==============

This library provides various operators for stream processing.

.. automodule:: unitxt.operators
   :members:
   :undoc-members:
   :show-inheritance:
   :no-index:


# docs/docs/saving_and_loading_from_catalog.rst
.. _using_catalog:

=====================================
Save/Load from Catalog
=====================================

The Unitxt catalog serves as a repository for people to share their processing tools. This includes templates, formats, operators, and other Unitxt assets. These can be shared through a local catalog located in a directory on the local filesystem or via a directory in a GitHub repository.

Defining a Local Catalog
------------------------

To define a local, private catalog, use the following code:

.. code-block:: python

    from unitxt import register_local_catalog

    register_local_catalog("path/to/catalog/directory")

Adding Assets to the Catalog
----------------------------

Once your catalog is registered, you can add artifacts to it:

.. code-block:: python

    from unitxt.task import Task
    from unitxt import add_to_catalog

    my_task = Task(...)

    catalog_name = "tasks.my_task"

    add_to_catalog(my_task, catalog_name, catalog_path="path/to/catalog/directory")

It's also possible to add artifacts to the library's default catalog:

.. code-block:: python

    add_to_catalog(my_task, catalog_name)

Using Catalog Assets
--------------------

To use catalog objects, simply specify their name in the Unitxt object that will use them.

.. code-block:: python

    from unitxt.card import TaskCard

    card = TaskCard(
        ...
        task="tasks.my_task"
    )

Modifying Catalog Assets on the Fly
-----------------------------------

To modify a catalog asset's fields dynamically, upon fetching the asset from the catalog, use the syntax: ``artifact_name[key_to_modify=new_value]``.
To assign lists, use: ``artifact_name[key_to_modify=[new_value_0, new_value_1]]``.
To assign dictionaries, use: ``artifact_name[key_to_modify={new_key_0=new_value_0,new_key_1=new_value_1}]``.
Note that the whole new value of the field has to be specified; not just one item of a list, or one key of the dictionary.
For instance, to change the metric specification of a task:

.. code-block:: python

    from unitxt.card import TaskCard

    card = TaskCard(
        ...
        task="tasks.my_task[metrics=[metrics.accuracy, metrics.f1[reduction=median]]]"
    )

Accessing Catalog Assets Directly
---------------------------------

Use ``get_from_catalog`` to directly access catalog assets, and obtain an asset instantiated as a python object of type ``unitxt.Artifact``:

.. code-block:: python

    from unitxt import get_from_catalog

    my_task = get_from_catalog("tasks.my_task")

A Catalog Asset Linking to Another Catalog Asset
------------------------------------------------

A catalog asset can be just a link to another asset.
This feature comes handy when for some reason, we want to change the catalog name
of an existing asset (e.g. ``asset1`` to ``asset2``), while there is already code
that uses the old name of the asset and we want to avoid non-backward compatible changes.

In such a case, we can save the asset as ``asset2``, create an asset of type
:class:`ArtifactLink <unitxt.artifact.ArtifactLink>` that links to ``asset2``, and save
that one as ``asset1``.
When ``asset1`` is accessed from an existing code, Unixt Catalog realizes that the asset fetched from position ``asset1``
is an ``ArtifactLink``, so it continues and fetches ``asset2`` -- the Artifact linked to by ``asset1``.

.. code-block:: python

    link_to_asset2 = ArtifactLink(to="asset2")
    add_to_catalog(
        link_to_asset2,
        "asset1",
        overwrite=True,
    )

Deprecated Asset
----------------

Every asset has a special field named ``__deprecated_msg__`` of type ``str``, whose default value is None.
When None, the asset is cocnsidered non-deprecated. When not None, the asset is considered deprecated, and
its ``__deprecated_msg__`` is logged at level WARN upon its instantiation. (Other than this logging,
the artifact is instantiated normally.)

Example of a deprecated catalog asset:

.. code-block:: python

    {
        "__type__": "textual_system_prompt",
        "__deprecated_msg__": "This legacy system prompt reflects a task specific instruction, which is best handled by the 'instruction' field of the template.",
        "text": "You are an agent in charge of answering a boolean (yes/no) question. The system presents you with a passage and a question. Read the passage carefully, and then answer yes or no. Think about your answer, and make sure it makes sense. Do not explain the answer. Only say yes or no."
    }

Combining this feature with ``ArtifactLink`` in the above example, we can also log a warning to the accessing code that
the name ``asset1`` is to be replaced by ``asset2``.

.. code-block:: python

    link_to_asset2 = ArtifactLink(to="asset2",
           __deprecated_msg__="'asset1' is going to be deprecated. In future uses, please access 'asset2' instead.")
    add_to_catalog(
        link_to_asset2,
        "asset1",
        overwrite=True,
    )


Using Multiple Catalogs
-----------------------

By default, Unitxt uses several catalogs, such as the local library catalog and online community catalogs hosted on GitHub. Assets are sourced from the last registered catalog containing the asset.

Defining Catalogs Through Environment Variables
-----------------------------------------------

When Unitxt is executed by another application, you might need to specify custom catalogs through an environment variable:

.. code-block:: bash

    export UNITXT_CATALOGS="path/to/first/catalog:path/to/second/catalog"

Learn more about catalogs here: :class:`catalog <unitxt.catalog>`.

# docs/docs/adding_format.rst
.. _adding_format:

.. note::

   To use this tutorial, you need to :ref:`install unitxt <install_unitxt>`.

=====================================
Formats âœ¨
=====================================

Formats define the overall textual layout of the example, including system prompt,
in-context learning demonstrations, and other special tokens.
The format and template work together to verbalize the model input -
the template verbalizes the task specific parts of the input prompt
while the format verbalizes the model specific aspects of the input prompt.

In-context learning is activated when the  ``num_demos`` parameter of
the :ref:`recipe <recipe>` is set to a non zero value.
Different demo examples are chosen per instance from a fixed set of examples called a ``demo_pool``.
Usually, the examples in the demo pool are taken from the train split, but this can be overridden by the ``demos_taken_from`` parameter.
The size of the demo pool is determined by a mandatory parameter called ``demos_pool_size`` parameter.


.. _prompt_format_layout:
.. image:: ../../assets/prompt_layout.png
   :alt: The unitxt prompt layout
   :width: 75%
   :align: center

It determines the positioning of the task `instruction`, `system_prompt` and `demos` the `source` query and required output from the model, the `target`.

Below is in example of how to define the layout of the different parts.
This example is based on this blog post explaining the prompt structure of the llama2 model: `Blog Post <https://huggingface.co/blog/llama2#how-to-prompt-llama-2>`_

So the actual template looks like this:

.. code-block:: text

   <s>[INST] <<SYS>>
   {{ system_prompt }}
   <</SYS>>

   {{ user_msg_1 }} [/INST] {{ model_answer_1 }} </s><s>[INST] {{ user_msg_2 }} [/INST]

An example for data point with the llama2 format and system prompt.

.. code-block:: text

   [INST] <<SYS>>
   You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.


   If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.


   Classify the grammatical acceptability of the following text to one of these options: unacceptable, acceptable.
   <<SYS>>


   text: The more we study verbs, the crazier they get. [/INST] The grammatical acceptability is acceptable </s><s>[INST] text: They drank the pub. [/INST]The grammatical acceptability is

To define this exact format you can use this code:

.. code-block:: python

   from unitxt.catalog import add_to_catalog
   from unitxt.formats import SystemFormat

   format = SystemFormat(
      demo_format="{source} [/INST] {target_prefix}{target} </s><s>[INST] ",
      model_input_format="[INST] <<SYS>>\n{system_prompt}\n\n{instruction}<<SYS>>\n\n\n{demos}{source} [/INST]{target_prefix}",
   )

   add_to_catalog(format, "formats.llama2", overwrite=True)

# docs/docs/production.rst
.. _production:

.. note::

   To use this tutorial, you need to :ref:`install unitxt <install_unitxt>`.

========================
Inference and Production
========================

In this guide you will learn how to use unitxt data recipes in production.

For instance, you learn how to make end-to-end functions like `paraphrase()`:

.. code-block:: python

  def paraphrase(input_text):
    return unitxt.infer(
      [{"input_text": input_text}],
      recipe="task=tasks.rewriting.paraphrase,template=templates.rewriting.paraphrase.default,format=formats.models.flan.exq_exa",
      engine="engines.model.flan.t5_small.hf"
    )

Which then can be used like:

.. code-block:: python

  paraphrase("So simple to paraphrase!")

In general, Unitxt is capable of:
 - Producing processed data according to a given recipe.
 - Post-processing predictions based on a recipe.
 - Performing end-to-end inference using a recipe and a specified inference engine.

Produce Data
------------

First lets define a task based recipe:

.. code-block:: python

  recipe = "task=tasks.classification.multi_class.relation,template=templates.classification.multi_class.relation.default"

Next, prepare a Python dictionary that matches the inputs schema required by the task:

.. code-block:: python

  instance = {
    "label": "?",
    "text_a": "It works perfectly",
    "text_b": "It works!",
    "classes": ["entailment", "not entailment"],
    "type_of_relation": "entailment",
    "text_a_type": "premise",
    "text_b_type": "hypothesis",
  }

Then, produce the model-ready input data with the `produce` function:

.. code-block:: python

  from unitxt import produce

  result = produce(instance, recipe)

If we want to add to the data few shot examples, we should specify recipe with a data card:

.. code-block:: python

  recipe = "card=cards.wnli,template=templates.classification.multi_class.relation.default,demos_pool_size=5,num_demos=2"

Again we can now produce the data with the few shot demonstrations:

.. code-block:: python

  result = produce(instance, recipe)

To view the formatted instance, print the result:

.. code-block::

  print(result["source"])

This will output instances like:

.. code-block::

    Given a premise and a hypothesis, classify the entailment of the hypothesis as either 'entailment' or 'not entailment'.

    premise: When Tatyana reached the cabin, her mother was sleeping. She was careful not to disturb her, undressing and climbing back into her berth., hypothesis: mother was careful not to disturb her, undressing and climbing back into her berth.
    The entailment class is entailment

    premise: The police arrested all of the gang members. They were trying to stop the drug trade in the neighborhood., hypothesis: The police were trying to stop the drug trade in the neighborhood.
    The entailment class is not entailment

    premise: It works perfectly, hypothesis: It works!
    The entailment class is

Post Process Data
-----------------

After obtaining predictions, they can be post-processed:

.. code-block:: python

  from unitxt import post_process

  prediction = model.generate(result["source"])
  processed_result = post_process(predictions=[prediction], data=[result])[0]

End-to-End Inference Pipeline
-----------------------------

You can also implement an end-to-end inference pipeline using your preferred data and an inference engine:

.. code-block:: python

  from unitxt import infer
  from unitxt.inference import HFPipelineBasedInferenceEngine

  engine = HFPipelineBasedInferenceEngine(
      model_name="google/flan-t5-small", max_new_tokens=32
  )

  infer(instance, recipe, engine)

Alternatively, you can specify any task, template and inference engine from the catalog:

.. code-block:: python

  infer(
    instance,
    recipe="task=tasks.classification.multi_class.relation,template=templates.classification.multi_class.relation.default",
    engine="engines.model.flan.t5_small.hf"
  )

# docs/docs/benchmark.rst
.. _adding_benchmark:

.. note::

   To use this tutorial, you need to :ref:`install Unitxt <install_unitxt>`.

=================
Benchmarks âœ¨
=================

This guide will assist you in adding or using your new benchmark in Unitxt.

Unitxt helps define the data you want to include in your benchmark and aggregate any final score you consider important.

The first tool to use in creating a benchmark is the Unitxt  :ref:`recipe <recipe>`.

To find more information about recipes, and how to start refer to :ref:`adding dataset guide <adding_dataset>`.

Once you have constructed a list of recipes, you can fuse them to create a benchmark.

Let's say we want to create the GLUE benchmark.

We can utilize the following Unitxt :ref:`cards <data_task_card>`:

 - ``cards.cola``
 - ``cards.mnli``
 - ``cards.mrpc``
 - ``cards.qnli``
 - ``cards.qqp``
 - ``cards.rte``
 - ``cards.sst2``
 - ``cards.stsb``
 - ``cards.wnli``

We can compile them together using Unitxt Benchmark:

.. code-block:: python

    from unitxt.benchmark import Benchmark
    from unitxt.standard import DatasetRecipe

    benchmark = Benchmark(
        format="formats.user_agent",
        max_samples_per_subset=5,
        loader_limit=300,
        subsets={
            "cola": DatasetRecipe(card="cards.cola", template="templates.classification.multi_class.instruction"),
            "mnli": DatasetRecipe(card="cards.mnli", template="templates.classification.multi_class.relation.default"),
            "mrpc": DatasetRecipe(card="cards.mrpc", template="templates.classification.multi_class.relation.default"),
            "qnli": DatasetRecipe(card="cards.qnli", template="templates.classification.multi_class.relation.default"),
            "rte": DatasetRecipe(card="cards.rte", template="templates.classification.multi_class.relation.default"),
            "sst2": DatasetRecipe(card="cards.sst2", template="templates.classification.multi_class.title"),
            "stsb": DatasetRecipe(card="cards.stsb", template="templates.regression.two_texts.title"),
            "wnli": DatasetRecipe(card="cards.wnli", template="templates.classification.multi_class.relation.default"),
        },
    )

Next, you can evaluate this benchmark by:

.. code-block:: python

    dataset = list(benchmark()["test"])

    # Inference using Flan-T5 Base via Hugging Face API
    model = HFPipelineBasedInferenceEngine(
        model_name="google/flan-t5-base", max_new_tokens=32
    )

    predictions = model(dataset)
    results = evaluate(predictions=predictions, data=dataset)

    print(results.subsets_scores.summary)

The result will contain the score per subset as well as the final global result:

.. code-block:: python

    ...
    mnli:
        ...
        score (float):
            0.4
        score_name (str):
            f1_micro
       ...
    mrpc:
        ...
        score (float):
            0.6
        score_name (str):
            f1_micro
        ...
    score (float):
        0.521666065848072
    score_name (str):
        subsets_mean


Saving and Loading Benchmarks
++++++++++++++++++++++++++++++

As always in Unitxt, you can save your benchmark to the catalog with:

.. code-block:: python

    add_to_catalog(benchmark, "benchmarks.glue")

Others can then load it from the catalog and evaluate on your benchmark with:

.. code-block:: python

    from unitxt import load_dataset

    dataset = load_dataset("benchmarks.glue")

If they want to modify the format or any other parameter of the benchmark, they can easily do so by:

.. code-block:: python

    from unitxt import load_dataset

    dataset = load_dataset("benchmarks.glue[format=formats.llama3]")

Additional Options
++++++++++++++++++

If you want to explore different templates, you can do so by defining a list of templates within any recipe. For instance:

.. code-block:: python

    DatasetRecipe(
        card="cards.cola",
        template=[
            "templates.classification.multi_class.instruction",
            "templates.classification.multi_class.title"
        ],
        group_by=["template"]
    )

This configuration will also provide the score per template for this recipe. To explore more configurations and capabilities, see the :ref:`evaluation guide <evaluating_datasets>`.
# docs/docs/installation.rst
.. _install_unitxt:

==============
Installation
==============

Install Unitxt locally from pip:

.. code-block:: bash

  pip install unitxt

The core of Unitxt has a minimal set of requirements.
Specific metrics, inference APIs and other assets, may require additional dependencies. 
A clear error message on what needs to be installed is provided if these dependencies are not available when the specific asset is used.
# docs/docs/loading_datasets.rst
.. _loading_datasets:

===================================
Loading Datasets
===================================


.. code-block:: python

  from unitxt import load_dataset

  dataset = load_dataset(card="cards.wnli")

.. code-block:: python

  print(dataset)
  DatasetDict({
    train: Dataset({
        features: ['metrics', 'source', 'target', 'references', 'group', 'postprocessors'],
        num_rows: 599
    })
    validation: Dataset({
        features: ['metrics', 'source', 'target', 'references', 'group', 'postprocessors'],
        num_rows: 36
    })
    test: Dataset({
        features: ['metrics', 'source', 'target', 'references', 'group', 'postprocessors'],
        num_rows: 71
    })
  })

The following prints the source text (input to the model) of the first sample in the training set:

.. code-block:: python

    print(dataset['train'][0]['source'])

.. code-block::

    Given a premise and hypothesis classify the entailment of the hypothesis to one of entailment, not entailment.
    premise: Grace was happy to trade me her sweater for my jacket. She thinks it looks dowdy on her.
    hypothesis: The sweater looks dowdy on her.
    The entailment class is

This prints the reference text (expected output of the model) of the first sample in the training set:

.. code-block:: python

    print(dataset['train'][0]['references'][0])

.. code-block::

    'not entailment'


Loading a customized dataset
----------------------------

Unitxt enables formatting the dataset in different ways.

As example, here we load wnli in 3 shots format:

.. code-block:: python

  from unitxt import load_dataset
  dataset = load_dataset(card="cards.wnli",template="templates.classification.multi_class.relation.default",num_demos=3,demos_pool_size=100)

Now the source text (input to the model) of the first sample in the training set has in-context examples:

.. code-block:: python

    print(dataset['train'][0]['source'])

.. code-block::

    Given a premise and hypothesis classify the entailment of the hypothesis to one of entailment, not entailment.
    premise: The journalists interviewed the stars of the new movie. They were very cooperative, so the interview lasted for a long time.
    hypothesis: The journalists were very cooperative, so the interview lasted for a long time.
    The entailment class is entailment

    premise: The table won't fit through the doorway because it is too narrow.
    hypothesis: The table is too narrow.
    The entailment class is entailment

    premise: Sam pulled up a chair to the piano, but it was broken, so he had to stand instead.
    hypothesis: The chair was broken, so he had to stand instead.
    The entailment class is not entailment

    premise: Grace was happy to trade me her sweater for my jacket. She thinks it looks dowdy on her.
    hypothesis: The sweater looks dowdy on her.
    The entailment class is

Loading in Chat API format
--------------------------

Unitxt can now produce data in the widely adopted Chat API format.
This ensures compatibility with popular LLM Provider APIs and avoid the need for custom per model formatting.
Additionally, the format supports multiple modalities such as text, images, and videos.

This is done by specifying `fromats.chat_api` in the format field, when loading the dataset.

.. code-block:: python

   from unitxt import load_dataset
   dataset = load_dataset(card="cards.wnli",format="formats.chat_api",num_demos=3,demos_pool_size=100)

Now the returned source is list of messages, per the ChatAPI specification.

.. code-block:: python

   import json
   print(json.dumps(dataset['train'][0]['source'],indent=4))

.. code-block::

   [
      {
          "role": "system",
          "content": "Given a premise and hypothesis classify the entailment of the hypothesis to one of entailment, not entailment."
      },
      {
          "role": "user",
          "content": "premise: Emma did not pass the ball to Janie although she was open.\nhypothesis: She saw that Janie was open."
      },
      {
          "role": "assistant",
          "content": "The entailment class is not entailment"
      },
      {
          "role": "user",
          "content": "premise: The foxes are getting in at night and attacking the chickens. I shall have to kill them.\nhypothesis: I shall have to kill The foxes."
      },
      {
          "role": "assistant",
          "content": "The entailment class is not entailment"
      },
      {
          "role": "user",
          "content": "premise: Fred is the only man alive who still remembers my father as an infant. When Fred first saw my father, he was twelve years old.\nhypothesis: When Fred first saw my father, My father was twelve years old."
      },
      {
          "role": "assistant",
          "content": "The entailment class is entailment"
      },
      {
          "role": "user",
          "content": "premise: Grace was happy to trade me her sweater for my jacket. She thinks it looks dowdy on her.\nhypothesis: The sweater looks dowdy on her."
      }
  ]



Loading a Dataset with Multiple Templates or Number of Demonstrations
---------------------------------------------------------------------

You can sample a template for each instance from a pool of templates by assigning the ``template`` argument a list of templates. Similarly, you can sample the number of demonstrations by assigning ``num_demos`` a list of integers.

Here is an example of using random templates and a varying number of demonstrations for each instance of the data:

.. code-block:: python

  dataset = load_dataset(card="cards.wnli",template=["templates.classification.multi_class.relation.default","templates.key_val"],num_demos=[0,1,3],demos_pool_size=100)

# docs/docs/tutorials.rst
.. _tutorials:

------------
Tutorials âœ¨
------------

.. toctree::
   :maxdepth: 1
   :caption: Dive into...

   adding_dataset
   adding_task
   adding_template
   adding_format
   adding_operator
   adding_metric
   data_classification_policy
   ../modules
   rag_support
   multimodality
   operators
   saving_and_loading_from_catalog
   inference
   production
   debugging
   helm
   lm_eval
   glossary
   benchmark
   llm_as_judge
   tags_and_descriptions
   types_and_serializers
   contributors_guide


# docs/docs/introduction.rst
.. image:: ../../assets/banner.png
   :alt: Optional alt text
   :width: 100%
   :align: center

===================
Introduction
===================

Unitxt is an innovative library for textual data preparation and evaluation of generative language models.

In the dynamic landscape of generative NLP, traditional text processing pipelines limit research flexibility and reproducibility, as they are tailored to specific dataset, task, and model combinations.
The escalating complexity, involving system prompts, model-specific formats, instructions, and more, calls for a shift to a structured, modular, and customizable solution.

Unitxt deconstructs the data preparation and evaluation flows into modular components, enabling easy customization and sharing between practitioners.

Key Capabilities:

- Built-in support for a variety of NLP tasks, including ones not typically found in other frameworks, such as multi label classification, targeted sentiment analysis, entity and relation extraction, table understanding, and retrieval augmented generation

- Support for changing templates and formats

- Support for loading data from different datasources (e.g., local files, HuggingFace, cloud storage, Kaggle)

- Large collection of metrics (including LLMs as Judges)

- Compatible with HuggingFace Datasets and Metrics APIs without needing any installation

- The same Unitxt data preparation pipeline can be used for both evaluation and inference in production systems

- Removes the requirement to run user Python code in dataset processing, reducing security risks

Unitxt can be used as standalone code. It can also be integrated with common libraries and evaluation frameworks such as
`HuggingFace`_, `Helm`_, and `LM-eval-harness`_. 

To get started, you can explore the Unitxt :ref:`catalog <demo>`. Learn how you can load a :ref:`dataset<loading_datasets>` and  :ref:`evaluate <evaluating_datasets>` it in a just a few lines of code.
You can then learn how to :ref:`add new datasets <adding_dataset>`.

Beyond being a tool, Unitxt is a community-driven platform, empowering users to build, share, and advance their pipelines collaboratively.

Join the Unitxt community at https://github.com/IBM/unitxt!

.. _Unitxt: https://github.com/IBM/unitxt
.. _HuggingFace: https://huggingface.co/
.. _LM-eval-harness: https://github.com/EleutherAI/lm-evaluation-harness
.. _Helm: https://github.com/stanford-crfm/helm

# docs/docs/adding_dataset.rst
.. _adding_dataset:

.. note::

   To use this tutorial, you need to :ref:`install Unitxt <install_unitxt>`.

=================
Datasets âœ¨
=================

This guide will assist you in adding or using your new dataset in Unitxt.

The information needed for loading your data will be defined in the :class:`TaskCard <unitxt.card.TaskCard>` class:

.. code-block:: python

    card = TaskCard(
        # will be defined in the rest of this guide
    )


The Task
---------

When we "add a dataset", we are actually adding data for a particiular NLP task such as translation, sentiment classification, question answering, summarization, etc.
In fact, the same dataset can be used for multiple NLP tasks. For example, a dataset with question-and-answer pairs can be used for both
question answering and question generation.  Similarly, a dataset with corresponding English and French sentences can be used for
an Engish-to-French translation task or for a French-to-English translation task.

The task schema is a formal definition of the NLP task.

The `input_fields` of the task are a set of fields that are used to format the textual input to the model.
The `reference_fields` of the task are a set of fields used to format the expected textual output from the model (gold references).
The `metrics` of the task are a set of default metrics used to evaluate the outputs of the model.

While language models generate textual predictions, the metrics often evaluate on different datatypes.  For example,
Spearman's correlation is evaluated on numeric predictions vs numeric references, and multi-label F1 is evaluated on a prediction which is a list of string class names
vs. a reference list of string class names.
`

The `prediction_type` of the task defines the common prediction (and reference) types for all metrics of the task.

Note that the task does not perform any verbalization or formatting of the task input and reference fields - this is the responsibility of the template.

In the example below, we formalize a translation task between `source_language` and a `target_language`.
The text to translate is in the field `text` and the reference answer in the `translation` field.
We use the `bleu` metric for a reference-based evaluation.

.. code-block:: python

    task=Task(
        input_fields= { "text" : str, "source_language" : str, "target_language" : str},
        reference_fields= {"translation" : str},
        prediction_type=str,
        metrics=["metrics.bleu"],
    ),

We have many predefined tasks in the catalog's :ref:`Tasks section <dir_catalog.tasks>`.

If a catalogued task fits your use case, you may reference it by name:

.. code-block:: python

    task='tasks.translation.directed',

Loading the Dataset
---------------------

To load data from an external source, we use a loader.
For example, to load the `wmt16` translation dataset from the HuggingFace hub:

.. code-block:: python

    loader=LoadHF(path="wmt16", name="de-en"),

More loaders for different sources are available in the  :class:`loaders <unitxt.loaders>` section.

The Preprocessing Pipeline
---------------------------

The preprocessing pipeline consists of operations to prepare your data according to the task's schema.

For example, to prepare the wmt16 dataset for the translation task, we need to map the raw dataset field names to the standard
input fields and reference fields of the task.  We also need to add new fields for the source and target language.

.. code-block:: python

    ...
    preprocess_steps=[
        # Copy the fields to prepare the fields required by the task schema
        Copy(field="translation/en", to_field="text"),
        Copy(field="translation/de", to_field="translation"),
        # Set new fields required by the task schema
        Set(
            fields={
                "source_language": "english",
                "target_language": "deutch",
            }
        ),
    ]

For more built-in operators, read :class:`operators <unitxt.operators>`.

Most data can be normalized to the task schema using built-in operators, ensuring your data is processed with verified high-standard streaming code.

For custom operators, refer to the :ref:`Operators Tutorial <adding_operator>`.

The Template
----------------

The responsibility of the template is to verbalize the task's input fields and reference fields to the input of the model and the gold references.
For example, the template can take the input fields `text`, `source_language`, and `target_language` and format them as a prompt.

`Translate this sentence from {source_language} to {target_language}: {text}.``

The template also verbalizes the reference fields as gold references.  In Unitxt, references are the expected textual outputs of the model.
In this example, the `translation` field is taken, as is, as a gold reference.
However, in other cases, the output field may undergo some transformations.

If using a predefined task, you can choose from the corresponding templates available in the catalog's :ref:`Templates section <dir_catalog.templates>`.

.. note::

   Use the :ref:`comprehensive guide on templates  <adding_template>` for more templates features.

Alternatively, you can define your custom templates:

.. code-block:: python

    ..
    templates=TemplatesList([
        InputOutputTemplate(
            input_format="Translate this sentence from {source_language} to {target_language}: {text}.",
            output_format='{translation}',
        ),
    ])

Testing Your Card
-------------------

Once your card is ready, you can test it.  Here we use standard translation templates from
the Unitxt catalog.

.. code-block:: python

        from unitxt.card import TaskCard
        from unitxt.loaders import LoadHF
        from unitxt.operators import Copy, Set
        from unitxt.test_utils.card import test_card

        card = TaskCard(
            loader=LoadHF(path="wmt16", name="de-en"),
            preprocess_steps=[
                # Copy the fields to prepare the fields required by the task schema
                Copy(field="translation/en", to_field="text"),
                Copy(field="translation/de", to_field="translation"),

                Set( # add new fields required by the task schema
                    fields={
                        "source_language": "english",
                        "target_language": "deutch",
                    }
                ),
            ],
            task="tasks.translation.directed",
            templates="templates.translation.directed.all"
        )

        test_card(card)


The `test_card` function generates the dataset using all templates defined in the card within context learning mode and one demonstration.
It prints out three examples from the test fold and runs the metrics defined on the datasets on
(1) randomly generated text, and
(2) text that is equal to one of the references.

Most metrics should return a low score (near 0) on random data and a score of 1 when the data is equal to the references.
Errors/warnings are printed if that's not the case.

Adding to the Catalog
-----------------------

Once your card is ready and tested, you can add it to the catalog.


.. code-block:: python

    from unitxt import add_to_catalog

    add_to_catalog(card, 'cards.wmt.en_de')

In the same way, you can save your custom templates and tasks, too.

.. note::
   By default, a new artifact is added to a local catalog stored
   in the library directory. To use a different catalog,
   use the `catalog_path` argument.

   In order to automatically load from your new catalog, remember to
   register your new catalog by `unitxt.register_catalog('my_catalog')`
   or by setting the `UNITXT_CATALOGS` environment variable to include your catalog.


Putting It All Together!
------------------------

Now everything is ready to use the data! We can load the dataset with three in-context examples.

.. code-block:: python

    from unitxt import load_dataset

    dataset = load_dataset(
        card='cards.wmt.en_de',
        num_demos=3, # The number of demonstrations for in-context learning
        demos_pool_size=100 # The size of the demonstration pool from which to sample the 5 demonstrations
        template_card_index=0 # Take the first template defined in the card
    )

The dataset can also be loaded using the HuggingFace Datasets API:

.. code-block:: python

    from datasets import load_dataset

    dataset = load_dataset('unitxt/data', 'card=cards.wmt.en_de,num_demos=5,demos_pool_size=100,template_card_index=0')

And the same results as before will be obtained.

Sharing the Dataset
--------------------

Once the dataset is loaded, it may be shared with others by simply sharing the card file
with them to paste into their local catalog.

You may also submit a PR to integrate your new datasets into the official Unitxt release.

# docs/docs/evaluating_datasets.rst
.. _evaluating_datasets:

===================================
Evaluating Datasets
===================================

Unitxt can be used to evaluate datasets from it's catalog and user defined datasets.

.. code-block:: python

  from unitxt import load_dataset, evaluate
  from unitxt.inference import HFAutoModelInferenceEngine

  dataset = load_dataset(
    card="cards.wnli",
    format="formats.chat_api",
    max_test_instances=100,
    split="test"
  )

  # The following lines can be replaced by any inference engine that receives the model_input
  # (found in dataset['source']) and returns model predictions as string.
  model = HFAutoModelInferenceEngine(
      model_name="Qwen/Qwen1.5-0.5B-Chat", max_new_tokens=32
  )
  predictions = model(dataset)

  results = evaluate(predictions, dataset)

The following prints the scores defined in WNLI task (f1_micro, f1_macro, accuracy, as well as their confidence intervals).

.. code-block:: python

    print(results.global_scores.summary)


.. code-block::

  | score_name        |   score | ci_low   | ci_high   |
  |:------------------|--------:|:---------|:----------|
  | accuracy          |    0.56 | 0.44     | 0.68      |
  | f1_entailment     |    0.73 |          |           |
  | f1_macro          |    0.36 | 0.32     | 0.41      |
  | f1_micro          |    0.57 | 0.48     | 0.7       |
  | f1_not entailment |    0    |          |           |
  | score             |    0.57 | 0.48     | 0.7       |
  Main Score: f1_micro
  Num Instances: 71


If you want to evaluate with few templates or few num_demos you can run:

.. code-block:: python

  dataset = load_dataset(
    card="cards.wnli",
    template=["templates.classification.multi_class.relation.default","templates.key_val"],
    num_demos=[0,1,3],
    demos_pool_size=10,
    max_test_instances=100
  )

This will randomly sample from the templates and for each instance assign a random template from the list and run number of demonstration from the list.

If you the want to explore the score per template and num of demonstrations you can add ``group_by=["template", "num_demos"]``.
If you want to get the score for each combination you should add ``group_by=[["template", "num_demos"]]`` or if you want for each group and for each combination you caption
add them all together ``group_by=["template", "num_demos", ["template", "num_demos"]]`` or in a full recipe:

.. code-block:: python

  dataset = load_dataset(
    card="cards.wnli",
    template=["templates.classification.multi_class.relation.default","templates.key_val"],
    num_demos=[0,1,3],
    group_by=["template","num_demos",["template","num_demos"]],
    demos_pool_size=10,
    max_test_instances=100
  )

The grouping can be done based on any field of the task or the metadata, so for classification task you can also group by label with ``group_by=["label"]``.

.. code-block:: python

    print(results.groups_scores.summary)

Will print:
.. code-block::

    # Group By: template
    | template                                              |    score | score_name   |   score_ci_low |   score_ci_high |   num_of_instances |
    |:------------------------------------------------------|---------:|:-------------|---------------:|----------------:|-------------------:|
    | templates.classification.multi_class.relation.default | 0.264151 | f1_micro     |       0.137052 |        0.421053 |                 41 |
    | templates.key_val                                     | 0.210526 | f1_micro     |       0.06367  |        0.388275 |                 30 |

    # Group By: num_demos
    |   num_demos |    score | score_name   |   score_ci_low |   score_ci_high |   num_of_instances |
    |------------:|---------:|:-------------|---------------:|----------------:|-------------------:|
    |           1 | 0.30303  | f1_micro     |      0.125     |        0.486229 |                 23 |
    |           3 | 0.275862 | f1_micro     |      0.0769231 |        0.478979 |                 22 |
    |           0 | 0.137931 | f1_micro     |      0         |        0.343992 |                 26 |

    # Group By: template, num_demos
    | template                                              |   num_demos |    score | score_name   | score_ci_low        | score_ci_high      |   num_of_instances |
    |:------------------------------------------------------|------------:|---------:|:-------------|:--------------------|:-------------------|-------------------:|
    | templates.classification.multi_class.relation.default |           1 | 0.333333 | f1_micro     | 0.08606627464804656 | 0.5990125628603442 |                 16 |
    | templates.key_val                                     |           3 | 0.272727 | f1_micro     | 0.09226935524612535 | 0.5454545454545454 |                 16 |
    | templates.key_val                                     |           1 | 0.222222 | f1_micro     | 0.0                 | 0.7225818346056374 |                  7 |
    | templates.classification.multi_class.relation.default |           3 | 0.285714 | f1_micro     | 0.0                 | 0.779447856172277  |                  6 |
    | templates.classification.multi_class.relation.default |           0 | 0.181818 | f1_micro     | 0.0                 | 0.4105379478071894 |                 19 |
    | templates.key_val                                     |           0 | 0        | f1_micro     |                     |                    |                  7 |
# docs/docs/multimodality.rst
.. _multi_modality:

==============
Multi-Modality
==============

.. note::

   This tutorial requires a :ref:`Unitxt installation <install_unitxt>`.

Introduction
------------

This tutorial explores multi-modality processing with Unitxt, focusing on handling image-text-to-text datasets and creating an evaluation and inference pipeline. By the end, you'll be equipped to process complex multi-modal data efficiently.

Part 1: Understanding Image-Text to Text Tasks
----------------------------------------------

Image-text to text tasks involve providing a model with a combination of text and images and expecting a textual answer. These tasks are increasingly relevant in modern AI applications.

Tutorial Overview
^^^^^^^^^^^^^^^^^

We'll create an image-text to text evaluation pipeline using Unitxt, concentrating on a document visual question answering (DocVQA) task. This task involves asking questions about images and generating textual answers.

Part 2: Data Preparation
------------------------

Creating a Unitxt DataCard
^^^^^^^^^^^^^^^^^^^^^^^^^^

Our first step is to prepare the data using a Unitxt DataCard. If you it's your first time adding a DataCard we recommend reading the :ref:`Adding Datasets Tutorial <adding_dataset>`.

Dataset Selection
^^^^^^^^^^^^^^^^^

We'll use the ``doc_vqa`` dataset from Hugging Face, formatting it for a question-answering task. Specifically, we'll use the ``tasks.qa.with_context.abstractive`` task from the Unitxt Catalog.

DataCard Implementation
^^^^^^^^^^^^^^^^^^^^^^^

Our goal in the DataCard will be to adjust the data as it comes from hugginface to task schema.
Create a Python file named ``doc_vqa.py`` and implement the DataCard as follows:

.. code-block:: python

    from unitxt.blocks import LoadHF, Set, TaskCard
    from unitxt.collections_operators import Explode, Wrap
    from unitxt.image_operators import ImageToText
    from unitxt.operators import Copy

    card = TaskCard(
        loader=LoadHF(path="cmarkea/doc-vqa"),
        preprocess_steps=[
            "splitters.small_no_dev",
            Explode(field=f"qa/en", to_field="pair"),
            Copy(field="pair/question", to_field="question"),
            Copy(field="pair/answer", to_field="answers"),
            Wrap(field="answers", inside="list"),
            Set(fields={"context_type": "image"}),
            ImageToText(field="image", to_field="context"),
        ],
        task="tasks.qa.with_context.abstractive",
        templates="templates.qa.with_context.all",
    )

The ImageToText Operator
^^^^^^^^^^^^^^^^^^^^^^^^

The ``ImageToText`` operator is a key component that integrates the image into the text, allowing inference engines to process both elements simultaneously.

Testing and Catalog Addition
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Test the card and add it to the catalog:

.. code-block:: python

    test_card(card)
    add_to_catalog(card, f"cards.doc_vqa.en", overwrite=True)

Part 3: Inference and Evaluation
--------------------------------

With our data prepared, we can now test model performance.

Pipeline Setup
^^^^^^^^^^^^^^

Set up the inference and evaluation pipeline:

.. code-block:: python

    from unitxt.api import evaluate, load_dataset
    from unitxt.inference import HFLlavaInferenceEngine
    from unitxt.text_utils import print_dict

    # Initialize the inference model
    model = HFLlavaInferenceEngine(
        model_name="llava-hf/llava-interleave-qwen-0.5b-hf", max_new_tokens=32
    )

    # Load and prepare the dataset
    dataset = load_dataset(
        card="cards.doc_vqa.en",
        template="templates.qa.with_context.title",
        format="formats.models.llava_interleave",
        loader_limit=30,
        split="test"
    )

    # Select a subset for testing
    dataset = dataset.select(range(5))

Executing Inference and Evaluation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Run the model and evaluate the results:

.. code-block:: python

    # Perform inference
    predictions = model(dataset)

    # Evaluate the predictions
    results = evaluate(predictions=predictions, data=dataset)

    # Print the results
    print(results.global_scores.summary)
    print(results.instances_scores.summary)

Conclusion
----------

You have now successfully implemented an image-text to text evaluation pipeline with Unitxt. This tool enables the processing of complex multi-modal data, opening up new possibilities for AI applications.

We encourage you to explore further by experimenting with different datasets, models, and tasks to fully leverage Unitxt's capabilities in multi-modal processing.
# docs/docs/glossary.rst
.. _glossary:

========
Glossary
========

.. _artificat:

Artifact
---------

An **Artifact** is a class that can be saved in human readable format in the Unitxt catalog.
Almost all Unitxt classes inherit from the Artifact class.

.. _unitxt_catalog:

Catalog
-------
All Unitxt artifacts -- recipes, data-task cards, templates, pre-processing operators, formats and metrics --
can be stored in the :ref:`Unitxt Catalog <dir_catalog>`.

In addition to the open-source catalog, which can be found in the documentation, users can choose to define a private catalog.
This enables teams and organizations to harness the open Unitxt Catalog while upholding organizational requirements for additional proprietary artifacts.

.. image:: ../../assets/flow_animation_4.gif
   :alt: Optional alt text
   :width: 80%
   :align: center

.. _data_preparation_pipeline:

Data Preparation Pipeline
-------------------------
The **Data Preparation Pipeline** begins with standardizing the raw data into the :ref:`task <task>` interface,
as defined in the :ref:`data-task card <data_task_card>`.
The examples are then verbalized by the :ref:`template <template>`. The :ref:`format <format>` operator applies system prompts,
special tokens and in-context learning examples.
To maintain compatibility, the output of this pipeline is a HuggingFace Dataset that can be saved or pushed to the hub.

The data preparation pipeline can be seen as the top flow in the following figure:

.. _unitxt_flow_diagram:

.. image:: ../../assets/unitxt_flow.png
   :alt: The unitxt flow
   :width: 100%
   :align: center

.. _data_task_card:

Data-Task Card
--------------
**Data-Task Cards** define how raw data is loaded from the dataset source and how it is standardized for a certain task.
Typically, it includes data wrangling actions, e.g., renaming fields,
filtering data instances, modifying values, train/test/val splitting etc.

The catalog contains predefined data-task cards for various datasets :ref:`here <dir_catalog.cards>`.

.. _data_evaluation_pipeline:

Demos (Demonstrations)
----------------------

**Demonstrations** are the examples added to the model prompt in in-context learning.
In-context learning is activated when the  ``num_demos`` parameter of the :ref:`recipe <recipe>` is set to a
non-zero value.   The demonstrations are verbalized by the :ref:`template <template>` and the :ref:`format <format>`
as seen in :ref:`this figure <prompt_structure>`.
Different demo examples are chosen per instance from a fixed set of examples called a ``demo_pool``.
Usually, the examples in the demo pool are taken from the train split,
but this can be overridden by the ``demos_taken_from`` parameter.
The size of the demo pool is determined by a mandatory parameter called the ``demos_pool_size`` parameter.



Evaluation Pipeline
-------------------

The **Evaluation Pipeline** is responsible for producing a list of evaluation scores that reflect model performance on a give dataset.
It includes a de-verbalization of the model outputs (as defined in the :ref:`template <template>`) and a computation of performance
by the metrics defined in the task.

The standardization of the :ref:`task <task>` interface, namely, having fixed names and types for its input and output fields,
allows the use of any metric that accept such fields as input.
In addition to the computed evaluation scores, Unitxt metrics support a built-in mechanism for confidence interval
reporting, using statistical bootstrap.

.. _extensions:

Extensions
-----------
Unitxt supports **Extensions** such as "**input-augmentation**"
(for example, adding random whitespace, introducing spelling mistakes, or replacing words with their synonyms) or
label-noising (replaces the labels in the demonstrations randomly from a list of options).
Such extensions can be added anywhere in the data-preparation pipeline between any two operators, depending on the
desired logic (see :ref:`the unitxt flow diagram <unitxt_flow_diagram>`).

Unitxt supports the addition of custom extensions to the Unitxt Catalog.
Each extension is an independent unit, reusable across different datasets and tasks, templates and formats.


.. _format:

Format
------
A Unitxt **Format** defines a set of additional formatting requirements, unrelated to the underlying data or task, including
those pertaining to system prompts, special tokens or user/agent prefixes, and in-context demonstrations.

Following the example in the  :ref:`figure <prompt_structure>`, the Unitxt format receives the text produced by the template
"**classify the sentence: ``I like toast''**", and adds three things: the system prompt "**<SYS>You are a helpful agent</SYS>**",
the Instruction-User-Agent schema cues, and the two presented demonstrations.

The catalog contains predefined formats :ref:`here <dir_catalog.formats>`.

.. _inference_engine:

Inference Engine
----------------

An **Inference Engine** in Unitxt is an object that performs model inference on Unitxt datasets.
Unitxt provides out of the box inference engines that wrap HuggingFace Pipelines, OpenAI, and IBMGenAI APIs.
Since Unitxt has separate data preparation and evaluation pipelines, you can use any external code or engine to generate
model predictions. The built-in inference engines can make it more convenient.
They also ensure that no sensitive data is passed to external services.
(`See code example here. <https://github.com/IBM/unitxt/blob/main/examples/standalone_qa_evaluation.py>`_)

.. _operator:

Operator
--------

An **Operator** is a class that takes multiple streams as input and produces multiple streams as output.
Every modification of the data in the stream is done by an operator.
Every operator should perform a single task and its name should reflect its operation.

.. image:: ../../assets/flow_animation_3.gif
   :alt: Optional alt text
   :width: 80%
   :align: center

Examples: AddDictToEveryInstance, RenameField, etc.

.. _post_processors:

Post processors
---------------

**Post Processors** are a set of  :ref:`operators <operator>` that de-verbalizes both the string model predictions and string references,
and converts them to the types required by the :ref:`metrics <adding_metric>`.  Each :ref:`template <template>` defines the
set of post processors that are appropriate for it.   For example, post processors in a binary classification
template could remove trailing whitespace, take the first word, convert `Yes` to `1` , and all other values to `0`.

.. _prediction_and_processed_prediction:

Prediction and Processed Prediction
-----------------------------------

A **Prediction** is the output of the model on the input provided to it.
The inference process used to generated the prediction can be done with an Unitxt :ref:`Inference Engine <inference_engine>` or any other
framework or code.  The predictions over all instances are  passed to the evaluation pipeline, together with the original dataset.

The textual predictions returned by the model are processed by the :ref:`Template <template>`'s :ref:`Post Processors <post_processors>`
before being passed to the :ref:`Metrics <adding_metric>`.  The post processors convert the textual prediction to the
type required by the metrics. For example, `Yes` and `No` values could be first normalized to `yes` and `no` and then converted
into `0.0` and `1.0`.

After evaluation, the `prediction` field of each instance in the resulting datasets contains the prediction returned by the model and
the  `processed_prediction` field holds the prediction after post processing by the template.

.. _recipe:

Recipe
------

A **Recipe** holds a complete specification of a \unitxt pipeline.

This includes :ref:`DataTask Card <data_task_card>`, :ref:`Template <template>`,
:ref:`Format <format>` and parameters for different :ref:`Extensions <extensions>`.

.. _references:

References and Processed References
-----------------------------------

**References** are the "correct answers" for the task of a given instance.
They are stored as a list of strings in the `references` field of the generated Unitxt dataset.
For example, a reference for a binary classification task could be `Yes` or `No`.

It is expect that the model will get a perfect score from the metrics if the model prediction
is equal to one of the references.

The textual references are processed by the :ref:`Template <template>`'s :ref:`Post Processors <post_processors>`
before being passed to the :ref:`Metrics <adding_metric>`.  The post processor converts the textual representation
of the references to the type required by the metrics. For example, `Yes` and `No`
values could be converted into `0.0` and `1`.

The `references` field of the dataset contains the textual references, and the resulting dataset after evaluation
contains an additional `processed_references` field with the references after post processing by the template.


.. _target:

Target
------
The **Target** is one of the :ref:`references <references>`.
It is used as the expected model output in in-context learning demonstrations.

.. _stream:

Stream
------

A **Stream** is a sequence of data. It can be finite or infinite. It can be synchronous or asynchronous.
Every instance in the stream is a simple python dictionary.

.. image:: ../../assets/flow_animation_1.gif
   :alt: Optional alt text
   :width: 80%
   :align: center

.. image:: ../../assets/flow_animation_2.gif
   :alt: Optional alt text
   :width: 80%
   :align: center

.. _system_prompt:

System Prompt
-------------

The **System Prompt** is the fixed text that is added to the model input by the :ref:`Format <format>` during
the verbalization process. It is specified by the `system_prompt` parameter of the :ref:`recipe <recipe>`

.. _task:

Task
----

A Unitxt **Task** follows the formal definition of an NLP task, such as multi-label classification, named entity extraction, abstractive summarization or translation.
A task is defined by its standard interface -- namely, input and output fields -- and by its evaluation metrics.
Given a dataset, its contents are standardized into the fields defined by an appropriate task by a :ref:`Data-Task Card <data_task_card>`.

The catalog contains predefined tasks :ref:`here <dir_catalog.tasks>`.

.. _template:

Template
--------

A Unitxt **Template** defines the verbalizations to be applied to the inputs and targets,
as well as the de-verbalization operations over the model predictions.
For example, applying the template to "**I like toast**" verbalizes it into "**classify the sentence: ``I like toast''**":

In the other direction, template de-verbalization involves two steps.
First, a general standardization of the output texts: taking only the first non-empty line of a model's predictions, lowercasing, stripping whitespaces, etc.
The second step standardizes the output to the specific task at hand.
For example, in Sentence Similarity, a prediction may be a quantized float number outputted as a string (e.g ``2.43''),
or a verbally expressed numeric expression (e.g ``two and a half``).
This depends on the verbalization defined by the template and the in-context demonstrations it constructs.
Both types of outputs should be standardized before evaluation begins -- e.g., to a float for sentence similarity.
Having the de-verbalization steps defined within the template enables templates to be reused across different models and datasets.


The templates, datasets and tasks in Unitxt are not exclusively tied.
Each task can harness multiple templates and a template can be used for different datasets.

The catalog contains predefined templates :ref:`here <dir_catalog.templates>`. :ref:`Tasks section <dir_catalog.tasks>`

.. _verbalization:

Verbalization
-------------

**Verbalization** is the process of taking the task fields and converting them into their
textual representation, which is provided as input to the model.

The verbalization process involves multiple components. The :ref:`Template <template>`
verbalizes the task-specific prompt, while the :ref:`Format <format>` and :ref:`System prompt <system_prompt>`
verbalize any model specific requirements (e.g. system prompt, dialog prefixes) as well as in-context examples.

The verbalization involves verbalizing the task input fields for the input, and the task output fields for references.

.. _prompt_structure:
.. image:: ../../assets/prompt_structure.png
   :alt: The unitxt prompt structure
   :width: 75%
   :align: center


Serialization
-------------

Is the process of taking a specific field of a task a converting it into a string.
This process is controlled by the :ref:`Serializer <types_and_serializers>`.
Unlike the verbalization which is task specific, namely putting into words an input of a specific task, the serialization
has nothing to do with task, and is just based on the type of the field.

# docs/docs/adding_task.rst
.. _adding_task:

.. note::

   To use this tutorial, you need to :ref:`install unitxt <install_unitxt>`.


=====================================
Tasks âœ¨
=====================================

Tasks are fundamental to Unitxt, acting as a standardized interface for integrating new datasets, metrics and templates.

The Task schema is a formal definition of the NLP task, including its inputs, outputs, and default evaluation metrics.

The `input_fields` of the task are a set of fields that are used to format the textual input to the model.
The `reference_fields` of the task are a set of fields that are used to format the expected textual output from the model (gold references).
The `metrics` of the task are a set of default metrics to be used to evaluate the outputs of the model.

As an example, consider an evaluation task for LLMs to evaluate how well they are able to calculate the sum of two integer numbers.
The task is formally defined as:

.. code-block:: python

   from unitxt.blocks import Task

   task = Task(
        input_fields={"num1" : int, "num2" : int},
        reference_fields={"sum" : int},
        prediction_type=int,
        metrics=[
            "metrics.sum_accuracy",
            "metrics.sum_accuracy_approximate"
        ],
   )

The `input_fields` and `reference_fields` of the task are used to format the textual input to the model.

The task does not verbalize the input to the model, as this can be done in different ways by different templates.
For example, same input could be verbalized as

`How much is 303 plus 104?`

or as

`How much is three hundred and three plus one hundred and four?`

The `output` fields of the tasks are used to format the textual expected output from the model (gold references).
There may be a single gold reference or multiple.

The gold references are used in two places.  When running in-context learning, gold references are used as example answers.
The gold references are also passed to metrics that are reference based.

The `metrics` of the task are a set of default metrics to be used to evaluate the outputs of the model.

While language models generate textual predictions, the metrics often evaluate on different datatypes.  For example,
in this case, the metrics calculate the accuracy of the sum of two integers, expecting an integer prediction.
It is the responsibility of the templates, via its post processors to convert the model textual predictions
into the `prediction_type`.

A Task can define a `default_template` attribute that determines the default :ref:`Template <adding_template>` used to create the input to the model, 
when the template is not explicitly specified by the user in the :ref:`Card <adding_dataset>` 
definition or in the  :ref:`load_dataset <evaluating_datasets>` template argument.

.. code-block:: python

   from unitxt.blocks import Task
   from unitxt.templates import InputOutputTemplate

   template = InputOutputTemplate(
         input_format="How much is {num1} plus {num2}?",
         output_format="{sum}",
   ) 
    
   task = Task(
        input_fields={"num1" : int, "num2" : int},
        reference_fields={"sum" : int},
        prediction_type=int,
        metrics=[
            "metrics.sum_accuracy",
            "metrics.sum_accuracy_approximate"
        ],
        default_template=template
   )

To register the task to the catalog

.. code-block:: python

   from unitxt import add_to_catalog

   add_to_catalog(task,"tasks.calculator.sum")


# docs/blog/index.rst
.. _blog:

------------
Blog ðŸ“£
------------

.. toctree::
   :maxdepth: 1
   :caption: Read more...

   inference_engines_blog
   vision_robustness_blog
# docs/blog/inference_engines_blog.rst
.. title:: Unitxt Embraces Rich Chat Format and Cross API Inference: Simplifying LLM Evaluation

:Authors: Elron Bandel

:Date: 2024-11-19

=================================================================================================
[19/11/2024] Unitxt Embraces Rich Chat Format and Cross API Inference: Simplifying LLM Evaluation
=================================================================================================

**Authors**: Elron Bandel

``19/11/2024``

Preparing data for training and testing language models is a complex task.
It involves handling various data formats, preprocessing, and ways of verbalizing tasks.
Ensuring reproducibility and compatibility across platforms further adds to the complexity.

Recognizing these challenges, Unitxt has always aimed to simplify data preparation.
Today, we are introducing two major updates to redefine our support for LLM workflows.

Introducing Two Major Enhancements
-----------------------------------

1. **Producing Data in Chat API Format**
   Unitxt can produce data in the widely adopted Chat API format.
   This ensures compatibility with popular LLM Provider APIs and avoid the need for custom per model formatting.
   Additionally, the format supports multiple modalities such as text, images, and videos.

2. **A Comprehensive Array of Inference Engines**
   We added wrappers for local inference platforms like Llama and Hugging Face
   as well as remote APIs such as LiteLLM, OpenAI, Watsonx, and more.
   
   These wrappers make executing evaluation and inference tasks seamless
   and platform-agnostic, in just a `few lines of code <https://github.com/IBM/unitxt/blob/main/examples/evaluate_existing_dataset_with_install.py>`_.


.. code-block:: python

    # Illustration of rich chat api ready for inference:

    [
        {
            "role": "system",
            "content": "You are an assistant that helps classify images."
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "What does this image depict?"
                },
                {
                    "type": "image",
                    "image": {
                        "mime_type": "image/jpeg",
                        "data": <ENCODED_IMAGE>
                    }
                }
            ]
        }
    ]

Expanding Opportunities for the Community
------------------------------------------

These updates unlock significant opportunities, including:

- **Full Evaluation Pipelines**:
  Design and execute `end-to-end workflows <https://www.unitxt.ai/en/latest/docs/examples.html#evaluation-usecases>`_ directly in the Unitxt framework.
  For example, evaluate the impact of different templates, in-context example selection, answering multiple questions in one inference, and more.

- **Multi-Modality Evaluation**:
  Evaluate `models with diverse inputs <https://www.unitxt.ai/en/latest/docs/examples.html#multi-modality>`_, from text to images and beyond.

- **Easy Assembly of LLM Judges**:
  Quickly set up `LLMs as evaluators <https://github.com/IBM/unitxt/blob/main/examples/standalone_evaluation_llm_as_judge.py>`_ using Unitxt inference engines.


Our Commitment to Collaboration
-------------------------------

Although you can now run end to end evaluation in Unitxt, Unitxt is still a general data preparation library.
That means we remain committed to partnerships with other evaluation platforms such as `HELM <https://www.unitxt.ai/en/latest/docs/helm.html>`_, `LM Eval Harness <https://www.unitxt.ai/en/latest/docs/lm_eval.html>`_, and others.
Our Chat API format and inference engine support enhance accessibility and compatibility.
These updates empower our partners to adopt the latest standards seamlessly.

Conclusion
----------

Unitxt is adapting to the evolving landscape of language models and their capabilities.
By supporting the Chat API format and inference engines, we simplify model workflows.
These updates position Unitxt as the premier platform for LLM evaluation and integration.

We invite you to explore these features and join us in advancing model capabilities.

---

For more information, visit the :ref:`inference engines guide <inference>` or see many of our :ref:`code examples <examples>`.

# docs/blog/vision_robustness_blog.rst
.. title:: If Your LLM sees White Noise, Try Asking Differently: Revealing AIâ€™s Text and Image Sensitivities with Unitxt

:Authors:
    Elron Bandel
    Nimrod Shabtay

:Date: 2024-11-01

==========================================================================================================================
[01/11/2024] If Your LLM sees White Noise, Try Asking Differently: Revealing AIâ€™s Text and Image Sensitivities with Unitxt
==========================================================================================================================

**Authors**: Elron Bandel and Nimrod Shabtay

``01/11/2024``

Introduction: When Models Struggle with Visual Noise
----------------------------------------------------

AI models often appear remarkably accurate when evaluated in clean, controlled environments. But in real-world settings, these models can stumbleâ€”particularly when faced with image quality issues, like old TV-style white noise, or subtle variations in question formatting. Even for multiple-choice questions, something as simple as how answer options are enumerated (capitalized vs. lowercase) can impact performance, showing that models arenâ€™t always as robust as they seem.

In this post, weâ€™ll use the Unitxt library to evaluate a modelâ€™s robustness by simulating â€œold TVâ€ white noise, replacing 30% of pixels with random noise. Weâ€™ll also test the impact of different answer enumerations, using both uppercase and lowercase options, to reveal how sensitive the model is to these variations. The results highlight the importance of robust evaluation and tailored question formats for optimal model performance. Unitxt, as the largest hub for datasets and diverse data augmentation operators, enables precisely this kind of comprehensive testing, making it an invaluable tool for building more resilient AI models.

The Challenge: White Noise and Answer Enumeration Sensitivity
-------------------------------------------------------------

Imagine using a model to answer questions based on degraded images. This could happen in scenarios with low-quality visuals or historical media with degraded quality. Add to this the effect of changing the text format of multiple-choice answer options, like capitalizing the options (â€œA, B, Câ€) or using lowercase (â€œa, b, câ€). Small differences like these reveal model sensitivities and limitations that are otherwise hidden in standard tests.

With Unitxt, we can set up these tests easily, benchmarking how well the model performs under different noise and enumeration settings.

Setting Up the Test with Unitxt
-------------------------------

Hereâ€™s the code used to set up our tests. This example uses Unitxt to create subsets for both clear and noisy images with variations in multiple-choice answer enumeration.

.. code-block:: python

    subsets={}
    for card in ["cards.seed_bench", "cards.ai2d"]:
        for enumerator in ["capitals", "lowercase"]:
            for augmentor in [None, "augmentors.image.white_noise"]:
                subsets[f"{card} {enumerator} {augmentor}"] = DatasetRecipe(
                    card=card,
                    template=f"templates.qa.multiple_choice.with_context.lmms_eval[enumerator={enumerator}]",
                    loader_limit=100,
                    augmentor=augmentor,
                )

    benchmark = Benchmark(subsets=subsets)

    data = list(benchmark()["test"])

    model = LMMSEvalInferenceEngine(
        model_type="llava_onevision",
        model_args={"pretrained": "lmms-lab/llava-onevision-qwen2-7b-ov"},
        max_new_tokens=2,
    )

    predictions = model(data)
    results = evaluate(predictions=predictions, data=data)

    print(results.subsets_scores.summary)

In order to run this you will first have to install llms-eval library which might not work on mac.

*Full code example at:* https://github.com/IBM/unitxt/blob/main/examples/robustness_testing_for_vision_text_models.py

In this setup:

* **Cards and Enumerators**: We test both â€œcards.ai2dâ€ and â€œcards.seed_benchâ€ datasets, varying the enumeration of multiple-choice answers with either â€œcapitalsâ€ (A, B, C) or â€œlowercaseâ€ (a, b, c).
* **Noise Augmentation**: We simulate â€œold TVâ€ white noise with "augmentors.image.white_noise", obscuring 30% of the image pixels to evaluate the modelâ€™s performance on degraded visuals.

The model we use is the one vision model who recently gained popularity at the open source vision-language community.

Visual Examples From The Data
------------------------------
Example from AI2D:

.. image:: ../../assets/blog/vision_rubstness/ai2d.png
   :alt: Results
   :width: 80%
   :align: center

.. list-table::
   :header-rows: 1

   * - With Uppercase
     - With Lowercase
   * - .. code-block::

         Which of these define oil.
         A. b
         B. a
         C. d
         D. k.
         Answer with the option's letter from the given choices directly.
         D
     - .. code-block::

         Which of these define oil.
         a. b
         b. a
         c. d
         d. k.
         Answer with the option's letter from the given choices directly.
         d

Example from Seed-bench:

.. image:: ../../assets/blog/vision_rubstness/seed_bench.png
   :alt: Results
   :width: 80%
   :align: center

.. list-table::
   :header-rows: 1

   * - With Uppercase
     - With Lowercase
   * - .. code-block::

         How many men are on the airplane in the image?
         A. One
         B. Two
         C. Three
         D. Four
         Answer with the option's letter from the given choices directly.
         B
     - .. code-block::

         How many men are on the airplane in the image?
         a. One
         b. Two
         c. Three
         d. Four
         Answer with the option's letter from the given choices directly.
         b

Results: Performance Across Formats and Noise Levels
----------------------------------------------------

Below are the results for each setup. Note that â€œUppercaseâ€ indicates multiple-choice questions with capitalized answer options, while â€œLowercaseâ€ uses lowercase options. The percentage differences between clean images and those with white noise provide insight into how sensitive the model is to these variations.

.. image:: ../../assets/blog/vision_rubstness/results.png
   :alt: Results
   :width: 80%
   :align: center

These results reveal a significant performance drop when noise is introduced, particularly with lowercase answer enumeration. On the Seed-Bench dataset, for example, the lowercase format drops from 0.24 to 0.17 with noiseâ€”a 29.17% reduction. Meanwhile, the capitalized options show a smaller but still notable drop, from 0.75 to 0.71 (a 5.33% decrease). The AI2D dataset shows similar patterns, with the capitalized options decreasing from 0.82 to 0.75 under noise (an 8.54% drop), while lowercase options remain consistent at 0.53 across conditions. This highlights variability between datasets and how sensitive performance can be to noise, particularly for lowercase answer options.

These findings emphasize a crucial takeaway: if you have noisy data, framing your questions and options correctly is essential. Small changes in formatting can make a big difference, so for robust performance, rememberâ€”if your data is noisy, youâ€™d better ask your model in the right way!

Conclusion: Understanding and Addressing Model Sensitivities
------------------------------------------------------------

Our test with Unitxt demonstrates that models can be highly sensitive to seemingly minor variations, such as noise in images and the formatting of answer choices. The â€œold TVâ€ white noise consistently reduced performance, especially when combined with lowercase enumeration.

By using tools like Unitxt, we can quickly evaluate these nuances and gain a deeper understanding of model strengths and weaknesses. Knowing these sensitivities helps in designing more robust models and ensures theyâ€™re better suited for real-world conditionsâ€”whether they encounter noisy images or unexpected answer formats. As the largest hub for datasets and a wide range of data augmentation operators, Unitxt empowers comprehensive testing, making it a go-to resource for building resilient, real-world-ready AI models.# examples/evaluate_classification_dataset_with_given_predictions.py
from unitxt.api import create_dataset, evaluate

classes = ["positive", "negative"]

dataset = [
    {"text": "I am happy.", "label": "positive", "classes": classes},
    {"text": "It was a great movie.", "label": "positive", "classes": classes},
    {"text": "I never felt so bad", "label": "negative", "classes": classes},
]

predictions = ["Positive.", "negative.", "negative"]

dataset = create_dataset(
    task="tasks.classification.multi_class",
    format="formats.chat_api",
    test_set=dataset,
    postprocessors=["processors.take_first_word", "processors.lower_case"],
)

results = evaluate(predictions, dataset["test"])

# Print Results:

print(f"Final Score ({results.global_scores.score_name}):")
print(results.global_scores.score)

print("Global Results:")
print(results.global_scores.summary)

print("Instance Results:")
print(results.instance_scores.summary)

# examples/evaluate_tables_benchmark.py
from unitxt import evaluate, load_dataset, settings
from unitxt.inference import (
    CrossProviderInferenceEngine,
)

with settings.context(
    disable_hf_datasets_cache=False,
    allow_unverified_code=True,
    mock_inference_mode=True,
):
    test_dataset = load_dataset(
        "benchmarks.tables_benchmark[loader_limit=30,max_samples_per_subset=30]", split="test"
    )

# Infer
model = CrossProviderInferenceEngine(
    model="llama-3-8b-instruct",
    max_tokens=30,
)
"""
We are using a CrossProviderInferenceEngine inference engine that supply api access to provider such as:
watsonx, bam, openai, azure, aws and more.

For the arguments these inference engines can receive, please refer to the classes documentation or read
about the the open ai api arguments the CrossProviderInferenceEngine follows.
"""

predictions = model(test_dataset)
results = evaluate(predictions=predictions, data=test_dataset)

results.instance_scores.to_df().to_csv("results.csv")
print("Global scores:")
print(results.global_scores.summary)
print("Subsets scores:")
print(results.subsets_scores.summary)

# examples/evaluate_llm_as_judge_direct_predefined_criteria.py
from unitxt import get_logger
from unitxt.api import create_dataset, evaluate

logger = get_logger()

data = [
    {"question": "How is the weather?"},
    {"question": "How is the weather?"},
    {"question": "How is the weather?"},
]

criteria = "metrics.llm_as_judge.direct.criterias.temperature_in_celsius_and_fahrenheit"
metrics = [
    f"metrics.llm_as_judge.direct.rits.llama3_1_70b[criteria={criteria}, context_fields=[question]]"
]

dataset = create_dataset(
    task="tasks.qa.open", test_set=data, metrics=metrics, split="test"
)

predictions = [
    """On most days, the weather is warm and humid, with temperatures often soaring into the high 80s and low 90s Fahrenheit (around 31-34Â°C). The dense foliage of the jungle acts as a natural air conditioner, keeping the temperature relatively stable and comfortable for the inhabitants.""",
    """On most days, the weather is warm and humid, with temperatures often soaring into the high 80s and low 90s Fahrenheit. The dense foliage of the jungle acts as a natural air conditioner, keeping the temperature relatively stable and comfortable for the inhabitants.""",
    """On most days, the weather is warm and humid. The dense foliage of the jungle acts as a natural air conditioner, keeping the temperature relatively stable and comfortable for the inhabitants.""",
]

results = evaluate(predictions=predictions, data=dataset)

print("Global Scores:")
print(results.global_scores.summary)

print("Instance Scores:")
print(results.instance_scores.summary)

# examples/evaluate_image_text_to_text_lmms_eval_inference.py
from unitxt import settings
from unitxt.api import evaluate, load_dataset
from unitxt.inference import (
    LMMSEvalInferenceEngine,
)

with settings.context(
    disable_hf_datasets_cache=False,
):
    dataset = load_dataset(
        card="cards.seed_bench",
        format="formats.chat_api",
        loader_limit=30,
        split="test",
    )

    model = LMMSEvalInferenceEngine(
        model_type="llava",
        model_args={
            "pretrained": "liuhaotian/llava-v1.5-7b",
        },
        max_new_tokens=2,
    )

    predictions = model(dataset)
    results = evaluate(predictions=predictions, data=dataset)

    print("Global Results:")
    print(results.global_scores.summary)

    print("Instance Results:")
    print(results.instance_scores.summary)

# examples/evaluate_existing_dataset_no_install.py
import json

import evaluate
from datasets import load_dataset

# Use the HF load_dataset API, to load the wnli entailment dataset using the standard template in the catalog for relation task with 2-shot in-context learning.
# We set loader_limit to 200 to limit reduce download time.
dataset = load_dataset(
    "unitxt/data",
    "card=cards.wnli,template=templates.classification.multi_class.relation.default,num_demos=2,demos_pool_size=100,loader_limit=200",
    trust_remote_code=True,
    split="test",
)

# Print the resulting dataset.
# The 'source' field contains the input to the model, and the 'references' field contains
# that expected answer.

print("Sample dataset instance:")
print(json.dumps(dataset[0], indent=4))

# Generate predictions which are always entailment. Can be replaced with any inference method.
predictions = ["entailment" for _ in dataset]

# Use the huggingface evaluate API to evaluate using the built in metrics for the task
# (f1_micro, f1_macro, accuracy, including confidence intervals)

metric = evaluate.load("unitxt/metric")
results = metric.compute(predictions=predictions, references=dataset)

print("Global Results:")
print(results.global_scores.summary)

# examples/inference_using_ibm_watsonx_ai.py
import os

from unitxt.api import load_dataset
from unitxt.inference import WMLInferenceEngine
from unitxt.text_utils import print_dict

if __name__ == "__main__":
    # Set required env variables using your WML credentials:
    os.environ["WML_URL"] = ""
    os.environ["WML_PROJECT_ID"] = ""
    os.environ["WML_APIKEY"] = ""

    # Preparing WML inference engine:
    model_name = "google/flan-t5-xl"
    wml_inference = WMLInferenceEngine(
        model_name=model_name,
        data_classification_policy=["public"],
        random_seed=111,
        min_new_tokens=16,
        max_new_tokens=128,
        top_p=0.5,
        top_k=1,
        repetition_penalty=1.5,
        decoding_method="greedy",
    )

    # Loading dataset:
    dataset = load_dataset(
        card="cards.go_emotions.simplified",
        template="templates.classification.multi_label.empty",
        loader_limit=3,
    )
    test_data = dataset["test"]

    # Performing inference:
    predictions = wml_inference(test_data)
    for inp, prediction in zip(test_data, predictions):
        result = {**inp, "prediction": prediction}
        print_dict(result, keys_to_print=["source", "prediction"])

# examples/robustness_testing_for_vision_text_models.py
from unitxt import settings
from unitxt.api import evaluate
from unitxt.benchmark import Benchmark
from unitxt.inference import (
    LMMSEvalInferenceEngine,
)
from unitxt.logging_utils import get_logger
from unitxt.standard import DatasetRecipe

logger = get_logger()

with settings.context(
    disable_hf_datasets_cache=False,
):
    subsets = {}
    for card in ["cards.seed_bench", "cards.ai2d"]:
        for enumerator in ["capitals", "lowercase"]:
            for augmentor in [None, "augmentors.image.white_noise"]:
                subsets[f"{card} {enumerator} {augmentor}"] = DatasetRecipe(
                    card=card,
                    template=f"templates.qa.multiple_choice.with_context.lmms_eval[enumerator={enumerator}]",
                    format="formats.chat_api",
                    loader_limit=30,
                    augmentor=augmentor,
                )

    benchmark = Benchmark(subsets=subsets, max_samples_per_subset=5)

    data = benchmark()["test"].to_dataset()

    model = LMMSEvalInferenceEngine(
        model_type="llava",
        model_args={"pretrained": "llava-hf/llava-v1.6-mistral-7b-hf"},
        max_new_tokens=2,
    )

    predictions = model(data)
    results = evaluate(predictions=predictions, data=data)

    print("Subsets scores:")
    print(results.subsets_scores.summary)

# examples/evaluate_image_text_to_text_vllm_inference.py
from unitxt import settings
from unitxt.api import evaluate, load_dataset
from unitxt.inference import (
    VLLMInferenceEngine,
)

with settings.context(
    disable_hf_datasets_cache=False,
):
    dataset = load_dataset(
        card="cards.seed_bench",
        format="formats.chat_api",
        loader_limit=30,
        split="test",
    )

    inference_model = VLLMInferenceEngine(
        model="microsoft/Phi-3-vision-128k-instruct",
        max_tokens=2,
    )

    predictions = inference_model(dataset)
    results = evaluate(predictions=predictions, data=dataset)

    print("Global Results:")
    print(results.global_scores.summary)

    print("Instance Results:")
    print(results.instance_scores.summary)

# examples/evaluate_different_templates.py
import os
import tempfile

import pandas as pd
from unitxt import add_to_catalog, register_local_catalog
from unitxt.api import evaluate, load_dataset
from unitxt.inference import CrossProviderInferenceEngine
from unitxt.templates import InputOutputTemplate


# Register a local catalog
def create_path_and_register_as_local_catalog(path):
    if not os.path.exists(path):
        os.mkdir(path)
    register_local_catalog(path)
    return path


catalog_dir = tempfile.gettempdir()  # You can replace with any fixed directory
my_catalog = create_path_and_register_as_local_catalog(catalog_dir)


# Add two templates for entailment tasks to local catalog:
# One template embeds the hypothesis and premise into a single sentence question
# The other templates, places the hypothesis and premise in separate fields with a field prefix.
template1 = InputOutputTemplate(
    input_format='Is "{text_b}" entailed by, neutral to, or contradicts "{text_a}". Answer with one of these following options: {classes}.',
    output_format="{label}",
    postprocessors=[
        "processors.take_first_non_empty_line",
        "processors.lower_case_till_punc",
    ],
)
add_to_catalog(
    template1,
    "templates.my_entailment_as_question",
    catalog_path=my_catalog,
    overwrite=True,
)

template2 = InputOutputTemplate(
    instruction="Indicate whether each hypothesis is entailed by, neutral to, or contradicts the premise. Answer with one of these following options: {classes}.",
    input_format="Premise:\n{text_a}\nHypothesis:\n{text_b}\nEntailment:\n",
    output_format="{label}",
    postprocessors=[
        "processors.take_first_non_empty_line",
        "processors.lower_case_till_punc",
    ],
)
add_to_catalog(
    template2,
    "templates.my_entailment_as_fields",
    catalog_path=my_catalog,
    overwrite=True,
)

# Run inference on mnli (entailment task) on the two templates with both 0 and 3 shot in context learning.
model = CrossProviderInferenceEngine(model="llama-3-2-1b-instruct", max_tokens=32)
"""
We are using a CrossProviderInferenceEngine inference engine that supply api access to provider such as:
watsonx, bam, openai, azure, aws and more.

For the arguments these inference engines can receive, please refer to the classes documentation or read
about the the open ai api arguments the CrossProviderInferenceEngine follows.
"""

df = pd.DataFrame(columns=["template", "num_demos", "f1_micro", "ci_low", "ci_high"])

for template in [
    "templates.my_entailment_as_question",
    "templates.my_entailment_as_fields",
]:
    for num_demos in [0, 3]:
        dataset = load_dataset(
            card="cards.mnli",
            template=template,
            format="formats.chat_api",
            num_demos=num_demos,
            demos_pool_size=100,
            loader_limit=500,
            max_test_instances=10,
            split="test",
        )

        predictions = model(dataset)

        results = evaluate(predictions=predictions, data=dataset)

        print(
            f"Sample input and output for template '{template}' and num_demos '{num_demos}':"
        )
        print(
            results.instance_scores.to_df(
                columns=["source", "prediction", "processed_prediction"]
            ),
        )

        global_scores = results.global_scores

        df.loc[len(df)] = [
            template,
            num_demos,
            global_scores["score"],
            global_scores["score_ci_low"],
            global_scores["score_ci_high"],
        ]

df = df.round(decimals=2)
print(df.to_markdown())

# examples/evaluate_rag.py
import json
import sys
from ast import literal_eval

import pandas as pd
from unitxt.eval_utils import evaluate

if __name__ == "__main__":
    df = pd.read_csv(
        filepath_or_buffer=sys.argv[1],
        converters={
            "ground_truths": literal_eval,
            "ground_truths_context_ids": literal_eval,
            "contexts": literal_eval,
            "context_ids": literal_eval,
        },
    )

    # passing list of dicts
    result, _ = evaluate(
        df.to_dict("records"),
        metric_names=[
            "metrics.rag.mrr",
            "metrics.rag.map",
            "metrics.rag.answer_correctness",
        ],
    )
    with open("dataset_out.json", "w") as f:
        json.dump(result, f, indent=4)

    result, _ = evaluate(
        df,
        metric_names=[
            # default implementations
            "metrics.rag.context_correctness",
            "metrics.rag.context_relevance",
            "metrics.rag.context_perplexity",
            "metrics.rag.faithfulness",
            "metrics.rag.answer_reward",
            "metrics.rag.answer_correctness",
            # specific implementations
            "metrics.rag.context_correctness.mrr",
            "metrics.rag.context_correctness.map",
            "metrics.rag.context_relevance.perplexity_flan_t5_small",
            "metrics.rag.context_relevance.sentence_bert_bge",
            "metrics.rag.context_relevance.sentence_bert_mini_lm",
            "metrics.rag.faithfulness.token_k_precision",
            "metrics.rag.faithfulness.bert_score_k_precision",
            "metrics.rag.faithfulness.sentence_bert_bge",
            "metrics.rag.faithfulness.sentence_bert_mini_lm",
            "metrics.rag.answer_correctness.token_recall",
            "metrics.rag.answer_correctness.bert_score_recall",
            "metrics.rag.answer_correctness.sentence_bert_bge",
            "metrics.rag.answer_correctness.sentence_bert_mini_lm",
        ],
    )
    result.round(2).to_csv("dataset_out.csv")

# examples/evaluate_image_text_to_text_with_different_templates.py
from unitxt import settings
from unitxt.api import evaluate
from unitxt.benchmark import Benchmark
from unitxt.inference import (
    LMMSEvalInferenceEngine,
)
from unitxt.logging_utils import get_logger
from unitxt.standard import DatasetRecipe

logger = get_logger()

with settings.context(
    disable_hf_datasets_cache=False,
):
    card = "cards.seed_bench"

    dataset = Benchmark(
        subsets={
            "capitals": DatasetRecipe(
                card=card,
                template="templates.qa.multiple_choice.with_context.lmms_eval[enumerator=capitals]",
                loader_limit=20,
            ),
            "lowercase": DatasetRecipe(
                card=card,
                template="templates.qa.multiple_choice.with_context.lmms_eval[enumerator=lowercase]",
                loader_limit=20,
            ),
            "capitals-greyscale": DatasetRecipe(
                card=card,
                template="templates.qa.multiple_choice.with_context.lmms_eval[enumerator=capitals]",
                loader_limit=20,
                augmentor="augmentors.image.grid_lines",
            ),
        },
    )

    data = list(dataset()["test"])

    model = LMMSEvalInferenceEngine(
        model_type="llava_onevision",
        model_args={"pretrained": "lmms-lab/llava-onevision-qwen2-7b-ov"},
        max_new_tokens=2,
    )

    predictions = model(data)
    results = evaluate(predictions=predictions, data=data)

    for subset in dataset.subsets:
        logger.info(
            f'{subset.title()}: {results[0]["score"]["subsets"][subset]["score"]}'
        )

# examples/evaluate_benchmark_with_custom_provider.py
from unitxt import evaluate, load_dataset
from unitxt.inference import CrossProviderInferenceEngine

data = load_dataset(
    "benchmarks.glue[max_samples_per_subset=5, format=formats.chat_api, system_prompt=system_prompts.general.be_concise]",
    split="test",
    disable_cache=False,
)

model = CrossProviderInferenceEngine(
    model="llama-3-8b-instruct", temperature=0.0, top_p=1.0, provider="watsonx"
)
"""
We are using a CrossProviderInferenceEngine inference engine that supply api access to provider such as:
watsonx, bam, openai, azure, aws and more.

For the arguments these inference engines can receive, please refer to the classes documentation or read
about the the open ai api arguments the CrossProviderInferenceEngine follows.
"""

predictions = model(data)

results = evaluate(predictions=predictions, data=data)

print("Global Results:")
print(results.global_scores.summary)

print("Instance Results:")
print(results.instance_scores.summary)

# examples/evaluate_bluebench.py
from unitxt import evaluate, load_dataset, settings
from unitxt.inference import (
    CrossProviderInferenceEngine,
)

with settings.context(
    disable_hf_datasets_cache=False,
    allow_unverified_code=True,
    mock_inference_mode=True,
):
    test_dataset = load_dataset(
        "benchmarks.bluebench[loader_limit=30,max_samples_per_subset=30]", split="test"
    )

# Infer
model = CrossProviderInferenceEngine(
    model="llama-3-8b-instruct",
    max_tokens=30,
)
"""
We are using a CrossProviderInferenceEngine inference engine that supply api access to provider such as:
watsonx, bam, openai, azure, aws and more.

For the arguments these inference engines can receive, please refer to the classes documentation or read
about the the open ai api arguments the CrossProviderInferenceEngine follows.
"""

predictions = model(test_dataset)
results = evaluate(predictions=predictions, data=test_dataset)

print("Global scores:")
print(results.global_scores.summary)
print("Subsets scores:")
print(results.subsets_scores.summary)

# examples/evaluate_rag_using_binary_llm_as_judge.py
from unitxt import get_logger
from unitxt.api import evaluate, load_dataset
from unitxt.blocks import TaskCard
from unitxt.inference import WMLInferenceEngine
from unitxt.loaders import LoadFromDictionary
from unitxt.templates import TemplatesDict

logger = get_logger()

# some input rag examples
test_examples = [
    {
        "question": "What foundation models are available in watsonx.ai ?",
        "contexts": [
            "Supported foundation models available with watsonx.ai. Watsonx.ai offers numerous foundation models."
        ],
        "contexts_ids": [0],
        "reference_answers": ["Many Large Language Models are supported by Watsonx.ai"],
    },
    {
        "question": "What foundation models are available in watsonx.ai ?",
        "contexts_ids": [0],
        "contexts": [
            "Supported foundation models available with Meta. Meta AI offers numerous foundation models."
        ],
        "reference_answers": ["Many Large Language Models are supported by Watsonx.ai"],
    },
]

if __name__ == "__main__":
    # define our card for the generation task
    card = TaskCard(
        # Load the data from the dictionary.
        loader=LoadFromDictionary(
            data={"test": test_examples}, data_classification_policy=["public"]
        ),
        # define generation task and templates
        task="tasks.rag.response_generation",
        templates=TemplatesDict(
            {
                "please_respond": "templates.rag.response_generation.please_respond",
                "answer_based_on_context": "templates.rag.response_generation.answer_based_on_context",
            }
        ),
    )

    # Select the desired metric(s).
    # Each metric measures a certain aspect of the generated answer (answer_correctness, faithfulness,
    # answer_relevance, context_relevance and correctness_holistic).
    # All available metrics are under "catalog.metrics.rag"
    # Those with extension "logprobs" provide a real value prediction in [0,1], the others provide a binary prediction.
    # By default, all judges use llama_3_1_70b_instruct_wml. We will soon see how to change this.
    metric_name = "metrics.rag.answer_correctness.llama_3_1_70b_instruct_wml_q_a_gt_loose_logprobs"

    # The binary rag judges tasks expect the input fields among the following: "question", "contexts", "ground_truths".
    # In our generation task the ground truth are in the "reference_answers" field, so we need to inform the metric
    # about this mapping. This is done using the "judge_to_generator_fields_mapping" attribute:
    mapping_override = (
        "judge_to_generator_fields_mapping={ground_truths=reference_answers}"
    )
    correctness_judge_metric_llama = f"{metric_name}[{mapping_override}]"

    # We can also use another inference model by overriding the "model" attribute of the metric.
    # all available models for this judge are under "catalog.engines.classification"
    mixtral_engine = "engines.classification.mixtral_8x7b_instruct_v01_wml"
    correctness_judge_metric_mixtral = (
        f"{metric_name}[{mapping_override}, model={mixtral_engine}]"
    )

    metrics = [correctness_judge_metric_llama, correctness_judge_metric_mixtral]

    # Verbalize the dataset using the template
    dataset = load_dataset(
        card=card, template_card_index="answer_based_on_context", metrics=metrics
    )
    test_dataset = dataset["test"]

    # Infer using flan t5 xl using wml
    model_name = "google/flan-t5-xl"
    model = WMLInferenceEngine(model_name=model_name, max_new_tokens=32)
    predictions = model(test_dataset)

    # Evaluate the generated predictions using the selected metrics
    results = evaluate(predictions=predictions, data=test_dataset)

    print("Global Results:")
    print(results.global_scores.summary)

    print("Instance Results:")
    print(results.instance_scores.summary)

# examples/evaluate_ensemble_judge.py
from unitxt import get_logger
from unitxt.api import evaluate, load_dataset
from unitxt.blocks import Task, TaskCard
from unitxt.loaders import LoadFromDictionary
from unitxt.templates import InputOutputTemplate, TemplatesDict

logger = get_logger()

data = {
    "test": [
        {"conversation": "user: is eugene from walking dead really a scientist"},
        {"conversation": "user: is eugene from walking dead really a scientist"},
        {
            "conversation": "user: is eugene from walking dead really a scientist\nagent: No, Eugene from The Walking Dead is not a real scientist. He lied about being a scientist to manipulate the other survivors into taking him to Washington D.C., believing it to be the best chance for survival. In reality, he is a high school science teacher who does not know how to cure the virus. However, he is shown to be highly intelligent and resourceful, able to do things such as repairing the solar power system in Alexandria and making bullets for the Saviors.\nuser: Why did he lied about himself being a scientist?"
        },
    ]
}

predictions = [
    "In the Walking Dead series, Eugene Porter initially claims to be a scientist who knows the cure to the zombie plague. However, it is eventually revealed that he is not a scientist, but a high school science teacher who lied about knowing the cure to manipulate other survivors into helping him.",
    "thank you for the question.",
    "Eugene lied about being a scientist to gain the trust and protection of Abraham and Rosita, as well as to believe that going to Washington D.C. was the best chance for survival. He was not a scientist and did not know how to cure the virus, but he used his intelligence and resourcefulness to help the group survive in other ways.",
]

card = TaskCard(
    loader=LoadFromDictionary(data=data),
    task=Task(
        input_fields={"conversation": "str"},
        reference_fields={},
        prediction_type="str",
        metrics=[
            "metrics.llm_as_judge.conversation_answer_topicality.ensemble_v1_ibmgenai_judges"
        ],
    ),
    templates=TemplatesDict(
        {
            "simple": InputOutputTemplate(
                input_format="{conversation}",
                output_format="",
            )
        }
    ),
)

test_dataset = load_dataset(card=card, template_card_index="simple")["test"]
results = evaluate(predictions=predictions, data=test_dataset)

print("Global Results:")
print(results.global_scores.summary)

print("Instance Results:")
print(results.instance_scores.summary)

# examples/evaluate_llm_as_judge_pairwise_criteria_from_dataset.py
from typing import Any, List

from unitxt import evaluate, load_dataset
from unitxt.blocks import Task, TaskCard
from unitxt.llm_as_judge_operators import (
    CreateCriteriaFromString,
)
from unitxt.loaders import LoadFromDictionary
from unitxt.templates import NullTemplate

data = {
    "test": [
        {
            "question": "How is the weather?",
            "judgement": "The temperature is described in both Fahrenheit and Celsius.",
        },
        {
            "question": "Tell me a joke about cats",
            "judgement": "Is the response funny?",
        },
    ]
}

card = TaskCard(
    loader=LoadFromDictionary(data=data, data_classification_policy=["public"]),
    preprocess_steps=[
        CreateCriteriaFromString(field="judgement", to_field="criteria"),
    ],
    task=Task(
        input_fields={"question": str},
        reference_fields={"criteria": Any},
        prediction_type=List[str],
        metrics=[
            "metrics.llm_as_judge.pairwise.rits.llama3_1_70b[context_fields=question,criteria_field=criteria]"
        ],
        default_template=NullTemplate(),
    ),
)

test_dataset = load_dataset(card=card, split="test")

predictions = [
    [
        """On most days, the weather is warm and humid, with temperatures often soaring into the high 80s and low 90s Fahrenheit (around 31-34Â°C). The dense foliage of the jungle acts as a natural air conditioner, keeping the temperature relatively stable and comfortable for the inhabitants.""",
        """On most days, the weather is warm and humid, with temperatures often soaring into the high 80s and low 90s Fahrenheit. The dense foliage of the jungle acts as a natural air conditioner, keeping the temperature relatively stable and comfortable for the inhabitants.""",
        """On most days, the weather is warm and humid. The dense foliage of the jungle acts as a natural air conditioner, keeping the temperature relatively stable and comfortable for the inhabitants.""",
    ],
    [
        """Why did the cat cross the road? To cat to the other side.""",
        """Why did the cat sit on the computer? Because it wanted to keep an eye on the mouse!""",
        """What is red, yellow and green? A traffic light.""",
    ],
]

results = evaluate(predictions=predictions, data=test_dataset)

print("Global Scores:")
print(results.global_scores.summary)

print("Instance Scores:")
print(results.instance_scores.summary)

# examples/evaluate_a_judge_model_capabilities_on_arena_hard.py
from unitxt import evaluate, load_dataset
from unitxt.inference import CrossProviderInferenceEngine

"""
We are evaluating only on a small subset (by using `max_test_instances=4`), in order for the example to finish quickly.
The dataset full size if around 40k examples. You should use around 1k-4k in your evaluations.
"""
dataset = load_dataset(
    card="cards.arena_hard.response_assessment.pairwise_comparative_rating.both_games_gpt_4_judge",
    template="templates.response_assessment.pairwise_comparative_rating.arena_hard_with_shuffling",
    format="formats.chat_api",
    max_test_instances=None,
    split="test",
).select(range(5))

model = CrossProviderInferenceEngine(model="llama-3-2-1b-instruct", provider="watsonx")
"""
We are using a CrossProviderInferenceEngine inference engine that supply api access to provider such as:
watsonx, bam, openai, azure, aws and more.

For the arguments these inference engines can receive, please refer to the classes documentation or read
about the the open ai api arguments the CrossProviderInferenceEngine follows.
"""

predictions = model(dataset)
results = evaluate(predictions=predictions, data=dataset)

# Print Results:
print(results.global_scores.summary)

# examples/evaluate_idk_judge.py
from unitxt import get_logger
from unitxt.api import evaluate, load_dataset
from unitxt.blocks import Task, TaskCard
from unitxt.loaders import LoadFromDictionary
from unitxt.templates import InputOutputTemplate, TemplatesDict

logger = get_logger()


data = {
    "test": [
        {
            "inquiry": "Where can I find more information on health centers?",
        },
        {
            "inquiry": "How do I connect to customer care representative?",
        },
        {
            "inquiry": "How do I connect to customer care representative?",
        },
    ]
}

predictions = [
    "I am sorry, but the provided document does not contain answer to your question.",
    "The document does not provide a specific answer to your question.",
    "Hello, you can chat with a representative by clicking on the chat icon at the top of the page.",
]

card = TaskCard(
    loader=LoadFromDictionary(data=data),
    task=Task(
        input_fields={"inquiry": "str"},
        reference_fields={},
        prediction_type="str",
        metrics=[
            "metrics.llm_as_judge.conversation_answer_idk.llama3_v1_ibmgenai_judges"
        ],
    ),
    templates=TemplatesDict(
        {
            "simple": InputOutputTemplate(
                input_format="{inquiry}",
                output_format="",
            )
        }
    ),
)

dataset = load_dataset(card=card, template_card_index="simple", split="test")
results = evaluate(predictions=predictions, data=dataset)

print("Global Results:")
print(results.global_scores.summary)

print("Instance Results:")
print(results.instance_scores.summary)

# examples/evaluate_llm_as_judge_pairwise_predefined_criteria.py
from typing import Any, List

from unitxt import evaluate, load_dataset
from unitxt.blocks import Task, TaskCard
from unitxt.llm_as_judge_operators import LoadCriteria
from unitxt.loaders import LoadFromDictionary
from unitxt.templates import NullTemplate

data = {
    "test": [
        {
            "question": "How is the weather?",
            "criteria": "metrics.llm_as_judge.pairwise.criterias.temperature_in_celsius_and_fahrenheit",
        },
        {
            "question": "Tell me a joke about cats",
            "criteria": "metrics.llm_as_judge.pairwise.criterias.funny_joke",
        },
    ]
}

card = TaskCard(
    loader=LoadFromDictionary(data=data, data_classification_policy=["public"]),
    preprocess_steps=[
        LoadCriteria(field="criteria", to_field="criteria"),
    ],
    task=Task(
        input_fields={"question": str},
        reference_fields={"criteria": Any},
        prediction_type=List[str],
        metrics=[
            "metrics.llm_as_judge.pairwise.watsonx.llama3_1_70b[context_fields=question,criteria_field=criteria]"
        ],
        default_template=NullTemplate(),
    ),
)

dataset = load_dataset(card=card, split="test")

predictions = [
    [
        """On most days, the weather is warm and humid, with temperatures often soaring into the high 80s and low 90s Fahrenheit (around 31-34Â°C). The dense foliage of the jungle acts as a natural air conditioner, keeping the temperature relatively stable and comfortable for the inhabitants.""",
        """On most days, the weather is warm and humid, with temperatures often soaring into the high 80s and low 90s Fahrenheit. The dense foliage of the jungle acts as a natural air conditioner, keeping the temperature relatively stable and comfortable for the inhabitants.""",
        """On most days, the weather is warm and humid. The dense foliage of the jungle acts as a natural air conditioner, keeping the temperature relatively stable and comfortable for the inhabitants.""",
    ],
    [
        """Why did the cat cross the road? To cat to the other side.""",
        """Why did the cat sit on the computer? Because it wanted to keep an eye on the mouse!""",
        """What is red, yellow and green? A traffic light.""",
    ],
]

results = evaluate(predictions=predictions, data=dataset)

print("Global Scores:")
print(results.global_scores.summary)

print("Instance Scores:")
print(results.instance_scores.summary)

# examples/evaluate_existing_dataset_by_llm_as_judge_from_template.py
from unitxt import get_logger, get_settings, load_dataset
from unitxt.api import evaluate
from unitxt.inference import (
    CrossProviderInferenceEngine,
)

logger = get_logger()
settings = get_settings()

with settings.context(allow_unverified_code=True):
    # Use the HF load_dataset API, to load the squad QA dataset using the standard template in the catalog.
    # We set loader_limit to 20 to reduce download time.
    dataset = load_dataset(
        card="cards.squad",
        template="templates.qa.with_context.simple",
        format="formats.chat_api",
        metrics=[
            "metrics.llm_as_judge.rating.llama_3_70b_instruct.generic_single_turn"
        ],
        loader_limit=20,
        max_test_instances=20,
        split="test",
    )

    # Infer a model to get predictions.
    model = CrossProviderInferenceEngine(
        model="llama-3-2-1b-instruct", provider="watsonx"
    )
    """
    We are using a CrossProviderInferenceEngine inference engine that supply api access to provider such as:
    watsonx, bam, openai, azure, aws and more.

    For the arguments these inference engines can receive, please refer to the classes documentation or read
    about the the open ai api arguments the CrossProviderInferenceEngine follows.
    """
    predictions = model(dataset)

    # Evaluate the predictions using the defined metric.
    results = evaluate(predictions=predictions, data=dataset)

    print("Global Results:")
    print(results.global_scores.summary)

    print("Instance Results:")
    print(results.instance_scores.summary)

# examples/evaluate_summarization_dataset_llm_as_judge.py
from unitxt import get_logger
from unitxt.api import evaluate, load_dataset
from unitxt.inference import (
    CrossProviderInferenceEngine,
    HFPipelineBasedInferenceEngine,
)
from unitxt.llm_as_judge import LLMAsJudge
from unitxt.templates import InputOutputTemplate

logger = get_logger()

# First, we define the judge template.
judge_summary_rating_template = InputOutputTemplate(
    instruction=(
        "Please act as an impartial judge and evaluate if the assistant's summary summarise well the given text.\n"
        'You must respond according the following format: "[[rate]] - explanation".\n'
        'Were the rate is a score between 0 to 10 (10 for great summary, 0 for a very poor one)".\n'
        "The explanation describe shortly why you decided to give the rank you chosen.\n"
        "Please make sure to start with your rank ([[rank]]) before anything else.\n"
        "For example: [[9]] The summary catches the main text ideas."
        ".\n\n"
    ),
    input_format="[Text:\n{question}\n\n" "Assistant's summary:\n{answer}\n",
    output_format="[[{rating}]]",
    postprocessors=[
        r"processors.extract_mt_bench_rating_judgment",
    ],
)

# Second, we define the inference engine we use for judge, with the preferred model and provider.
# You can change the provider to any of: "watsonx", "together-ai", "open-ai", "aws", "ollama", "bam"
model = CrossProviderInferenceEngine(model="llama-3-8b-instruct", provider="watsonx")

# Third, We define the metric as LLM as a judge, with the desired platform and model.
llm_judge_metric = LLMAsJudge(
    inference_model=model,
    template=judge_summary_rating_template,
    format="formats.chat_api",
    task="rating.single_turn",
    main_score="llm_judge_llama_3_8b",
    strip_system_prompt_and_format_from_inputs=False,
)

# Load XSUM dataset, with the above metric.
dataset = load_dataset(
    card="cards.xsum",
    template="templates.summarization.abstractive.formal",
    metrics=[llm_judge_metric],
    loader_limit=5,
    split="test",
)

# Infer using Llama-3.2-1B base using HF API
model = HFPipelineBasedInferenceEngine(
    model_name="meta-llama/Llama-3.2-1B", max_new_tokens=32
)
# Change to this to infer with external APIs:
# CrossProviderInferenceEngine(model="llama-3-2-1b-instruct", provider="watsonx")
# The provider can be one of: ["watsonx", "together-ai", "open-ai", "aws", "ollama", "bam"]

predictions = model(dataset)

# Evaluate the predictions using the defined metric.
results = evaluate(predictions=predictions, data=dataset)

print("Global Results:")
print(results.global_scores.summary)

print("Instance Results:")
print(results.instance_scores.summary)


logger.info(
    "Now, we will repeat the example except this time we will use the reference for the judgement."
)

judge_summary_rating_with_reference_template = InputOutputTemplate(
    instruction="Please act as an impartial judge and evaluate if the assistant's summary summarise well the given text.\n"
    "You will be given a reference answer and the assistant's answer."
    " Begin your evaluation by comparing the assistant's answer with the reference answer."
    " Identify and correct any mistakes."
    'You must respond according the following format: "[[rate]] - explanation".\n'
    'Were the rate is a score between 0 to 10 (10 for great summary, 0 for a very poor one)".\n'
    "The explanation describe shortly why you decided to give the rank you chosen.\n"
    "Please make sure to start with your rank ([[rank]]) before anything else.\n"
    "For example: [[9]] The summary catches the main text ideas."
    ".\n\n",
    input_format="[Text:\n{question}\n\n"
    "[The Start of Reference Summary]\n{reference_answer}\n[The End of Reference summary]\n\n"
    "[The Start of Assistant's summary]\n{answer}\n[The End of Assistant's summary]",
    output_format="[[{rating}]]",
    postprocessors=[
        r"processors.extract_mt_bench_rating_judgment",
    ],
)

llm_judge_with_summary_metric = LLMAsJudge(
    inference_model=model,
    template=judge_summary_rating_with_reference_template,
    task="rating.single_turn_with_reference",
    main_score="llm_judge_llama_3_2_1b_hf",
    single_reference_per_prediction=True,
    strip_system_prompt_and_format_from_inputs=False,
)

# Load XSUM dataset, with the above metric.
dataset = load_dataset(
    card="cards.xsum",
    template="templates.summarization.abstractive.formal",
    format="formats.chat_api",
    metrics=[llm_judge_with_summary_metric],
    loader_limit=5,
    split="test",
)

# Infer using Llama-3.2-1B base using HF API
model = HFPipelineBasedInferenceEngine(
    model_name="meta-llama/Llama-3.2-1B", max_new_tokens=32
)
# Change to this to infer with external APIs:
# CrossProviderInferenceEngine(model="llama-3-2-1b-instruct", provider="watsonx")
# The provider can be one of: ["watsonx", "together-ai", "open-ai", "aws", "ollama", "bam"]

predictions = model(dataset)

# Evaluate the predictions using the defined metric.
results = evaluate(predictions=predictions, data=dataset)

print("Global Results:")
print(results.global_scores.summary)

print("Instance Results:")
print(results.instance_scores.summary)

# examples/evaluate_llm_as_judge_direct_criteria_from_dataset.py
from typing import Any

from unitxt import evaluate, load_dataset
from unitxt.blocks import Task, TaskCard
from unitxt.llm_as_judge_operators import CreateYesNoCriteriaFromString
from unitxt.loaders import LoadFromDictionary

data = {
    "test": [
        {
            "question": "How is the weather?",
            "judgement": "In the response, if there is a numerical temperature present, is it denominated in both Fahrenheit and Celsius?",
        },
        {
            "question": "Tell me a joke about cats",
            "judgement": "Is the response funny?",
        },
    ]
}

card = TaskCard(
    loader=LoadFromDictionary(data=data, data_classification_policy=["public"]),
    preprocess_steps=[
        CreateYesNoCriteriaFromString(field="judgement", to_field="criteria"),
    ],
    task=Task(
        input_fields={"question": str},
        reference_fields={"criteria": Any},
        prediction_type=str,
        metrics=[
            "metrics.llm_as_judge.direct.watsonx.llama3_1_70b[context_fields=question,criteria_field=criteria]"
        ],
    ),
)

dataset = load_dataset(card=card, template="templates.empty", split="test")

predictions = [
    """On most days, the weather is warm and humid, with temperatures often soaring into the high 80s and low 90s Fahrenheit (around 31-34Â°C). The dense foliage of the jungle acts as a natural air conditioner, keeping the temperature relatively stable and comfortable for the inhabitants.""",
    """Why did the cat cross the road? To cat to the other side.""",
]

results = evaluate(predictions=predictions, data=dataset)

print("Global Scores:")
print(results.global_scores.summary)

print("Instance Scores:")
print(results.instance_scores.summary)

# examples/evaluate_llm_as_judge_direct_user_criteria_no_catalog.py
from unitxt.api import create_dataset, evaluate
from unitxt.inference import CrossProviderInferenceEngine
from unitxt.llm_as_judge import LLMJudgeDirect
from unitxt.llm_as_judge_constants import (
    CriteriaWithOptions,
)

criteria = CriteriaWithOptions.from_obj(
    {
        "name": "Temperature in Fahrenheit and Celsius",
        "description": "In the response, if there is a numerical temperature present, is it denominated in both Fahrenheit and Celsius?",
        "options": [
            {
                "name": "Yes",
                "description": "The temperature reading is provided in both Fahrenheit and Celsius.",
            },
            {
                "name": "No",
                "description": "The temperature reading is provided either in Fahrenheit or Celsius, but not both.",
            },
            {
                "name": "Pass",
                "description": "There is no numerical temperature reading in the response.",
            },
        ],
        "option_map": {"Yes": 1.0, "No": 0.5, "Pass": 0.0},
    }
)


data = [
    {"question": "How is the weather?"},
    {"question": "How is the weather?"},
    {"question": "How is the weather?"},
]

metric = LLMJudgeDirect(
    inference_engine=CrossProviderInferenceEngine(
        model="llama-3-1-70b-instruct", max_tokens=1024
    ),
    criteria=criteria,
    context_fields=["question"],
    criteria_field="criteria",
)

dataset = create_dataset(
    task="tasks.qa.open", test_set=data, metrics=[metric], split="test"
)

predictions = [
    """On most days, the weather is warm and humid, with temperatures often soaring into the high 80s and low 90s Fahrenheit (around 31-34Â°C). The dense foliage of the jungle acts as a natural air conditioner, keeping the temperature relatively stable and comfortable for the inhabitants.""",
    """On most days, the weather is warm and humid, with temperatures often soaring into the high 80s and low 90s Fahrenheit. The dense foliage of the jungle acts as a natural air conditioner, keeping the temperature relatively stable and comfortable for the inhabitants.""",
    """On most days, the weather is warm and humid. The dense foliage of the jungle acts as a natural air conditioner, keeping the temperature relatively stable and comfortable for the inhabitants.""",
]

results = evaluate(predictions=predictions, data=dataset)

print("Global Scores:")
print(results.global_scores.summary)

print("Instance Scores:")
print(results.instance_scores.summary)

# examples/evaluate_external_rag_results_with_binary_llm_as_judge.py
import pandas as pd
from unitxt.operator import SequentialOperator
from unitxt.stream import MultiStream

# some toy input examples
test_examples = [
    {
        "question": "What foundation models are available in watsonx.ai ?",
        "answer": "Watsonx.ai supports no foundation models",
        "contexts": [
            "Supported foundation models available with watsonx.ai. Watsonx.ai offers numerous foundation models."
        ],
        "ground_truths": ["Many Large Language Models are supported by Watsonx.ai"],
        "metadata": {"data_classification_policy": ["public"]},
    },
    {
        "question": "What foundation models are available in watsonx.ai ?",
        "answer": "Watsonx.ai supports a variety of foundation models",
        "contexts": [
            "Supported foundation models available with Meta. Meta AI offers numerous foundation models."
        ],
        "ground_truths": ["Many Large Language Models are supported by Watsonx.ai"],
        "metadata": {"data_classification_policy": ["public"]},
    },
    {
        "question": "What foundation models are available in watsonx.ai ?",
        "answer": "Meta.ai supports a variety of foundation models",
        "contexts": [
            "Supported foundation models available with Meta. Meta AI offers numerous foundation models."
        ],
        "ground_truths": ["Many Large Language Models are supported by Watsonx.ai"],
        "metadata": {"data_classification_policy": ["public"]},
    },
    {
        "question": "What foundation models are available in watsonx.ai ?",
        "answer": "Watsonx.ai supports a variety of foundation models, the most prominent are Llama, Mixtral and Granite.",
        "contexts": [
            "Supported foundation models available with Meta. Meta AI offers numerous foundation models."
        ],
        "ground_truths": ["Many Large Language Models are supported by Watsonx.ai"],
        "metadata": {"data_classification_policy": ["public"]},
    },
    {
        "question": "What foundation models are available in watsonx.ai ?",
        "answer": "never gonna give you up, never gonna let you down.",
        "contexts": [
            "Supported foundation models available with Meta. Meta AI offers numerous foundation models."
        ],
        "ground_truths": ["Many Large Language Models are supported by Watsonx.ai"],
        "metadata": {"data_classification_policy": ["public"]},
    },
]

# Select the desired metric(s).
# Each metric measures a certain aspect of the generated answer (answer_correctness, faithfulness,
# answer_relevance, context_relevance and correctness_holistic).
# All available metrics are under "catalog.metrics.rag"
# Those with extension "logprobs" provide a real value prediction in [0,1], the others provide a binary prediction.
# By default, all judges use llama_3_1_70b_instruct_wml. We will soon see how to change this.
metric_names = [
    "metrics.rag.answer_correctness.llama_3_1_70b_instruct_wml_q_a_gt_loose_logprobs",
    "metrics.rag.faithfulness.llama_3_1_70b_instruct_wml_q_c_a_logprobs",
]

# select the desired model.
# all available models are under "catalog.engines.classification"
model_names = [
    "engines.classification.mixtral_8x7b_instruct_v01_wml",
    "engines.classification.llama_3_1_70b_instruct_wml",
    # "engines.classification.gpt_4_turbo_openai",
]

if __name__ == "__main__":
    multi_stream = MultiStream.from_iterables({"test": test_examples}, copying=True)

    # to keep all results
    results = test_examples.copy()
    global_scores = {"question": "global"}

    for metric_name in metric_names:
        for model_name in model_names:
            # override the metric with the inference model. the default model is llama_3_1_70b_instruct_wml so
            # no need to override when using it.
            llmaj_metric_name = f"{metric_name}[model={model_name}]"

            # apply the metric over the input
            metrics_operator = SequentialOperator(steps=[llmaj_metric_name])
            instances = metrics_operator(multi_stream)["test"]
            instances = list(instances)

            score_name = instances[0]["score"]["instance"]["score_name"]
            for i in range(len(instances)):
                results[i][score_name] = instances[i]["score"]["instance"][score_name]
                results[i][f"{score_name}_source"] = instances[i]["score"]["instance"][
                    f"{score_name}_judge_raw_input"
                ]
            global_scores[score_name] = instances[0]["score"]["global"][score_name]

    pd.DataFrame(results).transpose().to_csv("dataset_out.csv")

# examples/evaluate_benchmark.py
from unitxt.api import evaluate
from unitxt.benchmark import Benchmark
from unitxt.inference import (
    CrossProviderInferenceEngine,
)
from unitxt.standard import DatasetRecipe

benchmark = Benchmark(
    format="formats.user_agent",
    max_samples_per_subset=5,
    loader_limit=30,
    subsets={
        "cola": DatasetRecipe(
            card="cards.cola",
            template="templates.classification.multi_class.instruction",
        ),
        "mnli": DatasetRecipe(
            card="cards.mnli",
            template="templates.classification.multi_class.relation.default",
        ),
        "mrpc": DatasetRecipe(
            card="cards.mrpc",
            template="templates.classification.multi_class.relation.default",
        ),
        "qnli": DatasetRecipe(
            card="cards.qnli",
            template="templates.classification.multi_class.relation.default",
        ),
        "rte": DatasetRecipe(
            card="cards.rte",
            template="templates.classification.multi_class.relation.default",
        ),
        "sst2": DatasetRecipe(
            card="cards.sst2", template="templates.classification.multi_class.title"
        ),
        "stsb": DatasetRecipe(
            card="cards.stsb", template="templates.regression.two_texts.title"
        ),
        "wnli": DatasetRecipe(
            card="cards.wnli",
            template="templates.classification.multi_class.relation.default",
        ),
    },
)

test_dataset = list(benchmark()["test"])


# Infer using llama-3-2-1b base using Watsonx API
model = CrossProviderInferenceEngine(model="llama-3-2-1b-instruct", provider="watsonx")
"""
We are using a CrossProviderInferenceEngine inference engine that supply api access to provider such as:
watsonx, bam, openai, azure, aws and more.

For the arguments these inference engines can receive, please refer to the classes documentation or read
about the the open ai api arguments the CrossProviderInferenceEngine follows.
"""

predictions = model(test_dataset)
results = evaluate(predictions=predictions, data=test_dataset)

print("Global Results:")
print(results.global_scores.summary)

print("Subsets Results:")
print(results.subsets_scores.summary)

# examples/evaluate_existing_dataset_by_llm_as_judge_direct.py
import statistics

from unitxt import get_logger, get_settings, load_dataset
from unitxt.api import evaluate
from unitxt.inference import (
    CrossProviderInferenceEngine,
)
from unitxt.text_utils import print_dict

logger = get_logger()
settings = get_settings()

# Use the HF load_dataset API, to load the squad QA dataset using the standard template in the catalog.
# We set loader_limit to 20 to reduce download time.
criterias = ["answer_relevance", "coherence", "conciseness"]
metrics = [
    "metrics.llm_as_judge.direct.rits.llama3_1_70b"
    "[context_fields=[context,question],"
    f"criteria=metrics.llm_as_judge.direct.criterias.{criteria},"
    f"score_prefix={criteria}_]"
    for criteria in criterias
]
dataset = load_dataset(
    card="cards.squad",
    metrics=metrics,
    loader_limit=10,
    max_test_instances=10,
    split="test",
)

# Infer a model to get predictions.
inference_model = CrossProviderInferenceEngine(
    model="llama-3-2-1b-instruct", provider="watsonx"
)

"""
We are using a CrossProviderInferenceEngine inference engine that supply api access to provider such as:
watsonx, bam, openai, azure, aws and more.

For the arguments these inference engines can receive, please refer to the classes documentation or read
about the the open ai api arguments the CrossProviderInferenceEngine follows.
"""
predictions = inference_model.infer(dataset)

gold_answers = [d[0] for d in dataset["references"]]

# Evaluate the predictions using the defined metric.
evaluated_predictions = evaluate(predictions=predictions, data=dataset)
evaluated_gold_answers = evaluate(predictions=gold_answers, data=dataset)

print_dict(
    evaluated_predictions[0],
    keys_to_print=[
        "source",
        "score",
    ],
)
print_dict(
    evaluated_gold_answers[0],
    keys_to_print=[
        "source",
        "score",
    ],
)

for criteria in criterias:
    logger.info(f"Scores for criteria '{criteria}'")
    gold_answer_scores = [
        instance["score"]["instance"][f"{criteria}_llm_as_a_judge_score"]
        for instance in evaluated_gold_answers
    ]
    gold_answer_position_bias = [
        int(instance["score"]["instance"][f"{criteria}_positional_bias"])
        for instance in evaluated_gold_answers
    ]
    prediction_scores = [
        instance["score"]["instance"][f"{criteria}_llm_as_a_judge_score"]
        for instance in evaluated_predictions
    ]
    prediction_position_bias = [
        int(instance["score"]["instance"][f"{criteria}_positional_bias"])
        for instance in evaluated_predictions
    ]

    logger.info(
        f"Scores of gold answers: {statistics.mean(gold_answer_scores)} +/- {statistics.stdev(gold_answer_scores)}"
    )
    logger.info(
        f"Scores of predicted answers: {statistics.mean(prediction_scores)} +/- {statistics.stdev(prediction_scores)}"
    )
    logger.info(
        f"Positional bias occurrence on gold answers: {statistics.mean(gold_answer_position_bias)}"
    )
    logger.info(
        f"Positional bias occurrence on predicted answers: {statistics.mean(prediction_position_bias)}\n"
    )

"""
Output with 100 examples

Scores for criteria 'answer_relevance'
Scores of gold answers: 0.9625 +/- 0.14811526360619054
Scores of predicted answers: 0.5125 +/- 0.4638102516061385
Positional bias occurrence on gold answers: 0.03
Positional bias occurrence on predicted answers: 0.12

Scores for criteria 'coherence'
Scores of gold answers: 0.159 +/- 0.15689216524464028
Scores of predicted answers: 0.066 +/- 0.11121005695384194
Positional bias occurrence on gold answers: 0.16
Positional bias occurrence on predicted answers: 0.07

Scores for criteria 'conciseness'
Scores of gold answers: 1.0 +/- 0.0
Scores of predicted answers: 0.34 +/- 0.47609522856952335
Positional bias occurrence on gold answers: 0.03
Positional bias occurrence on predicted answers: 0.01
"""

# examples/evaluate_same_datasets_and_models_with_multiple_providers.py
import pandas as pd
from unitxt.api import evaluate, load_dataset
from unitxt.artifact import fetch_artifact
from unitxt.formats import SystemFormat

df = pd.DataFrame(
    columns=[
        "provider",
        "model",
        "format_as_chat_api",
        "num_instances",
        "score_name",
        "score",
        "ci_low",
        "ci_high",
    ]
)

for provider in [
    "watsonx-sdk",
    "watsonx",
]:
    for model_name in [
        "granite-3-8b-instruct",
        "llama-3-8b-instruct",
    ]:
        for format_as_chat_api in [True, False]:
            if format_as_chat_api and provider == "watsonx-sdk":
                continue
            if format_as_chat_api:
                format = "formats.chat_api"
            else:
                if model_name.startswith("llama"):
                    format = "formats.llama3_instruct"
                if model_name.startswith("granite"):
                    format = SystemFormat(
                        demo_format=(
                            "{instruction}\\N{source}\\N<|end_of_text|>\n"
                            "<|start_of_role|>assistant<|end_of_role|>{target}\\N<|end_of_text|>\n"
                            "<|start_of_role|>user<|end_of_role|>"
                        ),
                        model_input_format=(
                            "<|start_of_role|>system<|end_of_role|>{system_prompt}<|end_of_text|>\n"
                            "<|start_of_role|>user<|end_of_role|>{demos}{instruction}\\N{source}\\N<|end_of_text|>\n"
                            "<|start_of_role|>assistant<|end_of_role|>"
                        ),
                    )
            card, _ = fetch_artifact("cards.sst2")

            dataset = load_dataset(
                card=card,
                template_card_index=0,
                format=format,
                num_demos=1,
                demos_pool_size=10,
                loader_limit=1000,
                max_test_instances=10,
                disable_cache=False,
                split="test",
            )

            from unitxt.inference import CrossProviderInferenceEngine

            model = CrossProviderInferenceEngine(
                model=model_name, max_tokens=1024, provider=provider
            )
            predictions = model(dataset)

            results = evaluate(predictions=predictions, data=dataset)
            # import pandas as pd
            # result_df = pd.json_normalize(evaluated_dataset)
            # result_df.to_csv(f"output.csv")
            # Print results
            print(
                results.instance_scores.to_df(
                    columns=[
                        "source",
                        "prediction",
                        "processed_prediction",
                        "processed_references",
                    ],
                )
            )

            global_scores = results.global_scores
            df.loc[len(df)] = [
                provider,
                model_name,
                format_as_chat_api,
                global_scores["num_of_instances"],
                global_scores["score_name"],
                global_scores["score"],
                global_scores["score_ci_low"],
                global_scores["score_ci_high"],
            ]

            df = df.round(decimals=2)
            print(df.to_markdown())

# examples/custom_type.py
from typing import Any, Dict, List, Literal, NewType, TypedDict

from unitxt import load_dataset
from unitxt.card import TaskCard
from unitxt.loaders import LoadFromDictionary
from unitxt.serializers import SingleTypeSerializer
from unitxt.task import Task
from unitxt.templates import InputOutputTemplate
from unitxt.text_utils import print_dict
from unitxt.type_utils import register_type


class CustomTurn(TypedDict):
    role: Literal["system", "user", "agent"]
    content: str


CustomDialog = NewType("CustomDialog", List[CustomTurn])

register_type(CustomTurn)
register_type(CustomDialog)


class CustomDialogSerializer(SingleTypeSerializer):
    serialized_type = CustomDialog

    def serialize(self, value: CustomDialog, instance: Dict[str, Any]) -> str:
        return (
            "___\n"
            + "\n".join(f"  {turn['role']}: {turn['content']}" for turn in value)
            + "\n___"
        )


dialog_summarization_task = Task(
    input_fields={"dialog": CustomDialog},
    reference_fields={"summary": str},
    prediction_type=str,
    metrics=["metrics.rouge"],
)

data = {
    "test": [
        {
            "dialog": [
                {"role": "user", "content": "What is the time?"},
                {"role": "system", "content": "4:13 PM"},
            ],
            "summary": "User asked for the time and got an answer.",
        }
    ]
}


card = TaskCard(
    loader=LoadFromDictionary(data=data),
    task=dialog_summarization_task,
)


dataset = load_dataset(
    card=card,
    template=InputOutputTemplate(
        instruction="Summarize the following dialog.",
        input_format="{dialog}",
        output_format="{summary}",
    ),
    serializer=CustomDialogSerializer(),
)

print_dict(
    dataset["test"][0],
    keys_to_print=[
        "source",
        "target",
    ],
)

# examples/evaluate_tables_as_images.py
from unitxt.api import evaluate, load_dataset
from unitxt.inference import CrossProviderInferenceEngine
from unitxt.struct_data_operators import SerializeTableAsImage

# Use the Unitxt APIs to load the wnli entailment dataset using the standard template in the catalog for relation task with 2-shot in-context learning.
# We set loader_limit to 20 to limit reduce inference time.
dataset = load_dataset(
    card="cards.wikitq",
    format="formats.chat_api",
    system_prompt="system_prompts.general.be_concise",
    loader_limit=20,
    serializer=[
        SerializeTableAsImage(),
    ],
    split="test",
)

model = CrossProviderInferenceEngine(model="llama-3-2-11b-vision-instruct")
"""
We are using a CrossProviderInferenceEngine inference engine that supply api access to providers such as:
watsonx, bam, openai, azure, aws and more.
For more information, visit the :ref:`inference engines guide <inference>`
"""
predictions = model(dataset)

results = evaluate(predictions=predictions, data=dataset)

print("Global Results:")
print(results.global_scores.summary)

# print("Instance Results:")
# print(results.instance_scores.summary)

# examples/evaluate_existing_dataset_with_install.py
from unitxt.api import evaluate, load_dataset
from unitxt.inference import CrossProviderInferenceEngine

# Use the Unitxt APIs to load the wnli entailment dataset using the standard template in the catalog for relation task with 2-shot in-context learning.
# We set loader_limit to 20 to limit reduce inference time.
dataset = load_dataset(
    card="cards.wnli",
    template="templates.classification.multi_class.relation.default",
    format="formats.chat_api",
    num_demos=2,
    demos_pool_size=10,
    loader_limit=20,
    split="test",
)

model = CrossProviderInferenceEngine(model="llama-3-2-1b-instruct", provider="watsonx")
"""
We are using a CrossProviderInferenceEngine inference engine that supply api access to providers such as:
watsonx, bam, openai, azure, aws and more.
For more information, visit the :ref:`inference engines guide <inference>`
"""
predictions = model(dataset)

results = evaluate(predictions=predictions, data=dataset)

print("Global Results:")
print(results.global_scores.summary)

print("Instance Results:")
print(results.instance_scores.summary)

# examples/evaluate_rag_end_to_end_dataset_with_given_predictions.py
from unitxt import get_logger
from unitxt.api import create_dataset, evaluate

logger = get_logger()

#
contexts = [
    "Austin is the capital of Texas.",
    "Houston is in Texas",
    "Houston is the the largest city in the state but not the capital of it.",
]

# Set up question answer pairs in a dictionary
dataset = [
    {
        "question": "What is the capital of Texas?",
        "question_id": 0,
        "reference_answers": ["Austin"],
        "reference_contexts": [contexts[0]],
        "reference_context_ids": [0],
        "is_answerable_label": True,
    },
    {
        "question": "Which is the the largest city in Texas?",
        "question_id": 1,
        "reference_answers": ["Houston"],
        "reference_contexts": [contexts[1], contexts[2]],
        "reference_context_ids": [1, 2],
        "is_answerable_label": True,
    },
]

predictions = [
    {
        "answer": "Houston",
        "contexts": [contexts[2]],
        "context_ids": [2],
        "is_answerable": True,
    },
    {
        "answer": "Houston",
        "contexts": [contexts[2]],
        "context_ids": [2],
        "is_answerable": True,
    },
]

dataset = create_dataset(
    task="tasks.rag.end_to_end",
    test_set=dataset,
    split="test",
    postprocessors=[],
)

results = evaluate(predictions, dataset)

# Print Results:

print("Global Results:")
print(results.global_scores.summary)

print("Instance Results:")
print(results.instance_scores.summary)

# examples/evaluate_qa_dataset_with_given_predictions.py
from unitxt import get_logger
from unitxt.api import create_dataset, evaluate

logger = get_logger()

# Set up question answer pairs in a dictionary
dataset = [
    {"question": "What is the capital of Texas?", "answers": ["Austin"]},
    {"question": "What is the color of the sky?", "answers": ["Blue"]},
]

predictions = ["San Antonio", "blue"]

dataset = create_dataset(
    task="tasks.qa.open",
    test_set=dataset,
    metrics=[
        "metrics.qa.open.recommended_no_gpu",
        # "metrics.qa.open.recommended_llm_as_judge",
    ],
)

results = evaluate(predictions, dataset["test"])

print("Global Results:")
print(results.global_scores.summary)

print("Instance Results:")
print(results.instance_scores.summary)

# examples/evaluate_rag_response_generation.py
from unitxt.api import evaluate, load_dataset
from unitxt.blocks import (
    TaskCard,
)
from unitxt.collections_operators import Wrap
from unitxt.inference import (
    HFPipelineBasedInferenceEngine,
)
from unitxt.loaders import LoadFromDictionary
from unitxt.operators import Rename, Set
from unitxt.templates import MultiReferenceTemplate, TemplatesDict

# Assume the RAG data is proved in this format
data = {
    "test": [
        {
            "query": "What city is the largest in Texas?",
            "extracted_chunks": "Austin is the capital of Texas.\nHouston is the the largest city in Texas but not the capital of it. ",
            "expected_answer": "Houston",
        },
        {
            "query": "What city is the capital of Texas?",
            "extracted_chunks": "Houston is the the largest city in Texas but not the capital of it. ",
            "expected_answer": "Austin",
        },
    ]
}


card = TaskCard(
    # Assumes this csv, contains 3 fields
    # question (string), extracted_chunks (string), expected_answer (string)
    loader=LoadFromDictionary(data=data),
    # Map these fields to the fields of the task.rag.response_generation task.
    # See https://www.unitxt.ai/en/latest/catalog/catalog.tasks.rag.response_generation.html
    preprocess_steps=[
        Rename(field_to_field={"query": "question"}),
        Wrap(field="extracted_chunks", inside="list", to_field="contexts"),
        Wrap(field="expected_answer", inside="list", to_field="reference_answers"),
        Set(
            fields={
                "contexts_ids": [],
            }
        ),
    ],
    # Specify the task and the desired metrics (note that these are part of the default
    # metrics for the task, so the metrics selection can be omitted).
    task="tasks.rag.response_generation",
    # Specify a default template
    templates=TemplatesDict(
        {
            "simple": MultiReferenceTemplate(
                instruction="Answer the question based on the information provided in the document given below.\n\n",
                input_format="Document: {contexts}\nQuestion: {question}",
                references_field="reference_answers",
            ),
        }
    ),
)

# Verbalize the dataset using the template
dataset = load_dataset(
    card=card,
    template_card_index="simple",
    format="formats.chat_api",
    split="test",
    max_test_instances=10,
)


# Infer using Llama-3.2-1B base using HF API
model = HFPipelineBasedInferenceEngine(
    model_name="meta-llama/Llama-3.2-1B", max_new_tokens=32
)
# Change to this to infer with external APIs:
# CrossProviderInferenceEngine(model="llama-3-2-1b-instruct", provider="watsonx")
# The provider can be one of: ["watsonx", "together-ai", "open-ai", "aws", "ollama", "bam"]

predictions = model(dataset)
results = evaluate(predictions=predictions, data=dataset)

print("Global Results:")
print(results.global_scores.summary)

print("Instance Results:")
print(results.instance_scores.summary)

# examples/evaluate_a_model_using_arena_hard.py
from unitxt import evaluate, load_dataset
from unitxt.inference import CrossProviderInferenceEngine

"""
We are evaluating only on a small subset (by using `max_test_instances=4`), in order for the example to finish quickly.
The dataset full size if around 40k examples. You should use around 1k-4k in your evaluations.
"""
dataset = load_dataset(
    card="cards.arena_hard.generation.english_gpt_4_0314_reference",
    template="templates.generation.empty",
    format="formats.chat_api",
    metrics=[
        "metrics.llm_as_judge.pairwise_comparative_rating.llama_3_8b_instruct.template_arena_hard"
    ],
    max_test_instances=4,
    split="test",
)

model = CrossProviderInferenceEngine(model="llama-3-2-1b-instruct", provider="watsonx")
"""
We are using a CrossProviderInferenceEngine inference engine that supply api access to provider such as:
watsonx, bam, openai, azure, aws and more.

For the arguments these inference engines can receive, please refer to the classes documentation or read
about the the open ai api arguments the CrossProviderInferenceEngine follows.
"""

predictions = model(dataset)
results = evaluate(predictions=predictions, data=dataset)

# Print Results:
print(results.global_scores.summary)

# examples/evaluate_different_templates_num_demos.py
from unitxt import evaluate, load_dataset
from unitxt.logging_utils import get_logger

logger = get_logger()

dataset = load_dataset(
    card="cards.wnli",
    template=[
        "templates.classification.multi_class.relation.default",
        "templates.key_val",
    ],
    num_demos=[0, 5],
    group_by=["template", "num_demos", ["template", "num_demos"]],
    demos_pool_size=10,
    loader_limit=200,
    max_test_instances=10,
    split="test",
)

predictions = ["entailment" for _ in dataset]

results = evaluate(predictions=predictions, data=dataset)

# Print Results:

print("Global Results:")
print(results.global_scores.summary)

print("Groups Results:")
print(results.groups_scores.summary)

# examples/standalone_evaluation_llm_as_judge.py
from unitxt import get_logger
from unitxt.api import create_dataset, evaluate
from unitxt.blocks import Task
from unitxt.inference import (
    CrossProviderInferenceEngine,
    HFPipelineBasedInferenceEngine,
)
from unitxt.llm_as_judge import LLMAsJudge
from unitxt.templates import InputOutputTemplate

logger = get_logger()

# First, we define the examples data we want to evaluate using LLM as judge.
data = [
    {
        "query": "What is the capital of Texas?",
        "document": "The capital of Texas is Austin.",
        "reference_answer": "Austin",
    },
    {
        "query": "What is the color of the sky right now?",
        "document": "The sky is generally black during the night.",
        "reference_answer": "Black",
    },
]

# Second, We define the prompt we show to the judge.
#
# Note that "question" is the full input provided to the original model, and "answer" is the original model
# output.  For example , this is sample input provided to the LLM as judge model.
#
# Please act as an impartial judge and evaluate if the assistant's answer is correct. Answer "[[10]]" if the answer is accurate, and "[[0]]" if the answer is wrong. Please use the exact format of the verdict as "[[rate]]".
# You can explain your answer after the verdict.
# [User's input]
# Answer the following query based on the provided document.
# Document:
# The sky is generally black during the night.
# Query:
# What is the color of the sky?
#
# [Assistant's Answer]
# black

judge_correctness_template = InputOutputTemplate(
    instruction="Please act as an impartial judge and evaluate if the assistant's answer is correct."
    ' Answer "[[10]]" if the answer is accurate, and "[[0]]" if the answer is wrong. '
    'Please use the exact format of the verdict as "[[rate]]". '
    "You can explain your answer after the verdict"
    ".\n\n",
    input_format="[User's input]\n{question}\n" "[Assistant's Answer]\n{answer}\n",
    output_format="[[{rating}]]",
    postprocessors=[
        r"processors.extract_mt_bench_rating_judgment",
    ],
)

# Third, We define the metric as LLM as a judge, with the desired platform and model.
llm_judge_metric = LLMAsJudge(
    inference_model=CrossProviderInferenceEngine(
        model="llama-3-8b-instruct", max_tokens=1024, provider="watsonx"
    ),
    template=judge_correctness_template,
    format="formats.chat_api",
    task="rating.single_turn",
    main_score="llm_judge_score",
    strip_system_prompt_and_format_from_inputs=False,
)

task = Task(
    input_fields={"query": str, "document": str},
    reference_fields={"reference_answer": str},
    prediction_type=str,
    metrics=[llm_judge_metric],
)

template = InputOutputTemplate(
    instruction="Answer the following query based on the provided document.",
    input_format="Document:\n{document}\nQuery:\n{query}",
    output_format="{reference_answer}",
    postprocessors=["processors.lower_case"],
)

dataset = create_dataset(
    test_set=data,
    task=task,
    template=template,
    format="formats.chat_api",
    split="test",
    max_test_instances=10,
)

# Infer using Llama-3.2-1B base using HF API
model = HFPipelineBasedInferenceEngine(
    model_name="Qwen/Qwen1.5-0.5B-Chat", max_new_tokens=32
)
predictions = model(dataset)

# Evaluate the predictions using the defined metric.
results = evaluate(predictions=predictions, data=dataset)

print("Global Results:")
print(results.global_scores.summary)

print("Instance Results:")
print(results.instance_scores.summary)

# examples/qa_evaluation.py
from unitxt import get_logger
from unitxt.api import create_dataset, evaluate
from unitxt.inference import (
    HFPipelineBasedInferenceEngine,
)

logger = get_logger()

# Set up question answer pairs in a dictionary
test_set = [
    {"question": "What is the capital of Texas?", "answers": ["Austin"]},
    {"question": "What is the color of the sky?", "answers": ["Blue"]},
]


# Verbalize the dataset using the catalog template which adds an instruction "Answer the question.",
# and "Question:"/"Answer:" prefixes.
#
# "Answer the question.
#  Question:
#  What is the color of the sky?
#  Answer:
# "

dataset = create_dataset(
    task="tasks.qa.open",
    test_set=test_set,
    template="templates.qa.open",
    split="test",
    format="formats.chat_api",
)

# Infer using Llama-3.2-1B base using HF API
model = HFPipelineBasedInferenceEngine(
    model_name="Qwen/Qwen1.5-0.5B-Chat", max_new_tokens=32
)
# Change to this to infer with external APIs:
# from unitxt.inference import CrossProviderInferenceEngine
# engine = CrossProviderInferenceEngine(model="llama-3-2-1b-instruct", provider="watsonx")
# The provider can be one of: ["watsonx", "together-ai", "open-ai", "aws", "ollama", "bam"]


predictions = model(dataset)
results = evaluate(predictions=predictions, data=dataset)

print("Global Results:")
print(results.global_scores.summary)

print("Instance Results:")
print(results.instance_scores.summary)

# examples/standalone_qa_evaluation.py
from unitxt import get_logger
from unitxt.api import create_dataset, evaluate
from unitxt.blocks import Task
from unitxt.inference import HFPipelineBasedInferenceEngine
from unitxt.templates import InputOutputTemplate

logger = get_logger()

# Set up question answer pairs in a dictionary
data = [
    {"question": "What is the capital of Texas?", "answer": "Austin"},
    {"question": "What is the color of the sky?", "answer": "Blue"},
]


# define the QA task
task = Task(
    input_fields={"question": str},
    reference_fields={"answer": str},
    prediction_type=str,
    metrics=["metrics.rouge"],
)


# Create a simple template that formats the input.
# Add lowercase normalization as a post processor.

template = InputOutputTemplate(
    instruction="Answer the following question in one word.",
    input_format="{question}",
    output_format="{answer}",
    postprocessors=["processors.lower_case"],
)
# Verbalize the dataset using the template
dataset = create_dataset(
    task=task, test_set=data, template=template, format="formats.chat_api", split="test"
)


# Infer using Llama-3.2-1B base using HF API
model = HFPipelineBasedInferenceEngine(
    model_name="Qwen/Qwen1.5-0.5B-Chat", max_new_tokens=32
)
# Change to this to infer with external APIs:
# from unitxt.inference import CrossProviderInferenceEngine
# engine = CrossProviderInferenceEngine(model="llama-3-2-1b-instruct", provider="watsonx")
# The provider can be one of: ["watsonx", "together-ai", "open-ai", "aws", "ollama", "bam". "rits"]


predictions = model(dataset)
results = evaluate(predictions=predictions, data=dataset)

print("Global Results:")
print(results.global_scores.summary)

print("Instance Results:")
print(results.instance_scores.summary)

# examples/evaluate_llm_as_judge_from_template.py
import statistics

import numpy as np
from unitxt import get_logger
from unitxt.api import evaluate, load_dataset

logger = get_logger()

# This example demonstrates how to evaluate the quality of LLM as judge
# on a task by using the gold references of a dataset.

# It checks two llama3 based judges - one based on a 8b model and one on a 70b model on a
# summarization dataset.
#
# The results indicate that the 8b model gives a higher score to a wrong prediction over the correct
# prediction in 20% of the examples, and gives a truncated corrected prediction a higher score than
# the correct prediction in 35% of examples.  This means it is not so good as a judge for this task.
#
# On the other hand the 70b model is better.  It always gives a higher score for the correct prediction,
# and in only 5% of the cases it gives the truncated prediction a higher score.
# Note that even the 70b model gives relatively low average score for correct predictions (0.395 +/ 0.17)

# List of metrics to evaluate
metrics_to_check = [
    "metrics.llm_as_judge.rating.llama_3_8b_instruct_ibm_genai_template_mt_bench_single_turn",
    "metrics.llm_as_judge.rating.llama_3_70b_instruct_ibm_genai_template_generic_single_turn",
]

for metric_to_check in metrics_to_check:
    # The dataset used to evaluate the metrics based on its gold answers
    dataset = load_dataset(
        card="cards.xsum",
        template="templates.summarization.abstractive.formal",
        metrics=[metric_to_check],
        loader_limit=20,
    )
    test_dataset = dataset["test"]

    # Prepare three sets of predictions :
    # 1. the correct predictions taken from the gold answer
    # 2. wrong predictions (where the prediction is the gold answer of another question)
    # 3. truncated predictions taken as the first half of the gold answer
    correct_predictions = test_dataset["target"]
    wrong_predictions = [correct_predictions[-1]]
    wrong_predictions.extend(correct_predictions[0:-1])
    truncated_predictions = [
        prediction[: len(prediction) // 2] for prediction in correct_predictions
    ]

    # Evaluate over the correct, wrong and truncated predictions using the defined metric.
    correct_evaluated_dataset = evaluate(
        predictions=correct_predictions, data=test_dataset
    )
    wrong_evaluated_dataset = evaluate(predictions=wrong_predictions, data=test_dataset)
    truncated_evaluated_dataset = evaluate(
        predictions=truncated_predictions, data=test_dataset
    )

    correct_prediction_scores = [
        correct_evaluated_dataset[i]["score"]["instance"]["score"]
        for i in range(len(correct_predictions))
    ]
    wrong_prediction_scores = [
        wrong_evaluated_dataset[i]["score"]["instance"]["score"]
        for i in range(len(wrong_predictions))
    ]
    truncated_prediction_scores = [
        truncated_evaluated_dataset[i]["score"]["instance"]["score"]
        for i in range(len(truncated_predictions))
    ]

    # Print the scores of the metric on each type of prediction.
    # The score of correct predictions should be close to 1 with low standard deviation
    # The score of wrong predictions should be close to 0 with low standard deviation
    # The score of the truncated prediction, should be between the values.
    # Also prints the percent of examples the wrong / truncated prediction get a higher score than the correct prediction.

    logger.info(f"Meta evaluation of metric: {metric_to_check}")
    logger.info(f"Scores of correct predictions: {correct_prediction_scores}")
    logger.info(f"Scores of wrong predictions: {wrong_prediction_scores}")
    logger.info(f"Scores of truncated predictions: {truncated_prediction_scores}")
    logger.info(
        f"Average score of correct predictions: {statistics.mean(correct_prediction_scores)} +/- {statistics.stdev(correct_prediction_scores)}"
    )
    logger.info(
        f"Average score of wrong predictions: {statistics.mean(wrong_prediction_scores)} +/- {statistics.stdev(wrong_prediction_scores)}"
    )
    logger.info(
        f"% Wrong predictions scores greater than correct prediction scores: {np.sum(np.greater(wrong_prediction_scores, correct_prediction_scores)) * 100/ len(correct_predictions)}"
    )
    logger.info(
        f"Average score of truncated predictions: {statistics.mean(truncated_prediction_scores)} +/- {statistics.stdev(truncated_prediction_scores)}"
    )
    logger.info(
        f"% Truncated predictions scores greater than correct prediction scores: {np.sum(np.greater(truncated_prediction_scores, correct_prediction_scores)) * 100/ len(correct_predictions)}"
    )

# examples/evaluate_image_text_to_text.py
from unitxt import settings
from unitxt.api import evaluate, load_dataset
from unitxt.inference import (
    LMMSEvalInferenceEngine,
)

with settings.context(
    disable_hf_datasets_cache=False,
):
    inference_model = LMMSEvalInferenceEngine(
        model_type="llava",
        model_args={"pretrained": "liuhaotian/llava-v1.5-7b"},
        max_new_tokens=128,
    )
    dataset = load_dataset(
        card="cards.websrc",
        format="formats.chat_api",
        # max_test_instances=20,
        split="test",
    )

    predictions = inference_model.infer(dataset)
    results = evaluate(predictions=predictions, data=dataset)

    print("Global Results:")
    print(results.global_scores.summary)

    print("Instance Results:")
    print(results.instance_scores.summary)

# examples/ner_evaluation.py
import json

from unitxt import get_logger
from unitxt.api import create_dataset, evaluate
from unitxt.inference import (
    CrossProviderInferenceEngine,
)

logger = get_logger()
entity_types = ["Person", "Location", "Organization"]


test_set = [
    {
        "text": "John lives in Texas.",
        "entity_types": entity_types,
        "spans_starts": [0, 14],
        "spans_ends": [5, 19],
        "labels": ["Person", "Location"],
    },
    {
        "text": "Phil works at Apple and eats an apple.",
        "entity_types": entity_types,
        "spans_starts": [0, 14],
        "spans_ends": [5, 19],
        "labels": ["Person", "Organization"],
    },
]


dataset = create_dataset(
    task="tasks.ner.all_entity_types",
    test_set=test_set,
    split="test",
    format="formats.chat_api",
)

# Infer using Llama-3.2-1B base using HF API
# model = HFPipelineBasedInferenceEngine(
#   model_name="Qwen/Qwen1.5-0.5B-Chat", max_new_tokens=32
# )
# Change to this to infer with external APIs:

model = CrossProviderInferenceEngine(model="llama-3-8b-instruct", provider="watsonx")
# The provider can be one of: ["watsonx", "together-ai", "open-ai", "aws", "ollama", "bam"]


predictions = model(dataset)
results = evaluate(predictions=predictions, data=dataset)

print("Global Results:")
print(results.global_scores.summary)

print("Example prompt:")

print(json.dumps(results.instance_scores[0]["source"], indent=4))

print("Instance Results:")
print(
    results.instance_scores.to_df(
        columns=[
            "text",
            "prediction",
            "processed_prediction",
            "processed_references",
            "score",
            "score_name",
        ]
    ).to_markdown()
)

# examples/evaluate_grounded_ensemble_judge.py
from unitxt.api import evaluate, load_dataset
from unitxt.blocks import Task, TaskCard
from unitxt.loaders import LoadFromDictionary
from unitxt.templates import InputOutputTemplate, TemplatesDict

data = {
    "test": [
        {
            "conversation": "user: is eugene from walking dead really a scientist\nagent: No, Eugene from The Walking Dead is not a real scientist. He lied about being a scientist to manipulate the other survivors into taking him to Washington D.C., believing it to be the best chance for survival. In reality, he is a high school science teacher who does not know how to cure the virus. However, he is shown to be highly intelligent and resourceful, able to do things such as repairing the solar power system in Alexandria and making bullets for the Saviors.\nuser: Why did he lied about himself being a scientist?\nagent: Eugene lied about being a scientist to manipulate the other survivors into taking him to Washington D.C., believing it to be the best chance for survival. He initially used this lie to gain the protection of Abraham Ford. He assists the group with technical tasks even after the truth is revealed, which also improves his own confidence. His lie caused tension within the group, particularly between him and Abraham, but eventually Abraham forgave him and they resumed their friendship.\nuser: Is The Walking Dead currently shown on TV? Which station?",
            "document": "The Walking Dead (season 6)\nOn October 7 , 2014 , AMC renewed The Walking Dead for a sixth season . Scott M. Gimple said that the sixth season would continue to remix material from the comic and explained that there would be a flashback backstory to some of the characters :\n\nThe Walking Dead (season 6)\nThe sixth season of The Walking Dead , an American post-apocalyptic horror television series on AMC , premiered on October 11 , 2015 , and concluded on April 3 , 2016 , consisting of 16 episodes . Developed for television by Frank Darabont , the series is based on the eponymous series of comic books by Robert Kirkman , Tony Moore , and Charlie Adlard . The executive producers are Kirkman , David Alpert , Scott M. Gimple , Greg Nicotero , Tom Luse , and Gale Anne Hurd , with Gimple as showrunner for the third consecutive season . The sixth season received positive reviews by critics . It was nominated for multiple awards and won three , including Best Horror Television Series , at the 42nd Saturn Awards .",
        },
        {
            "conversation": "user: is eugene from walking dead really a scientist\nagent: No, Eugene from The Walking Dead is not a real scientist. He lied about being a scientist to manipulate the other survivors into taking him to Washington D.C., believing it to be the best chance for survival. In reality, he is a high school science teacher who does not know how to cure the virus. However, he is shown to be highly intelligent and resourceful, able to do things such as repairing the solar power system in Alexandria and making bullets for the Saviors.\nuser: Why did he lied about himself being a scientist?\nagent: Eugene lied about being a scientist to manipulate the other survivors into taking him to Washington D.C., believing it to be the best chance for survival. He initially used this lie to gain the protection of Abraham Ford. He assists the group with technical tasks even after the truth is revealed, which also improves his own confidence. His lie caused tension within the group, particularly between him and Abraham, but eventually Abraham forgave him and they resumed their friendship.\nuser: Is The Walking Dead currently shown on TV? Which station?",
            "document": "The Walking Dead (season 6)\nOn October 7 , 2014 , AMC renewed The Walking Dead for a sixth season . Scott M. Gimple said that the sixth season would continue to remix material from the comic and explained that there would be a flashback backstory to some of the characters :\n\nThe Walking Dead (season 6)\nThe sixth season of The Walking Dead , an American post-apocalyptic horror television series on AMC , premiered on October 11 , 2015 , and concluded on April 3 , 2016 , consisting of 16 episodes . Developed for television by Frank Darabont , the series is based on the eponymous series of comic books by Robert Kirkman , Tony Moore , and Charlie Adlard . The executive producers are Kirkman , David Alpert , Scott M. Gimple , Greg Nicotero , Tom Luse , and Gale Anne Hurd , with Gimple as showrunner for the third consecutive season . The sixth season received positive reviews by critics . It was nominated for multiple awards and won three , including Best Horror Television Series , at the 42nd Saturn Awards .",
        },
        {
            "conversation": "user: is eugene from walking dead really a scientist\nagent: No, Eugene from The Walking Dead is not a real scientist. He lied about being a scientist to manipulate the other survivors into taking him to Washington D.C., believing it to be the best chance for survival. In reality, he is a high school science teacher who does not know how to cure the virus. However, he is shown to be highly intelligent and resourceful, able to do things such as repairing the solar power system in Alexandria and making bullets for the Saviors.\nuser: Why did he lied about himself being a scientist?",
            "document": "Eugene Porter\nDr. Eugene Porter is a fictional character from the comic book series The Walking Dead and is portrayed by Josh McDermitt beginning in the fourth season of the American television series of the same name . In both mediums he claims to be a scientist who knows the cure to the zombie plague and is being escorted to Washington D.C. by Sgt . Abraham Ford and Rosita Espinosa , and encounter Rick Grimes and his group and recruit them to assist their mission . Eugene is overweight , possesses virtually no survival skills of his own and is extremely dependent on the group , but is highly intelligent and resourceful in using technology to ensure the group 's survival . Eventually it is revealed that Eugene is not a scientist , but a high school science teacher , and that he does n't know how to cure the virus but lied to manipulate the other survivors into taking him to Washington D.C. believing it to be the best chance for survival . This proves true as the group eventually finds the Alexandria Safe - Zone where Eugene becomes its primary engineer . Though his lie puts a strain on their friendship , Abraham eventually forgives him and they resume being friends .\n\nList of The Walking Dead (TV series) characters\nDr. Eugene Porter , portrayed by Josh McDermitt , is a survivor of the apocalypse and a member of Abraham Ford 's survivor group and later a minor antagonist after he unwillingly begins working for the Saviors . He comes across as timid and socially awkward . Initially when encountered by Rick 's group , he appears to be a scientist that knows of a cure to the outbreak located in Washington , D.C. , with Abraham and Rosita assisting him . Later , Eugene is forced to reveal that he lied to Abraham to gain his protection back when they first met , leading Abraham to punch him unconscious and creating a rift in their friendship . When the group makes it to Alexandria , Eugene assists the community with his technical knowledge to repair their solar power system . This helps to boost his confidence to learn how to defend himself , reconcile his friendship with Abraham , and help the community find a way to make more bullets . When Negan discovers that Eugene makes bullets , Eugene is taken to work for the Saviors . Although Eugene accepts the offer to become one of the Saviors , he still helps Sasha kill herself but lies about it to Negan . He is an important player for the Saviors in the All Out War arc , but is tormented with conflicting loyalties , especially after he discovers that Dwight is playing both sides .",
        },
    ]
}

predictions = [
    "No, The Walking Dead is not currently shown on TV. The final episode of the series aired on April 17, 2022, on AMC. The show has completed its 11-season run. However, if you're interested in watching previous seasons, they are available to stream on various platforms such as AMC+, Hulu, and Peacock. Additionally, you can find The Walking Dead merchandise and spin-off series on these platforms and other marketplaces.",
    "Yes, The Walking Dead is currently being shown on TV. It airs on AMC, an American basic cable network. AMV airs new episodes of The Walking Dead as well as reruns of previous seasons.",
    "Eugene lied about being a scientist to manipulate the other survivors into taking him to Washington D.C., believing it to be the best chance for survival. He initially used this lie to gain the protection of Abraham Ford. He assists the group with technical tasks even after the truth is revealed, which also improves his own confidence. His lie caused tension within the group, particularly between him and Abraham, but eventually Abraham forgave him and they resumed their friendship.",
]


card = TaskCard(
    loader=LoadFromDictionary(data=data),
    task=Task(
        input_fields={"conversation": "str", "document": "str"},
        reference_fields={},
        prediction_type="str",
        metrics=[
            "metrics.llm_as_judge.conversation_answer_groundedness.ensemble_v1_ibmgenai_judges"
        ],
    ),
    templates=TemplatesDict(
        {
            "simple": InputOutputTemplate(
                input_format="{conversation}\n\n\nDocument:\n{document}",
                output_format="",
            )
        }
    ),
)

dataset = load_dataset(card=card, template_card_index="simple", split="test")
results = evaluate(predictions=predictions, data=dataset)

print("Global Results:")
print(results.global_scores.summary)

print("Instance Results:")
print(results.instance_scores.summary)

# examples/evaluate_using_metrics_ensemble.py
from unitxt import get_logger
from unitxt.api import evaluate, load_dataset
from unitxt.inference import (
    HFPipelineBasedInferenceEngine,
)
from unitxt.metrics import MetricsEnsemble

logger = get_logger()

# define the metrics ensemble
ensemble_metric = MetricsEnsemble(
    metrics=[
        "metrics.llm_as_judge.rating.llama_3_70b_instruct.generic_single_turn",
        "metrics.llm_as_judge.rating.llama_3_8b_instruct_ibm_genai_template_mt_bench_single_turn",
    ],
    weights=[0.75, 0.25],
)
# Use the HF load_dataset API, to load the squad QA dataset using the standard template in the catalog.
# We set loader_limit to 20 to reduce download time.
dataset = load_dataset(
    card="cards.squad",
    template="templates.qa.with_context.simple",
    format="formats.chat_api",
    metrics=[ensemble_metric],
    loader_limit=20,
    max_test_instances=10,
    split="test",
)

# Infer using Llama-3.2-1B base using HF API
model = HFPipelineBasedInferenceEngine(
    model_name="Qwen/Qwen1.5-0.5B-Chat", max_new_tokens=32
)
# Change to this to infer with external APIs:
# CrossProviderInferenceEngine(model="llama-3-2-1b-instruct", provider="watsonx")
# The provider can be one of: ["watsonx", "together-ai", "open-ai", "aws", "ollama", "bam"]

predictions = model(dataset)

# Evaluate the predictions using the defined metric.
results = evaluate(predictions=predictions, data=dataset)

print("Global Results:")
print(results.global_scores.summary)

print("Instance Results:")
print(results.instance_scores.summary)

# examples/evaluate_llm_as_judge_pairwise_user_criteria_no_catalog.py
from typing import Any, List

from unitxt import get_logger
from unitxt.api import evaluate, load_dataset
from unitxt.card import Task, TaskCard
from unitxt.inference import CrossProviderInferenceEngine
from unitxt.llm_as_judge import LLMJudgePairwise
from unitxt.llm_as_judge_operators import CreateCriteriaFromDict
from unitxt.loaders import LoadFromDictionary
from unitxt.templates import NullTemplate

logger = get_logger()

temperature_criteria_json = {
    "name": "Temperature in Fahrenheit and Celsius",
    "description": "The temperature is described in both Fahrenheit and Celsius.",
}


funny_criteria_json = {"name": "Funny joke", "description": "Is the response funny?"}

data = {
    "test": [
        {"question": "How is the weather?", "judgement": temperature_criteria_json},
        {"question": "Tell me a joke about cats", "judgement": funny_criteria_json},
    ]
}

metric = LLMJudgePairwise(
    inference_engine=CrossProviderInferenceEngine(
        model="llama-3-1-70b-instruct", max_tokens=1024
    ),
    context_fields=["question"],
    criteria_field="criteria",
)


card = TaskCard(
    loader=LoadFromDictionary(data=data, data_classification_policy=["public"]),
    preprocess_steps=[
        CreateCriteriaFromDict(field="judgement", to_field="criteria"),
    ],
    task=Task(
        input_fields={"question": str},
        reference_fields={"criteria": Any},
        prediction_type=List[str],
        metrics=[metric],
        default_template=NullTemplate(),
    ),
)

predictions = [
    [
        """On most days, the weather is warm and humid, with temperatures often soaring into the high 80s and low 90s Fahrenheit (around 31-34Â°C). The dense foliage of the jungle acts as a natural air conditioner, keeping the temperature relatively stable and comfortable for the inhabitants.""",
        """On most days, the weather is warm and humid, with temperatures often soaring into the high 80s and low 90s Fahrenheit. The dense foliage of the jungle acts as a natural air conditioner, keeping the temperature relatively stable and comfortable for the inhabitants.""",
        """On most days, the weather is warm and humid. The dense foliage of the jungle acts as a natural air conditioner, keeping the temperature relatively stable and comfortable for the inhabitants.""",
    ],
    [
        """Why did the cat cross the road? To cat to the other side.""",
        """Why did the cat sit on the computer? Because it wanted to keep an eye on the mouse!""",
        """What is red, yellow and green? A traffic light.""",
    ],
]

dataset = load_dataset(card=card, split="test")

results = evaluate(predictions=predictions, data=dataset)

print("Global Scores:")
print(results.global_scores.summary)

print("Instance Scores:")
print(results.instance_scores.summary)

# examples/evaluate_batched_multiclass_classification.py
import re
from typing import Any, Dict, List, NewType, Tuple

import pandas as pd
from unitxt import get_logger
from unitxt.api import evaluate, load_dataset
from unitxt.artifact import fetch_artifact
from unitxt.formats import SystemFormat
from unitxt.operators import CollateInstances, Copy, FieldOperator, Rename
from unitxt.processors import PostProcess
from unitxt.serializers import MultiTypeSerializer, SingleTypeSerializer
from unitxt.task import Task
from unitxt.templates import InputOutputTemplate
from unitxt.type_utils import register_type

logger = get_logger()


# Parse string in the format
# """1. class1
# 2. class2
# 3. class3"""
# to lis of tuples [ ("1", "class1"), ("2","class2"), ("3", "class3")]


class ParseEnumeratedList(FieldOperator):
    def process_value(self, text: Any) -> Any:
        result = []
        for x in text.split("\n"):
            line_result = re.findall(r"(\d+)\.\s*(.*)", x)
            if len(line_result) == 1:
                result.append(line_result[0])
        return result


EnumeratedList = NewType("EnumeratedList", List[str])
register_type(EnumeratedList)


class EnumeratedListSerializer(SingleTypeSerializer):
    serialized_type = EnumeratedList

    def serialize(self, value: EnumeratedList, instance: Dict[str, Any]) -> str:
        return "\n".join([f"{i+1}. {v}" for i, v in enumerate(value)])


task = Task(
    __description__="""This is a batched multi-class classification task, where multiple 'texts' are classified to a given set of 'classes' in one inference call.
    The `type_of_class` field defines the type of classiication (e.g. "sentiment", "emotion", "topic" ) """,
    input_fields={
        "texts": EnumeratedList,
        "text_type": str,
        "classes": EnumeratedList,
        "type_of_class": str,
    },
    reference_fields={"labels": EnumeratedList},
    prediction_type=List[Tuple[str, str]],
    metrics=["metrics.ner"],
    augmentable_inputs=["texts"],
    defaults={"text_type": "text"},
)

template = InputOutputTemplate(
    input_format="Classify each of the texts to its corresponding {type_of_class} from one of these options:\n{classes}\nReturn for each index the correspond class in a separate line.\nTexts:\n{texts}",
    target_prefix="Answer:\n",
    output_format="{labels}",
    postprocessors=["processors.lower_case", PostProcess(ParseEnumeratedList())],
    serializer=MultiTypeSerializer(serializers=[EnumeratedListSerializer()]),
)
df = pd.DataFrame(
    columns=[
        "provider",
        "model",
        "batch_size",
        "num_instances",
        "f1_micro",
        "ci_low",
        "ci_high",
        "hellucinations",
    ]
)

for provider in [
    "watsonx",
    "bam",
]:
    for model_name in [
        "granite-3-8b-instruct",
        "llama-3-8b-instruct",
    ]:
        batch_sizes = [30, 20, 10, 5, 1]

        for batch_size in batch_sizes:
            card, _ = fetch_artifact("cards.banking77")
            card.preprocess_steps.extend(
                [
                    CollateInstances(batch_size=batch_size),
                    Rename(field_to_field={"text": "texts", "label": "labels"}),
                    Copy(field="text_type/0", to_field="text_type"),
                    Copy(field="classes/0", to_field="classes"),
                    Copy(field="type_of_class/0", to_field="type_of_class"),
                ]
            )
            card.task = task
            card.templates = [template]
            format = "formats.chat_api"
            if provider == "bam" and model_name.startswith("llama"):
                format = "formats.llama3_instruct"
            if provider == "bam" and model_name.startswith("granite"):
                format = SystemFormat(
                    demo_format=(
                        "{instruction}\\N{source}\\N<|end_of_text|>\n"
                        "<|start_of_role|>assistant<|end_of_role|>{target}\\N<|end_of_text|>\n"
                        "<|start_of_role|>user<|end_of_role|>"
                    ),
                    model_input_format=(
                        "<|start_of_role|>system<|end_of_role|>{system_prompt}<|end_of_text|>\n"
                        "<|start_of_role|>user<|end_of_role|>{demos}{instruction}\\N{source}\\N<|end_of_text|>\n"
                        "<|start_of_role|>assistant<|end_of_role|>"
                    ),
                )

            dataset = load_dataset(
                card=card,
                template_card_index=0,
                format=format,
                num_demos=1,
                demos_pool_size=5,
                loader_limit=1000,
                max_test_instances=200 / batch_size,
            )

            test_dataset = dataset["test"]
            from unitxt.inference import CrossProviderInferenceEngine

            model = CrossProviderInferenceEngine(
                model=model_name, max_tokens=1024, provider=provider
            )
            """
            We are using a CrossProviderInferenceEngine inference engine that supply api access to provider such as:
            watsonx, bam, openai, azure, aws and more.

            For the arguments these inference engines can receive, please refer to the classes documentation or read
            about the the open ai api arguments the CrossProviderInferenceEngine follows.
            """
            predictions = model(test_dataset)

            results = evaluate(predictions=predictions, data=test_dataset)

            print(
                results.instance_scores.to_df(
                    columns=[
                        "source",
                        "prediction",
                        "processed_prediction",
                        "processed_references",
                    ],
                )
            )

            global_scores = results.global_scores
            df.loc[len(df)] = [
                provider,
                model_name,
                batch_size,
                global_scores["num_of_instances"],
                global_scores["score"],
                global_scores["score_ci_low"],
                global_scores["score_ci_high"],
                1.0 - global_scores["in_classes_support"],
            ]

            df = df.round(decimals=2)
            logger.info(df.to_markdown())

# examples/evaluate_different_formats.py
import pandas as pd
from unitxt.api import evaluate, load_dataset
from unitxt.inference import CrossProviderInferenceEngine

model = CrossProviderInferenceEngine(
    model="llama-3-8b-instruct", max_tokens=32, provider="bam"
)
"""
We are using a CrossProviderInferenceEngine inference engine that supply api access to provider such as:
watsonx, bam, openai, azure, aws and more.

For the arguments these inference engines can receive, please refer to the classes documentation or read
about the the open ai api arguments the CrossProviderInferenceEngine follows.
"""

card = "cards.boolq.classification"
template = "templates.classification.multi_class.relation.default"

df = pd.DataFrame(columns=["format", "system_prompt", "f1_micro", "ci_low", "ci_high"])

for format in [
    "formats.llama3_instruct",
    "formats.empty",
    "formats.llama3_instruct_all_demos_in_one_turn",
]:
    for system_prompt in [
        "system_prompts.models.llama2",
        "system_prompts.empty",
    ]:
        dataset = load_dataset(
            card=card,
            template=template,
            format=format,
            system_prompt=system_prompt,
            num_demos=2,
            demos_pool_size=50,
            loader_limit=300,
            max_test_instances=100,
            split="test",
        )

        predictions = model(dataset)
        results = evaluate(predictions=predictions, data=dataset)

        print(
            f"Sample input and output for format '{format}' and system prompt '{system_prompt}':"
        )

        print(
            results.instance_scores.to_df(
                columns=[
                    "source",
                    "prediction",
                ]
            )
        )

        global_scores = results.global_scores
        df.loc[len(df)] = [
            format,
            system_prompt,
            global_scores["score"],
            global_scores["score_ci_low"],
            global_scores["score_ci_high"],
        ]

        df = df.round(decimals=2)
        print(df.to_markdown())

# examples/evaluate_vision_benchmark.py
from unitxt import evaluate, load_dataset, settings
from unitxt.inference import (
    CrossProviderInferenceEngine,
)

with settings.context(
    disable_hf_datasets_cache=False,
    allow_unverified_code=True,
):
    test_dataset = load_dataset(
        "benchmarks.vision[format=formats.chat_api,loader_limit=30,max_samples_per_subset=30]",
        split="test",
    )

# Infer
model = CrossProviderInferenceEngine(
    model="llama-3-2-11b-vision-instruct", max_tokens=30, provider="rits"
)
"""
We are using a CrossProviderInferenceEngine inference engine that supply api access to provider such as:
watsonx, bam, openai, azure, aws and more.

For the arguments these inference engines can receive, please refer to the classes documentation or read
about the the open ai api arguments the CrossProviderInferenceEngine follows.
"""

predictions = model(test_dataset)
results = evaluate(predictions=predictions, data=test_dataset)

print("Global scores:")
print(results.global_scores.summary)
print("Subsets scores:")
print(results.subsets_scores.summary)

# | subset   |    score | score_name      |   num_of_instances |
# |:---------|---------:|:----------------|-------------------:|
# | ALL      | 0.429583 | subsets_mean    |                150 |
# | doc_vqa  | 0.79103  | anls            |                 30 |
# | info_vqa | 0.464885 | anls            |                 30 |
# | chart_qa | 0.3      | relaxed_overall |                 30 |
# | ai2d     | 0.2      | exact_match_mm  |                 30 |
# | websrc   | 0.392    | websrc_squad_f1 |                 30 |

# examples/evaluate_different_demo_selections.py
import pandas as pd
from unitxt import get_logger
from unitxt.api import evaluate, load_dataset
from unitxt.inference import CrossProviderInferenceEngine
from unitxt.splitters import CloseTextSampler, FixedIndicesSampler, RandomSampler

logger = get_logger()

# This examples evaluates different kinds of demo selection strategies on a classification task.
# The different strategies are evaluates in 1,3,5 shots. The examples are selected from a demo pool of 100 examples.
# RandomSampler - randomly sample a different set of examples for each test instance
# CloseTextSampler - select the lexically closest amples from the demo pool for each test instance
# FixedIndicesSampler - selec the same fixed set of demo examples for all instances

model = CrossProviderInferenceEngine(model="llama-3-2-1b-instruct", max_tokens=32)
"""
We are using a CrossProviderInferenceEngine inference engine that supply api access to provider such as:
watsonx, bam, openai, azure, aws and more.

For the arguments these inference engines can receive, please refer to the classes documentation or read
about the the open ai api arguments the CrossProviderInferenceEngine follows.
"""

df = pd.DataFrame(columns=["num_demos", "sampler", "f1_micro", "ci_low", "ci_high"])

for num_demos in [1, 2]:
    for demo_sampler in [
        RandomSampler(),
        CloseTextSampler(field="text"),
        FixedIndicesSampler(indices=[0, 1]),
    ]:
        dataset = load_dataset(
            card="cards.ledgar",
            template="templates.classification.multi_class.title",
            format="formats.chat_api",
            num_demos=num_demos,
            demos_pool_size=50,
            loader_limit=200,
            max_test_instances=10,
            sampler=demo_sampler,
            split="test",
        )

        predictions = model(dataset)
        results = evaluate(predictions=predictions, data=dataset)

        logger.info(
            f"Sample input and output for sampler {demo_sampler} and num_demos '{num_demos}':"
        )
        print(
            results.instance_scores.to_df(
                columns=["source", "prediction", "processed_prediction"]
            )
        )

        global_scores = results.global_scores

        df.loc[len(df)] = [
            num_demos,
            demo_sampler.to_json(),
            global_scores["score"],
            global_scores["score_ci_low"],
            global_scores["score_ci_high"],
        ]

        df = df.round(decimals=2)
        logger.info(df.to_markdown())
