[build-system]
requires = ["setuptools", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "unitxt"
dynamic = ["version"]
description = "Load any mixture of text to text data in one line of code"
authors = [
    { name = "IBM Research", email = "elron.bandel@ibm.com" }
]
requires-python = ">=3.8"
license = { file = "LICENSE" }
readme = "README.md"
classifiers = [
    "Programming Language :: Python :: 3",
    "Operating System :: OS Independent"
]
dependencies = [
    "datasets>=2.16.0",
    "evaluate",
    "absl-py",
    "ipadic",
    "scipy"
]

[project.urls]
Homepage = "https://www.unitxt.ai"
Documentation = "https://www.unitxt.ai/en/latest/documentation.html"
Repository = "https://github.com/ibm/unitxt"

# [tool.setuptools.dynamic]
# version = {attr = "src.unitxt.version.version"}

[tool.setuptools]
package-dir = {"" = "src"}
packages = {find = {where = ["src"]}}
include-package-data = true

[tool.setuptools.package-data]
unitxt = ["catalog/**/*.json", "ui/banner.png"]

[tool.setuptools.dynamic]
version = {attr = "unitxt.version.version"}

[project.optional-dependencies]
dev = [
    "ruff",
    "pre-commit",
    "detect-secrets",
    "tomli",
    "codespell",
    "fuzzywuzzy",
    "httpretty"
]
docs = [
    "sphinx_rtd_theme",
    "piccolo_theme",
    "sphinxext-opengraph",
    "datasets",
    "evaluate",
    "nltk",
    "sacrebleu",
    "absl-py",
    "rouge_score",
    "scikit-learn",
    "jiwer",
    "editdistance",
    "fuzzywuzzy"
]
helm = [
    "crfm-helm[unitxt]>=0.5.3"
]
service = [
    "torch==1.12.1",
    "fastapi==0.109.0",
    "uvicorn[standard]==0.27.0.post1",
    "python-jose[cryptography]==3.3.0",
    "transformers"
]
tests = [
    "bert_score",
    "transformers",
    "sentence_transformers",
    "ibm-cos-sdk",
    "kaggle==1.6.14",
    "opendatasets",
    "httpretty~=1.1.4",
    "editdistance",
    "rouge-score",
    "nltk",
    "mecab-python3",
    "sacrebleu[ko]",
    "scikit-learn",
    "jiwer",
    "conllu",
    "llama-index-core",
    "llama-index-llms-openai",
    "pytrec-eval",
    "SentencePiece",
    "fuzzywuzzy",
    "openai",
    "ibm-generative-ai",
    "bs4",
    "tenacity==8.3.0",
    "accelerate",
    "spacy"
]
ui = [
    "gradio",
    "transformers"
]
watsonx = [
    "ibm-watsonx-ai==1.1.14"
]
inference-tests = [
  "litellm==v1.52.9",
  "tenacity",
  "diskcache",
  "numpy==1.26.4"
]

# Combine all extras by referencing other groups
all = [
    "unitxt[base]",
    "unitxt[dev]",
    "unitxt[docs]",
    "unitxt[helm]",
    "unitxt[service]",
    "unitxt[tests]",
    "unitxt[ui]",
    "unitxt[watsonx]"
]

[project.scripts]
unitxt-explore = "unitxt.ui:launch"
unitxt-metrics-service = "unitxt.service.metrics.main:start_metrics_http_service"

[tool.ruff]
exclude = [
    ".bzr",
    ".direnv",
    ".eggs",
    ".git",
    ".git-rewrite",
    ".hg",
    ".mypy_cache",
    ".nox",
    ".pants.d",
    ".pytype",
    ".ruff_cache",
    ".svn",
    ".tox",
    ".venv",
    "__pypackages__",
    "_build",
    "buck-out",
    "build",
    "dist",
    "node_modules",
    "venv",
]

line-length = 88
indent-width = 4
target-version = "py38"

[tool.ruff.lint.per-file-ignores]
"src/*" = ["TID252"]
".github/*" = ["TID251"]
".vscode/*" = ["TID251"]
"tests/*" = ["TID251"]
"utils/*" = ["TID251"]
"src/unitxt/__init__.py" = ["F811", "F401"]
"src/unitxt/metric.py" = ["F811", "F401"]
"src/unitxt/dataset.py" = ["F811", "F401"]
"src/unitxt/blocks.py" = ["F811", "F401"]
"tests/library/test_loaders.py" = ["N802", "N803"]
"tests/library/test_dataclass.py" = ["F811", "E731"]
"src/unitxt/validate.py" = ["B024"]
"src/unitxt/standard.py" = ["C901"]
"src/unitxt/type_utils.py" = ["C901"]
"src/unitxt/metric_utils.py" = ["C901"]
"src/unitxt/dataclass.py" = ["C901"]
"src/unitxt/operators.py" = ["C901"]
"docs/conf.py" = ["E402"]
"prepare/cards/attaq_500.py" = ["RUF001"]
"prepare/instructions/models/llama.py" = ["RUF001"]
"utils/hf/prepare_dataset.py" = ["T201"]
"utils/hf/prepare_metric.py" = ["T201"]
"utils/compare_unitxt_datasets_between_versions.py" = ["C901"]
"performance/*.py" = ["T201"]

[tool.ruff.lint]
extend-select = [
  "UP",
  "D",
  "F",
  "E",
  "B",
  "C",
  "R",
  "T",
  "TID25",
  "W",
  "RUF100",
  "I",
  "G",
  "N",
  "Q",
  "RUF",
]
ignore = ["E501", "E203", "E722", "D101", "D102", "D103", "D100", "D104", "D105", "D106", "D107", "RUF012", "G004"]
fixable = ["ALL"]
unfixable = []
dummy-variable-rgx = "^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$"

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false
line-ending = "auto"

[tool.ruff.lint.pydocstyle]
convention = "google"

[tool.ruff.lint.flake8-bugbear]
extend-immutable-calls = ["fastapi.Depends", "fastapi.params.Depends", "fastapi.Query", "fastapi.params.Query"]

[tool.ruff.lint.flake8-tidy-imports.banned-api]
"src".msg = "Use unitxt outside src/ and relative imports inside src/ and install unitxt from source with `pip install -e '.[dev]'`."

[tool.codespell]
ignore-words-list = 'rouge,ot,ans,nd,cann,som,tha,vie,ment'
check-filenames = true
check-hidden = false
regex = "(?<![a-z])[a-z'`]+|[A-Z][a-z'`]*|[a-z]+'[a-z]*|[a-z]+(?=[_-])|[a-z]+(?=[A-Z])|\\d+"
skip = '*cards/mt/flores101*,*cards/trec*,*cards/belebele*,*cards/amazon_mass*,*cards/reuters21578*,*cards/attaq_500*,*cards/cohere_for_ai*,*egg-info*,*/logs/*'